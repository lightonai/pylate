{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"PyLate <p>Flexible Training and Retrieval for Late Interaction Models</p> <p></p> <p> </p> <p> PyLate is a library built on top of Sentence Transformers, designed to simplify and optimize fine-tuning, inference, and retrieval with state-of-the-art ColBERT models. It enables easy fine-tuning on both single and multiple GPUs, providing flexibility for various hardware setups. PyLate also streamlines document retrieval and allows you to load a wide range of models, enabling you to construct ColBERT models from most pre-trained language models. </p> <p> </p>"},{"location":"#installation","title":"Installation","text":"<p>You can install PyLate using pip:</p> <pre><code>pip install pylate\n</code></pre> <p>For evaluation dependencies, use:</p> <pre><code>pip install \"pylate[eval]\"\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<p>The complete documentation is available here, which includes in-depth guides, examples, and API references.</p> <p> </p>"},{"location":"#training","title":"Training","text":""},{"location":"#contrastive-training","title":"Contrastive Training","text":"<p>Here\u2019s a simple example of training a ColBERT model on the MS MARCO dataset triplet dataset using PyLate. This script demonstrates training with contrastive loss and evaluating the model on a held-out eval set:</p> <pre><code>import torch\nfrom datasets import load_dataset\nfrom sentence_transformers import (\n    SentenceTransformerTrainer,\n    SentenceTransformerTrainingArguments,\n)\n\nfrom pylate import evaluation, losses, models, utils\n\n# Define model parameters for contrastive training\nmodel_name = \"bert-base-uncased\"  # Choose the pre-trained model you want to use as base\nbatch_size = 32  # Larger batch size often improves results, but requires more memory\n\nnum_train_epochs = 1  # Adjust based on your requirements\n# Set the run name for logging and output directory\nrun_name = \"contrastive-bert-base-uncased\"\noutput_dir = f\"output/{run_name}\"\n\n# 1. Here we define our ColBERT model. If not a ColBERT model, will add a linear layer to the base encoder.\nmodel = models.ColBERT(model_name_or_path=model_name)\n\n# Compiling the model makes the training faster\nmodel = torch.compile(model)\n\n# Load dataset\ndataset = load_dataset(\"sentence-transformers/msmarco-bm25\", \"triplet\", split=\"train\")\n# Split the dataset (this dataset does not have a validation set, so we split the training set)\nsplits = dataset.train_test_split(test_size=0.01)\ntrain_dataset = splits[\"train\"]\neval_dataset = splits[\"test\"]\n\n# Define the loss function\ntrain_loss = losses.Contrastive(model=model)\n\n# Initialize the evaluator\ndev_evaluator = evaluation.ColBERTTripletEvaluator(\n    anchors=eval_dataset[\"query\"],\n    positives=eval_dataset[\"positive\"],\n    negatives=eval_dataset[\"negative\"],\n)\n\n# Configure the training arguments (e.g., batch size, evaluation strategy, logging steps)\nargs = SentenceTransformerTrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=num_train_epochs,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    fp16=True,  # Set to False if you get an error that your GPU can't run on FP16\n    bf16=False,  # Set to True if you have a GPU that supports BF16\n    run_name=run_name,  # Will be used in W&amp;B if `wandb` is installed\n    learning_rate=3e-6,\n)\n\n# Initialize the trainer for the contrastive training\ntrainer = SentenceTransformerTrainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    loss=train_loss,\n    evaluator=dev_evaluator,\n    data_collator=utils.ColBERTCollator(model.tokenize),\n)\n# Start the training process\ntrainer.train()\n</code></pre> <p>After training, the model can be loaded using the output directory path:</p> <pre><code>from pylate import models\n\nmodel = models.ColBERT(model_name_or_path=\"contrastive-bert-base-uncased\")\n</code></pre> <p>Please note that temperature parameter has a very high importance in contrastive learning, and a temperature around 0.02 is often used in the literature:</p> <pre><code>train_loss = losses.Contrastive(model=model, temperature=0.02)\n</code></pre> <p>As contrastive learning is not compatible with gradient accumulation, you can leverage GradCache to emulate bigger batch sizes without requiring more memory by using the <code>CachedContrastiveLoss</code> to define a mini_batch_size while increasing the <code>per_device_train_batch_size</code>:</p> <pre><code>train_loss = losses.CachedContrastive(\n        model=model, mini_batch_size=mini_batch_size\n)\n</code></pre> <p>Finally, if you are in a multi-GPU setting, you can gather all the elements from the different GPUs to create even bigger batch sizes by setting <code>gather_across_devices</code> to <code>True</code> (for both <code>Contrastive</code> and <code>CachedContrastive</code> losses):</p> <pre><code>train_loss = losses.Contrastive(model=model, gather_across_devices=True)\n</code></pre> <p> </p>"},{"location":"#knowledge-distillation","title":"Knowledge Distillation","text":"<p>To get the best performance when training a ColBERT model, you should use knowledge distillation to train the model using the scores of a strong teacher model. Here's a simple example of how to train a model using knowledge distillation in PyLate on MS MARCO:</p> <pre><code>import torch\nfrom datasets import load_dataset\nfrom sentence_transformers import (\n    SentenceTransformerTrainer,\n    SentenceTransformerTrainingArguments,\n)\n\nfrom pylate import losses, models, utils\n\n# Load the datasets required for knowledge distillation (train, queries, documents)\ntrain = load_dataset(\n    path=\"lightonai/ms-marco-en-bge\",\n    name=\"train\",\n)\n\nqueries = load_dataset(\n    path=\"lightonai/ms-marco-en-bge\",\n    name=\"queries\",\n)\n\ndocuments = load_dataset(\n    path=\"lightonai/ms-marco-en-bge\",\n    name=\"documents\",\n)\n\n# Set the transformation to load the documents/queries texts using the corresponding ids on the fly\ntrain.set_transform(\n    utils.KDProcessing(queries=queries, documents=documents).transform,\n)\n\n# Define the base model, training parameters, and output directory\nmodel_name = \"bert-base-uncased\"  # Choose the pre-trained model you want to use as base\nbatch_size = 16\nnum_train_epochs = 1\n# Set the run name for logging and output directory\nrun_name = \"knowledge-distillation-bert-base\"\noutput_dir = f\"output/{run_name}\"\n\n# Initialize the ColBERT model from the base model\nmodel = models.ColBERT(model_name_or_path=model_name)\n\n# Compiling the model to make the training faster\nmodel = torch.compile(model)\n\n# Configure the training arguments (e.g., epochs, batch size, learning rate)\nargs = SentenceTransformerTrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=num_train_epochs,\n    per_device_train_batch_size=batch_size,\n    fp16=True,  # Set to False if you get an error that your GPU can't run on FP16\n    bf16=False,  # Set to True if you have a GPU that supports BF16\n    run_name=run_name,\n    learning_rate=1e-5,\n)\n\n# Use the Distillation loss function for training\ntrain_loss = losses.Distillation(model=model)\n\n# Initialize the trainer\ntrainer = SentenceTransformerTrainer(\n    model=model,\n    args=args,\n    train_dataset=train,\n    loss=train_loss,\n    data_collator=utils.ColBERTCollator(tokenize_fn=model.tokenize),\n)\n\n# Start the training process\ntrainer.train()\n</code></pre>"},{"location":"#nanobeir-evaluator","title":"NanoBEIR evaluator","text":"<p>If you are training an English retrieval model, you can use NanoBEIR evaluator, which allows to run small version of BEIR to get quick validation results.</p> <pre><code>evaluator=evaluation.NanoBEIREvaluator(),\n</code></pre> <p> </p>"},{"location":"#datasets","title":"Datasets","text":"<p>PyLate supports Hugging Face Datasets, enabling seamless triplet / knowledge distillation based training. For contrastive training, you can use any of the existing sentence transformers triplet datasets. Below is an example of creating a custom triplet dataset for training:</p> <pre><code>from datasets import Dataset\n\ndataset = [\n    {\n        \"query\": \"example query 1\",\n        \"positive\": \"example positive document 1\",\n        \"negative\": \"example negative document 1\",\n    },\n    {\n        \"query\": \"example query 2\",\n        \"positive\": \"example positive document 2\",\n        \"negative\": \"example negative document 2\",\n    },\n    {\n        \"query\": \"example query 3\",\n        \"positive\": \"example positive document 3\",\n        \"negative\": \"example negative document 3\",\n    },\n]\n\ndataset = Dataset.from_list(mapping=dataset)\n\ntrain_dataset, test_dataset = dataset.train_test_split(test_size=0.3)\n</code></pre> <p>Note that PyLate supports more than one negative per query, simply add the additional negatives after the first one in the row.</p> <pre><code>{\n        \"query\": \"example query 1\",\n        \"positive\": \"example positive document 1\",\n        \"negative_1\": \"example negative document 1\",\n        \"negative_2\": \"example negative document 2\",\n}\n</code></pre> <p>To create a knowledge distillation dataset, you can use the following snippet:</p> <pre><code>from datasets import Dataset\n\ndataset = [\n    {\n        \"query_id\": 54528,\n        \"document_ids\": [\n            6862419,\n            335116,\n            339186,\n        ],\n        \"scores\": [\n            0.4546215673141326,\n            0.6575686537173476,\n            0.26825184192900203,\n        ],\n    },\n    {\n        \"query_id\": 749480,\n        \"document_ids\": [\n            6862419,\n            335116,\n            339186,\n        ],\n        \"scores\": [\n            0.2546215673141326,\n            0.7575686537173476,\n            0.96825184192900203,\n        ],\n    },\n]\n\n\ndataset = Dataset.from_list(mapping=dataset)\n\ndocuments = [\n    {\"document_id\": 6862419, \"text\": \"example doc 1\"},\n    {\"document_id\": 335116, \"text\": \"example doc 2\"},\n    {\"document_id\": 339186, \"text\": \"example doc 3\"},\n]\n\nqueries = [\n    {\"query_id\": 749480, \"text\": \"example query\"},\n]\n\ndocuments = Dataset.from_list(mapping=documents)\n\nqueries = Dataset.from_list(mapping=queries)\n</code></pre> <p> </p>"},{"location":"#retrieval","title":"Retrieval","text":"<p>PyLate provides an efficient index with FastPLAID. Simply load a ColBERT model and initialize the index to perform retrieval.</p> <pre><code>from pylate import indexes, models, retrieve\n\nmodel = models.ColBERT(\n    model_name_or_path=\"lightonai/GTE-ModernColBERT-v1\",\n)\n\nindex = indexes.PLAID(\n    index_folder=\"pylate-index\",\n    index_name=\"index\",\n    override=True,\n)\n\nretriever = retrieve.ColBERT(index=index)\n</code></pre> <p>Once the model and index are set up, we can add documents to the index using their embeddings and corresponding ids:</p> <pre><code>documents_ids = [\"1\", \"2\", \"3\"]\n\ndocuments = [\n    \"ColBERT\u2019s late-interaction keeps token-level embeddings to deliver cross-encoder-quality ranking at near-bi-encoder speed, enabling fine-grained relevance, robustness across domains, and hardware-friendly scalable search.\",\n\n    \"PLAID compresses ColBERT token vectors via product quantization to shrink storage by 10\u00d7, uses two-stage centroid scoring for sub-200 ms latency, and plugs directly into existing ColBERT pipelines.\",\n\n    \"PyLate is a library built on top of Sentence Transformers, designed to simplify and optimize fine-tuning, inference, and retrieval with state-of-the-art ColBERT models. It enables easy fine-tuning on both single and multiple GPUs, providing flexibility for various hardware setups. PyLate also streamlines document retrieval and allows you to load a wide range of models, enabling you to construct ColBERT models from most pre-trained language models.\",\n]\n\n# Encode the documents\ndocuments_embeddings = model.encode(\n    documents,\n    batch_size=32,\n    is_query=False, # Encoding documents\n    show_progress_bar=True,\n)\n\n# Add the documents ids and embeddings to the PLAID index\nindex.add_documents(\n    documents_ids=documents_ids,\n    documents_embeddings=documents_embeddings,\n)\n</code></pre> <p>Then we can retrieve the top-k documents for a given set of queries:</p> <pre><code>queries_embeddings = model.encode(\n    [\"query for document 3\", \"query for document 1\"],\n    batch_size=32,\n    is_query=True, # Encoding queries\n    show_progress_bar=True,\n)\n\nscores = retriever.retrieve(\n    queries_embeddings=queries_embeddings,\n    k=10,\n)\n\nprint(scores)\n</code></pre> <p>Sample Output:</p> <pre><code>[\n    [\n        {\"id\": \"3\", \"score\": 11.266985893249512},\n        {\"id\": \"1\", \"score\": 10.303335189819336},\n        {\"id\": \"2\", \"score\": 9.502392768859863},\n    ],\n    [\n        {\"id\": \"1\", \"score\": 10.88800048828125},\n        {\"id\": \"3\", \"score\": 9.950843811035156},\n        {\"id\": \"2\", \"score\": 9.602447509765625},\n    ],\n]\n</code></pre> <p> </p>"},{"location":"#reranking","title":"Reranking","text":"<p>If you want to use the ColBERT model to perform reranking on top of your first-stage retrieval pipeline without building an index, you can simply use <code>rank.rerank</code> function which takes the queries and documents embeddings along with the documents ids to rerank them:</p> <pre><code>from pylate import rank\n\nqueries = [\n    \"query A\",\n    \"query B\",\n]\n\ndocuments = [\n    [\"document A\", \"document B\"],\n    [\"document 1\", \"document C\", \"document B\"],\n]\n\ndocuments_ids = [\n    [1, 2],\n    [1, 3, 2],\n]\n\nqueries_embeddings = model.encode(\n    queries,\n    is_query=True,\n)\n\ndocuments_embeddings = model.encode(\n    documents,\n    is_query=False,\n)\n\nreranked_documents = rank.rerank(\n    documents_ids=documents_ids,\n    queries_embeddings=queries_embeddings,\n    documents_embeddings=documents_embeddings,\n)\n</code></pre> <p> </p>"},{"location":"#contributing","title":"Contributing","text":"<p>We welcome contributions! To get started:</p> <ol> <li>Install the development dependencies:</li> </ol> <pre><code>pip install \"pylate[dev]\"\n</code></pre> <ol> <li>Run tests:</li> </ol> <pre><code>make test\n</code></pre> <ol> <li>Format code with Ruff:</li> </ol> <pre><code>make lint\n</code></pre>"},{"location":"#citation","title":"Citation","text":"<p>You can refer to the library with this BibTeX:</p> <pre><code>@inproceedings{DBLP:conf/cikm/ChaffinS25,\n  author       = {Antoine Chaffin and\n                  Rapha{\\\"{e}}l Sourty},\n  editor       = {Meeyoung Cha and\n                  Chanyoung Park and\n                  Noseong Park and\n                  Carl Yang and\n                  Senjuti Basu Roy and\n                  Jessie Li and\n                  Jaap Kamps and\n                  Kijung Shin and\n                  Bryan Hooi and\n                  Lifang He},\n  title        = {PyLate: Flexible Training and Retrieval for Late Interaction Models},\n  booktitle    = {Proceedings of the 34th {ACM} International Conference on Information\n                  and Knowledge Management, {CIKM} 2025, Seoul, Republic of Korea, November\n                  10-14, 2025},\n  pages        = {6334--6339},\n  publisher    = {{ACM}},\n  year         = {2025},\n  url          = {https://github.com/lightonai/pylate},\n  doi          = {10.1145/3746252.3761608},\n}\n</code></pre>"},{"location":"#deepwiki","title":"DeepWiki","text":"<p>PyLate is indexed on DeepWiki so you can ask questions to LLMs using Deep Research to explore the codebase and get help to add new features.</p>"},{"location":"api/overview/","title":"Overview","text":""},{"location":"api/overview/#evaluation","title":"evaluation","text":"<p>Classes</p> <ul> <li>ColBERTDistillationEvaluator</li> <li>ColBERTTripletEvaluator</li> <li>NanoBEIREvaluator</li> <li>PyLateInformationRetrievalEvaluator</li> </ul> <p>Functions</p> <ul> <li>evaluate</li> <li>get_beir_triples</li> <li>load_beir</li> <li>load_custom_dataset</li> </ul>"},{"location":"api/overview/#hf_hub","title":"hf_hub","text":"<ul> <li>PylateModelCardData</li> </ul>"},{"location":"api/overview/#indexes","title":"indexes","text":"<ul> <li>PLAID</li> <li>Voyager</li> </ul>"},{"location":"api/overview/#losses","title":"losses","text":"<ul> <li>CachedContrastive</li> <li>Contrastive</li> <li>Distillation</li> </ul>"},{"location":"api/overview/#models","title":"models","text":"<ul> <li>ColBERT</li> <li>Dense</li> </ul>"},{"location":"api/overview/#rank","title":"rank","text":"<p>Classes</p> <ul> <li>RerankResult</li> </ul> <p>Functions</p> <ul> <li>rerank</li> </ul>"},{"location":"api/overview/#retrieve","title":"retrieve","text":"<ul> <li>ColBERT</li> </ul>"},{"location":"api/overview/#scores","title":"scores","text":"<p>Classes</p> <ul> <li>SimilarityFunction</li> </ul> <p>Functions</p> <ul> <li>colbert_kd_scores</li> <li>colbert_scores</li> <li>colbert_scores_pairwise</li> </ul>"},{"location":"api/overview/#utils","title":"utils","text":"<p>Classes</p> <ul> <li>ColBERTCollator</li> <li>KDProcessing</li> </ul> <p>Functions</p> <ul> <li>all_gather</li> <li>all_gather_with_gradients</li> <li>convert_to_tensor</li> <li>get_rank</li> <li>get_world_size</li> <li>iter_batch</li> </ul>"},{"location":"api/evaluation/ColBERTDistillationEvaluator/","title":"ColBERTDistillationEvaluator","text":"<p>ColBERT Distillation Evaluator. This class is used to monitor the distillation process of a ColBERT model.</p>"},{"location":"api/evaluation/ColBERTDistillationEvaluator/#parameters","title":"Parameters","text":"<ul> <li> <p>queries ('list[str]')</p> <p>Set of queries.</p> </li> <li> <p>documents ('list[list[str]]')</p> <p>Set of documents. Each query has a list of documents. Each document is a list of strings. Number of documents should be the same for each query.</p> </li> <li> <p>scores ('list[list[float]]')</p> <p>The scores associated with the documents. Each query / documents pairs has a list of scores.</p> </li> <li> <p>name ('str') \u2013 defaults to ``</p> <p>The name of the evaluator.</p> </li> <li> <p>batch_size ('int') \u2013 defaults to <code>16</code></p> <p>The batch size.</p> </li> <li> <p>show_progress_bar ('bool') \u2013 defaults to <code>False</code></p> <p>Whether to show the progress bar.</p> </li> <li> <p>write_csv ('bool') \u2013 defaults to <code>True</code></p> <p>Whether to write the results to a CSV file.</p> </li> <li> <p>truncate_dim ('int | None') \u2013 defaults to <code>None</code></p> <p>The dimension to truncate the embeddings.</p> </li> <li> <p>normalize_scores ('bool') \u2013 defaults to <code>True</code></p> </li> </ul>"},{"location":"api/evaluation/ColBERTDistillationEvaluator/#attributes","title":"Attributes","text":"<ul> <li> <p>description</p> <p>Returns a human-readable description of the evaluator: BinaryClassificationEvaluator -&gt; Binary Classification  1. Replace \"CE\" prefix with \"CrossEncoder\" 2. Remove \"Evaluator\" from the class name 3. Add a space before every capital letter</p> </li> </ul>"},{"location":"api/evaluation/ColBERTDistillationEvaluator/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from pylate import models, evaluation\n\n&gt;&gt;&gt; model = models.ColBERT(\n...     model_name_or_path=\"sentence-transformers/all-MiniLM-L6-v2\", device=\"cpu\"\n... )\n\n&gt;&gt;&gt; queries = [\n...     \"query A\",\n...     \"query B\",\n... ]\n\n&gt;&gt;&gt; documents = [\n...     [\"document A\", \"document B\", \"document C\"],\n...     [\"document C C\", \"document B B\", \"document A A\"],\n... ]\n\n&gt;&gt;&gt; scores = [\n...     [0.9, 0.1, 0.05],\n...     [0.05, 0.9, 0.1],\n... ]\n\n&gt;&gt;&gt; distillation_evaluator = evaluation.ColBERTDistillationEvaluator(\n...     queries=queries,\n...     documents=documents,\n...     scores=scores,\n...     write_csv=True,\n... )\n\n&gt;&gt;&gt; results = distillation_evaluator(model=model, output_path=\".\")\n\n&gt;&gt;&gt; assert \"kl_divergence\" in results\n&gt;&gt;&gt; assert isinstance(results[\"kl_divergence\"], float)\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; df = pd.read_csv(distillation_evaluator.csv_file)\n&gt;&gt;&gt; assert df.columns.tolist() == distillation_evaluator.csv_headers\n</code></pre>"},{"location":"api/evaluation/ColBERTDistillationEvaluator/#methods","title":"Methods","text":"call <p>This is called during training to evaluate the model. It returns a score for the evaluation with a higher score indicating a better result.</p> <p>Args:     model: the model to evaluate     output_path: path where predictions and metrics are written         to     epoch: the epoch where the evaluation takes place. This is         used for the file prefixes. If this is -1, then we         assume evaluation on test data.     steps: the steps in the current epoch at time of the         evaluation. This is used for the file prefixes. If this         is -1, then we assume evaluation at the end of the         epoch.  Returns:     Either a score for the evaluation with a higher score     indicating a better result, or a dictionary with scores. If     the latter is chosen, then <code>evaluator.primary_metric</code> must     be defined</p> <p>Parameters</p> <ul> <li>model     (\"'SentenceTransformer'\")    </li> <li>output_path     ('str')     \u2013 defaults to <code>None</code> </li> <li>epoch     ('int')     \u2013 defaults to <code>-1</code> </li> <li>steps     ('int')     \u2013 defaults to <code>-1</code> </li> </ul> embed_inputs <p>Call the encoder method of the model pass</p> <p>Args:     model (SentenceTransformer): Model we are evaluating     sentences (str | list[str] | np.ndarray): Text that we are embedding  Returns:     list[Tensor] | np.ndarray | Tensor | dict[str, Tensor] | list[dict[str, Tensor]]: The associated embedding</p> <p>Parameters</p> <ul> <li>model     ('SentenceTransformer')    </li> <li>sentences     ('str | list[str] | np.ndarray')    </li> <li>kwargs </li> </ul> get_config_dict <p>Return a dictionary with all meaningful configuration values of the evaluator to store in the model card.</p> prefix_name_to_metrics store_metrics_in_model_card_data"},{"location":"api/evaluation/ColBERTTripletEvaluator/","title":"ColBERTTripletEvaluator","text":"<p>Evaluate a model based on a set of triples. The evaluation will compare the score between the anchor and the positive sample with the score between the anchor and the negative sample. The accuracy is computed as the number of times the score between the anchor and the positive sample is higher than the score between the anchor and the negative sample.</p>"},{"location":"api/evaluation/ColBERTTripletEvaluator/#parameters","title":"Parameters","text":"<ul> <li> <p>anchors ('list[str]')</p> <p>Sentences to check similarity to. (e.g. a query)</p> </li> <li> <p>positives ('list[str]')</p> <p>List of positive sentences</p> </li> <li> <p>negatives ('list[str]')</p> <p>List of negative sentences</p> </li> <li> <p>name ('str') \u2013 defaults to ``</p> <p>Name for the output.</p> </li> <li> <p>batch_size ('int') \u2013 defaults to <code>32</code></p> <p>Batch size used to compute embeddings.</p> </li> <li> <p>show_progress_bar ('bool') \u2013 defaults to <code>False</code></p> <p>If true, prints a progress bar.</p> </li> <li> <p>write_csv ('bool') \u2013 defaults to <code>True</code></p> <p>Whether or not to write results to a CSV file.</p> </li> <li> <p>truncate_dim ('int | None') \u2013 defaults to <code>None</code></p> <p>The dimension to truncate sentence embeddings to. If None, do not truncate.</p> </li> </ul>"},{"location":"api/evaluation/ColBERTTripletEvaluator/#attributes","title":"Attributes","text":"<ul> <li> <p>description</p> <p>Returns a human-readable description of the evaluator: BinaryClassificationEvaluator -&gt; Binary Classification  1. Replace \"CE\" prefix with \"CrossEncoder\" 2. Remove \"Evaluator\" from the class name 3. Add a space before every capital letter</p> </li> </ul>"},{"location":"api/evaluation/ColBERTTripletEvaluator/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from pylate import evaluation, models\n\n&gt;&gt;&gt; model = models.ColBERT(\n...     model_name_or_path=\"sentence-transformers/all-MiniLM-L6-v2\",\n...     device=\"cpu\",\n... )\n\n&gt;&gt;&gt; anchors = [\n...     \"fruits are healthy.\",\n...     \"fruits are healthy.\",\n... ]\n\n&gt;&gt;&gt; positives = [\n...     \"fruits are good for health.\",\n...     \"Fruits are growing in the trees.\",\n... ]\n\n&gt;&gt;&gt; negatives = [\n...     \"Fruits are growing in the trees.\",\n...     \"fruits are good for health.\",\n... ]\n\n&gt;&gt;&gt; triplet_evaluation = evaluation.ColBERTTripletEvaluator(\n...     anchors=anchors,\n...     positives=positives,\n...     negatives=negatives,\n...     write_csv=True,\n... )\n\n&gt;&gt;&gt; results = triplet_evaluation(model=model, output_path=\".\")\n\n&gt;&gt;&gt; results\n{'accuracy': 0.5}\n\n&gt;&gt;&gt; triplet_evaluation.csv_headers\n['epoch', 'steps', 'accuracy']\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; df = pd.read_csv(triplet_evaluation.csv_file)\n&gt;&gt;&gt; assert df.columns.tolist() == triplet_evaluation.csv_headers\n</code></pre>"},{"location":"api/evaluation/ColBERTTripletEvaluator/#methods","title":"Methods","text":"call <p>Evaluate the model on the triplet dataset. Measure the scoring between the anchor and the positive with every other positive and negative samples using HITS@K.</p> <p>Parameters</p> <ul> <li>model     ('ColBERT')    </li> <li>output_path     ('str')     \u2013 defaults to <code>None</code> </li> <li>epoch     ('int')     \u2013 defaults to <code>-1</code> </li> <li>steps     ('int')     \u2013 defaults to <code>-1</code> </li> </ul> embed_inputs <p>Call the encoder method of the model pass</p> <p>Args:     model (SentenceTransformer): Model we are evaluating     sentences (str | list[str] | np.ndarray): Text that we are embedding  Returns:     list[Tensor] | np.ndarray | Tensor | dict[str, Tensor] | list[dict[str, Tensor]]: The associated embedding</p> <p>Parameters</p> <ul> <li>model     ('SentenceTransformer')    </li> <li>sentences     ('str | list[str] | np.ndarray')    </li> <li>kwargs </li> </ul> from_input_examples get_config_dict <p>Return a dictionary with all meaningful configuration values of the evaluator to store in the model card.</p> prefix_name_to_metrics store_metrics_in_model_card_data"},{"location":"api/evaluation/NanoBEIREvaluator/","title":"NanoBEIREvaluator","text":"<p>Evaluate the performance of a PyLate Model on the NanoBEIR collection.</p> <p>This is a direct extension of the NanoBEIREvaluator from the sentence-transformers library, leveraging the PyLateInformationRetrievalEvaluator class. The collection is a set of datasets based on the BEIR collection, but with a significantly smaller size, so it can be used for quickly evaluating the retrieval performance of a model before committing to a full evaluation. The Evaluator will return the same metrics as the InformationRetrievalEvaluator (i.e., MRR, nDCG, Recall@k), for each dataset and on average.</p>"},{"location":"api/evaluation/NanoBEIREvaluator/#parameters","title":"Parameters","text":"<ul> <li> <p>dataset_names ('list[DatasetNameType] | None') \u2013 defaults to <code>None</code></p> </li> <li> <p>mrr_at_k ('list[int]') \u2013 defaults to <code>[10]</code></p> </li> <li> <p>ndcg_at_k ('list[int]') \u2013 defaults to <code>[10]</code></p> </li> <li> <p>accuracy_at_k ('list[int]') \u2013 defaults to <code>[1, 3, 5, 10]</code></p> </li> <li> <p>precision_recall_at_k ('list[int]') \u2013 defaults to <code>[1, 3, 5, 10]</code></p> </li> <li> <p>map_at_k ('list[int]') \u2013 defaults to <code>[100]</code></p> </li> <li> <p>show_progress_bar ('bool') \u2013 defaults to <code>False</code></p> </li> <li> <p>batch_size ('int') \u2013 defaults to <code>32</code></p> </li> <li> <p>write_csv ('bool') \u2013 defaults to <code>True</code></p> </li> <li> <p>truncate_dim ('int | None') \u2013 defaults to <code>None</code></p> </li> <li> <p>score_functions ('dict[str, Callable[[Tensor, Tensor], Tensor]] | None') \u2013 defaults to <code>None</code></p> </li> <li> <p>main_score_function ('str | SimilarityFunction | None') \u2013 defaults to <code>None</code></p> </li> <li> <p>aggregate_fn ('Callable[[list[float]], float]') \u2013 defaults to <code>&lt;function mean at 0x7f2005569090&gt;</code></p> </li> <li> <p>aggregate_key ('str') \u2013 defaults to <code>mean</code></p> </li> <li> <p>query_prompts ('str | dict[str, str] | None') \u2013 defaults to <code>None</code></p> </li> <li> <p>corpus_prompts ('str | dict[str, str] | None') \u2013 defaults to <code>None</code></p> </li> <li> <p>write_predictions ('bool') \u2013 defaults to <code>False</code></p> </li> </ul>"},{"location":"api/evaluation/NanoBEIREvaluator/#attributes","title":"Attributes","text":"<ul> <li> <p>description</p> <p>Returns a human-readable description of the evaluator: BinaryClassificationEvaluator -&gt; Binary Classification  1. Replace \"CE\" prefix with \"CrossEncoder\" 2. Remove \"Evaluator\" from the class name 3. Add a space before every capital letter</p> </li> </ul>"},{"location":"api/evaluation/NanoBEIREvaluator/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from pylate import models, evaluation\n\n&gt;&gt;&gt; model = models.ColBERT(\n...     model_name_or_path=\"lightonai/colbertv2.0\"\n... )\n\n&gt;&gt;&gt; datasets = [\"SciFact\"]\n\n&gt;&gt;&gt; evaluator = evaluation.NanoBEIREvaluator(\n...    dataset_names=datasets\n... )\n</code></pre> <p>evaluator(model) {'NanoSciFact_MaxSim_accuracy@1': 0.62, 'NanoSciFact_MaxSim_accuracy@3': 0.74, 'NanoSciFact_MaxSim_accuracy@5': 0.8, 'NanoSciFact_MaxSim_accuracy@10': 0.86, 'NanoSciFact_MaxSim_precision@1': 0.62, 'NanoSciFact_MaxSim_precision@3': 0.26666666666666666, 'NanoSciFact_MaxSim_precision@5': 0.18, 'NanoSciFact_MaxSim_precision@10': 0.096, 'NanoSciFact_MaxSim_recall@1': 0.595, 'NanoSciFact_MaxSim_recall@3': 0.715, 'NanoSciFact_MaxSim_recall@5': 0.79, 'NanoSciFact_MaxSim_recall@10': 0.85, 'NanoSciFact_MaxSim_ndcg@10': 0.7279903941189909, 'NanoSciFact_MaxSim_mrr@10': 0.6912222222222222, 'NanoSciFact_MaxSim_map@100': 0.6903374780806633, 'NanoBEIR_mean_MaxSim_accuracy@1': 0.62, 'NanoBEIR_mean_MaxSim_accuracy@3': 0.74, 'NanoBEIR_mean_MaxSim_accuracy@5': 0.8, 'NanoBEIR_mean_MaxSim_accuracy@10': 0.86, 'NanoBEIR_mean_MaxSim_precision@1': 0.62, 'NanoBEIR_mean_MaxSim_precision@3': 0.26666666666666666, 'NanoBEIR_mean_MaxSim_precision@5': 0.18, 'NanoBEIR_mean_MaxSim_precision@10': 0.096, 'NanoBEIR_mean_MaxSim_recall@1': 0.595, 'NanoBEIR_mean_MaxSim_recall@3': 0.715, 'NanoBEIR_mean_MaxSim_recall@5': 0.79, 'NanoBEIR_mean_MaxSim_recall@10': 0.85, 'NanoBEIR_mean_MaxSim_ndcg@10': 0.7279903941189909, 'NanoBEIR_mean_MaxSim_mrr@10': 0.6912222222222222, 'NanoBEIR_mean_MaxSim_map@100': 0.6903374780806633}</p>"},{"location":"api/evaluation/NanoBEIREvaluator/#methods","title":"Methods","text":"call <p>This is called during training to evaluate the model. It returns a score for the evaluation with a higher score indicating a better result.</p> <p>Args:     model: the model to evaluate     output_path: path where predictions and metrics are written         to     epoch: the epoch where the evaluation takes place. This is         used for the file prefixes. If this is -1, then we         assume evaluation on test data.     steps: the steps in the current epoch at time of the         evaluation. This is used for the file prefixes. If this         is -1, then we assume evaluation at the end of the         epoch.  Returns:     Either a score for the evaluation with a higher score     indicating a better result, or a dictionary with scores. If     the latter is chosen, then <code>evaluator.primary_metric</code> must     be defined</p> <p>Parameters</p> <ul> <li>model     ('SentenceTransformer')    </li> <li>output_path     ('str | None')     \u2013 defaults to <code>None</code> </li> <li>epoch     ('int')     \u2013 defaults to <code>-1</code> </li> <li>steps     ('int')     \u2013 defaults to <code>-1</code> </li> <li>args </li> <li>kwargs </li> </ul> embed_inputs <p>Call the encoder method of the model pass</p> <p>Args:     model (SentenceTransformer): Model we are evaluating     sentences (str | list[str] | np.ndarray): Text that we are embedding  Returns:     list[Tensor] | np.ndarray | Tensor | dict[str, Tensor] | list[dict[str, Tensor]]: The associated embedding</p> <p>Parameters</p> <ul> <li>model     ('SentenceTransformer')    </li> <li>sentences     ('str | list[str] | np.ndarray')    </li> <li>kwargs </li> </ul> get_config_dict <p>Return a dictionary with all meaningful configuration values of the evaluator to store in the model card.</p> information_retrieval_class <p>This class evaluates an Information Retrieval (IR) setting.</p> <p>Given a set of queries and a large corpus set. It will retrieve for each query the top-k most similar document. It measures Mean Reciprocal Rank (MRR), Recall@k, and Normalized Discounted Cumulative Gain (NDCG)  Args:     queries (Dict[str, str]): A dictionary mapping query IDs to queries.     corpus (Dict[str, str]): A dictionary mapping document IDs to documents.     relevant_docs (Dict[str, Set[str]]): A dictionary mapping query IDs to a set of relevant document IDs.     corpus_chunk_size (int): The size of each chunk of the corpus. Defaults to 50000.     mrr_at_k (List[int]): A list of integers representing the values of k for MRR calculation. Defaults to [10].     ndcg_at_k (List[int]): A list of integers representing the values of k for NDCG calculation. Defaults to [10].     accuracy_at_k (List[int]): A list of integers representing the values of k for accuracy calculation. Defaults to [1, 3, 5, 10].     precision_recall_at_k (List[int]): A list of integers representing the values of k for precision and recall calculation. Defaults to [1, 3, 5, 10].     map_at_k (List[int]): A list of integers representing the values of k for MAP calculation. Defaults to [100].     show_progress_bar (bool): Whether to show a progress bar during evaluation. Defaults to False.     batch_size (int): The batch size for evaluation. Defaults to 32.     name (str): A name for the evaluation. Defaults to \"\".     write_csv (bool): Whether to write the evaluation results to a CSV file. Defaults to True.     truncate_dim (int, optional): The dimension to truncate the embeddings to. Defaults to None.     score_functions (Dict[str, Callable[[Tensor, Tensor], Tensor]]): A dictionary mapping score function names to score functions. Defaults to the <code>similarity</code> function from the <code>model</code>.     main_score_function (Union[str, SimilarityFunction], optional): The main score function to use for evaluation. Defaults to None.     query_prompt (str, optional): The prompt to be used when encoding the corpus. Defaults to None.     query_prompt_name (str, optional): The name of the prompt to be used when encoding the corpus. Defaults to None.     corpus_prompt (str, optional): The prompt to be used when encoding the corpus. Defaults to None.     corpus_prompt_name (str, optional): The name of the prompt to be used when encoding the corpus. Defaults to None.     write_predictions (bool): Whether to write the predictions to a JSONL file. Defaults to False.         This can be useful for downstream evaluation as it can be used as input to the :class:<code>~sentence_transformers.sparse_encoder.evaluation.ReciprocalRankFusionEvaluator</code> that accept precomputed predictions.  Example:     ::          import random         from sentence_transformers import SentenceTransformer         from sentence_transformers.evaluation import InformationRetrievalEvaluator         from datasets import load_dataset          # Load a model         model = SentenceTransformer('all-MiniLM-L6-v2')          # Load the Touche-2020 IR dataset (https://huggingface.co/datasets/BeIR/webis-touche2020, https://huggingface.co/datasets/BeIR/webis-touche2020-qrels)         corpus = load_dataset(\"BeIR/webis-touche2020\", \"corpus\", split=\"corpus\")         queries = load_dataset(\"BeIR/webis-touche2020\", \"queries\", split=\"queries\")         relevant_docs_data = load_dataset(\"BeIR/webis-touche2020-qrels\", split=\"test\")          # For this dataset, we want to concatenate the title and texts for the corpus         corpus = corpus.map(lambda x: {'text': x['title'] + \" \" + x['text']}, remove_columns=['title'])          # Shrink the corpus size heavily to only the relevant documents + 30,000 random documents         required_corpus_ids = set(map(str, relevant_docs_data[\"corpus-id\"]))         required_corpus_ids |= set(random.sample(corpus[\"_id\"], k=30_000))         corpus = corpus.filter(lambda x: x[\"_id\"] in required_corpus_ids)          # Convert the datasets to dictionaries         corpus = dict(zip(corpus[\"_id\"], corpus[\"text\"]))  # Our corpus (cid =&gt; document)         queries = dict(zip(queries[\"_id\"], queries[\"text\"]))  # Our queries (qid =&gt; question)         relevant_docs = {}  # Query ID to relevant documents (qid =&gt; set([relevant_cids])         for qid, corpus_ids in zip(relevant_docs_data[\"query-id\"], relevant_docs_data[\"corpus-id\"]):             qid = str(qid)             corpus_ids = str(corpus_ids)             if qid not in relevant_docs:                 relevant_docs[qid] = set()             relevant_docs[qid].add(corpus_ids)          # Given queries, a corpus and a mapping with relevant documents, the InformationRetrievalEvaluator computes different IR metrics.         ir_evaluator = InformationRetrievalEvaluator(             queries=queries,             corpus=corpus,             relevant_docs=relevant_docs,             name=\"BeIR-touche2020-subset-test\",         )         results = ir_evaluator(model)         '''         Information Retrieval Evaluation of the model on the BeIR-touche2020-test dataset:         Queries: 49         Corpus: 31923          Score-Function: cosine         Accuracy@1: 77.55%         Accuracy@3: 93.88%         Accuracy@5: 97.96%         Accuracy@10: 100.00%         Precision@1: 77.55%         Precision@3: 72.11%         Precision@5: 71.43%         Precision@10: 62.65%         Recall@1: 1.72%         Recall@3: 4.78%         Recall@5: 7.90%         Recall@10: 13.86%         MRR@10: 0.8580         NDCG@10: 0.6606         MAP@100: 0.2934         '''         print(ir_evaluator.primary_metric)         # =&gt; \"BeIR-touche2020-test_cosine_map@100\"         print(results[ir_evaluator.primary_metric])         # =&gt; 0.29335196224364596</p> <p>Parameters</p> <ul> <li>queries     ('dict[str, str]')    </li> <li>corpus     ('dict[str, str]')    </li> <li>relevant_docs     ('dict[str, set[str]]')    </li> <li>corpus_chunk_size     ('int')     \u2013 defaults to <code>50000</code> </li> <li>mrr_at_k     ('list[int]')     \u2013 defaults to <code>[10]</code> </li> <li>ndcg_at_k     ('list[int]')     \u2013 defaults to <code>[10]</code> </li> <li>accuracy_at_k     ('list[int]')     \u2013 defaults to <code>[1, 3, 5, 10]</code> </li> <li>precision_recall_at_k     ('list[int]')     \u2013 defaults to <code>[1, 3, 5, 10]</code> </li> <li>map_at_k     ('list[int]')     \u2013 defaults to <code>[100]</code> </li> <li>show_progress_bar     ('bool')     \u2013 defaults to <code>False</code> </li> <li>batch_size     ('int')     \u2013 defaults to <code>32</code> </li> <li>name     ('str')     \u2013 defaults to ``    </li> <li>write_csv     ('bool')     \u2013 defaults to <code>True</code> </li> <li>truncate_dim     ('int | None')     \u2013 defaults to <code>None</code> </li> <li>score_functions     ('dict[str, Callable[[Tensor, Tensor], Tensor]] | None')     \u2013 defaults to <code>None</code> </li> <li>main_score_function     ('str | SimilarityFunction | None')     \u2013 defaults to <code>None</code> </li> <li>query_prompt     ('str | None')     \u2013 defaults to <code>None</code> </li> <li>query_prompt_name     ('str | None')     \u2013 defaults to <code>None</code> </li> <li>corpus_prompt     ('str | None')     \u2013 defaults to <code>None</code> </li> <li>corpus_prompt_name     ('str | None')     \u2013 defaults to <code>None</code> </li> <li>write_predictions     ('bool')     \u2013 defaults to <code>False</code> </li> </ul> prefix_name_to_metrics store_metrics_in_model_card_data"},{"location":"api/evaluation/NanoBEIREvaluator/#references","title":"References","text":"<ul> <li>NanoBEIR</li> </ul>"},{"location":"api/evaluation/PyLateInformationRetrievalEvaluator/","title":"PyLateInformationRetrievalEvaluator","text":"<p>This class evaluates an Information Retrieval (IR) setting. This is a direct extension of the InformationRetrievalEvaluator from the sentence-transformers library, only override the compute_metrices method to be compilatible with PyLate models (define asymmetric encoding using is_query params and add padding).</p>"},{"location":"api/evaluation/PyLateInformationRetrievalEvaluator/#parameters","title":"Parameters","text":"<ul> <li> <p>queries ('dict[str, str]')</p> </li> <li> <p>corpus ('dict[str, str]')</p> </li> <li> <p>relevant_docs ('dict[str, set[str]]')</p> </li> <li> <p>corpus_chunk_size ('int') \u2013 defaults to <code>50000</code></p> </li> <li> <p>mrr_at_k ('list[int]') \u2013 defaults to <code>[10]</code></p> </li> <li> <p>ndcg_at_k ('list[int]') \u2013 defaults to <code>[10]</code></p> </li> <li> <p>accuracy_at_k ('list[int]') \u2013 defaults to <code>[1, 3, 5, 10]</code></p> </li> <li> <p>precision_recall_at_k ('list[int]') \u2013 defaults to <code>[1, 3, 5, 10]</code></p> </li> <li> <p>map_at_k ('list[int]') \u2013 defaults to <code>[100]</code></p> </li> <li> <p>show_progress_bar ('bool') \u2013 defaults to <code>False</code></p> </li> <li> <p>batch_size ('int') \u2013 defaults to <code>32</code></p> </li> <li> <p>name ('str') \u2013 defaults to ``</p> </li> <li> <p>write_csv ('bool') \u2013 defaults to <code>True</code></p> </li> <li> <p>truncate_dim ('int | None') \u2013 defaults to <code>None</code></p> </li> <li> <p>score_functions ('dict[str, Callable[[Tensor, Tensor], Tensor]] | None') \u2013 defaults to <code>None</code></p> </li> <li> <p>main_score_function ('str | SimilarityFunction | None') \u2013 defaults to <code>None</code></p> </li> <li> <p>query_prompt ('str | None') \u2013 defaults to <code>None</code></p> </li> <li> <p>query_prompt_name ('str | None') \u2013 defaults to <code>None</code></p> </li> <li> <p>corpus_prompt ('str | None') \u2013 defaults to <code>None</code></p> </li> <li> <p>corpus_prompt_name ('str | None') \u2013 defaults to <code>None</code></p> </li> <li> <p>write_predictions ('bool') \u2013 defaults to <code>False</code></p> </li> </ul>"},{"location":"api/evaluation/PyLateInformationRetrievalEvaluator/#attributes","title":"Attributes","text":"<ul> <li> <p>description</p> <p>Returns a human-readable description of the evaluator: BinaryClassificationEvaluator -&gt; Binary Classification  1. Replace \"CE\" prefix with \"CrossEncoder\" 2. Remove \"Evaluator\" from the class name 3. Add a space before every capital letter</p> </li> </ul>"},{"location":"api/evaluation/PyLateInformationRetrievalEvaluator/#methods","title":"Methods","text":"call <p>This is called during training to evaluate the model. It returns a score for the evaluation with a higher score indicating a better result.</p> <p>Args:     model: the model to evaluate     output_path: path where predictions and metrics are written         to     epoch: the epoch where the evaluation takes place. This is         used for the file prefixes. If this is -1, then we         assume evaluation on test data.     steps: the steps in the current epoch at time of the         evaluation. This is used for the file prefixes. If this         is -1, then we assume evaluation at the end of the         epoch.  Returns:     Either a score for the evaluation with a higher score     indicating a better result, or a dictionary with scores. If     the latter is chosen, then <code>evaluator.primary_metric</code> must     be defined</p> <p>Parameters</p> <ul> <li>model     ('SentenceTransformer')    </li> <li>output_path     ('str | None')     \u2013 defaults to <code>None</code> </li> <li>epoch     ('int')     \u2013 defaults to <code>-1</code> </li> <li>steps     ('int')     \u2013 defaults to <code>-1</code> </li> <li>args </li> <li>kwargs </li> </ul> compute_dcg_at_k compute_metrices compute_metrics embed_inputs <p>Call the encoder method of the model pass</p> <p>Args:     model (SentenceTransformer): Model we are evaluating     sentences (str | list[str] | np.ndarray): Text that we are embedding  Returns:     list[Tensor] | np.ndarray | Tensor | dict[str, Tensor] | list[dict[str, Tensor]]: The associated embedding</p> <p>Parameters</p> <ul> <li>model     ('SentenceTransformer')    </li> <li>sentences     ('str | list[str] | np.ndarray')    </li> <li>encode_fn_name     ('str | None')     \u2013 defaults to <code>None</code> </li> <li>prompt_name     ('str | None')     \u2013 defaults to <code>None</code> </li> <li>prompt     ('str | None')     \u2013 defaults to <code>None</code> </li> <li>kwargs </li> </ul> get_config_dict <p>Return a dictionary with all meaningful configuration values of the evaluator to store in the model card.</p> output_scores prefix_name_to_metrics store_metrics_in_model_card_data"},{"location":"api/evaluation/evaluate/","title":"evaluate","text":"<p>Evaluate candidates matches.</p>"},{"location":"api/evaluation/evaluate/#parameters","title":"Parameters","text":"<ul> <li> <p>scores ('list[list[dict]]')</p> <p>Scores of the retrieval model.</p> </li> <li> <p>qrels ('dict')</p> <p>Qrels.</p> </li> <li> <p>queries ('list[str]')</p> <p>index of queries of qrels.</p> </li> <li> <p>metrics ('list | None') \u2013 defaults to <code>None</code></p> <p>Metrics to compute.</p> </li> </ul>"},{"location":"api/evaluation/evaluate/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from pylate import evaluation\n\n&gt;&gt;&gt; scores = [\n...     [{\"id\": \"1\", \"score\": 0.9}, {\"id\": \"2\", \"score\": 0.8}],\n...     [{\"id\": \"3\", \"score\": 0.7}, {\"id\": \"4\", \"score\": 0.6}],\n... ]\n\n&gt;&gt;&gt; qrels = {\n...     \"query1\": {\"1\": True, \"2\": True},\n...     \"query2\": {\"3\": True, \"4\": True},\n... }\n\n&gt;&gt;&gt; queries = [\"query1\", \"query2\"]\n\n&gt;&gt;&gt; results = evaluation.evaluate(\n...     scores=scores,\n...     qrels=qrels,\n...     queries=queries,\n...     metrics=[\"ndcg@10\", \"hits@1\"],\n... )\n</code></pre>"},{"location":"api/evaluation/get-beir-triples/","title":"get_beir_triples","text":"<p>Build BEIR triples.</p>"},{"location":"api/evaluation/get-beir-triples/#parameters","title":"Parameters","text":"<ul> <li> <p>documents ('list')</p> <p>Documents.</p> </li> <li> <p>queries ('list[str]')</p> <p>Queries.</p> </li> <li> <p>qrels ('dict')</p> </li> </ul>"},{"location":"api/evaluation/get-beir-triples/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from pylate import evaluation\n\n&gt;&gt;&gt; documents, queries, qrels = evaluation.load_beir(\n...     \"scifact\",\n...     split=\"test\",\n... )\n\n&gt;&gt;&gt; triples = evaluation.get_beir_triples(\n...     documents=documents,\n...     queries=queries,\n...     qrels=qrels\n... )\n\n&gt;&gt;&gt; len(triples)\n339\n</code></pre>"},{"location":"api/evaluation/load-beir/","title":"load_beir","text":"<p>Load BEIR dataset.</p>"},{"location":"api/evaluation/load-beir/#parameters","title":"Parameters","text":"<ul> <li> <p>dataset_name ('str')</p> <p>Name of the beir dataset.</p> </li> <li> <p>split ('str') \u2013 defaults to <code>test</code></p> <p>Split to load.</p> </li> </ul>"},{"location":"api/evaluation/load-beir/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from pylate import evaluation\n\n&gt;&gt;&gt; documents, queries, qrels = evaluation.load_beir(\n...     \"scifact\",\n...     split=\"test\",\n... )\n\n&gt;&gt;&gt; len(documents)\n5183\n\n&gt;&gt;&gt; len(queries)\n300\n\n&gt;&gt;&gt; len(qrels)\n300\n</code></pre>"},{"location":"api/evaluation/load-custom-dataset/","title":"load_custom_dataset","text":"<p>Load a custom dataset.</p>"},{"location":"api/evaluation/load-custom-dataset/#parameters","title":"Parameters","text":"<ul> <li> <p>path ('str')</p> <p>Path of the dataset.</p> </li> <li> <p>split ('str') \u2013 defaults to <code>test</code></p> <p>Split to load.</p> </li> </ul>"},{"location":"api/hf-hub/PylateModelCardData/","title":"PylateModelCardData","text":"<p>A dataclass for storing data used in the model card.</p>"},{"location":"api/hf-hub/PylateModelCardData/#parameters","title":"Parameters","text":"<ul> <li> <p>language ('str | list[str] | None') \u2013 defaults to <code>&lt;factory&gt;</code></p> <p>The model language, either a string or a list of strings, e.g., \"en\" or [\"en\", \"de\", \"nl\"].</p> </li> <li> <p>license ('str | None') \u2013 defaults to <code>None</code></p> <p>The license of the model, e.g., \"apache-2.0\", \"mit\", or \"cc-by-nc-sa-4.0\".</p> </li> <li> <p>model_name ('str | None') \u2013 defaults to <code>None</code></p> <p>The pretty name of the model, e.g., \"SentenceTransformer based on microsoft/mpnet-base\".</p> </li> <li> <p>model_id ('str | None') \u2013 defaults to <code>None</code></p> <p>The model ID for pushing the model to the Hub, e.g., \"tomaarsen/sbert-mpnet-base-allnli\".</p> </li> <li> <p>train_datasets ('list[dict[str, str]]') \u2013 defaults to <code>&lt;factory&gt;</code></p> <p>A list of dictionaries containing names and/or Hugging Face dataset IDs for training datasets, e.g., [{\"name\": \"SNLI\", \"id\": \"stanfordnlp/snli\"}, {\"name\": \"MultiNLI\", \"id\": \"nyu-mll/multi_nli\"}, {\"name\": \"STSB\"}].</p> </li> <li> <p>eval_datasets ('list[dict[str, str]]') \u2013 defaults to <code>&lt;factory&gt;</code></p> <p>A list of dictionaries containing names and/or Hugging Face dataset IDs for evaluation datasets, e.g., [{\"name\": \"SNLI\", \"id\": \"stanfordnlp/snli\"}, {\"id\": \"mteb/stsbenchmark-sts\"}].</p> </li> <li> <p>task_name ('str') \u2013 defaults to <code>semantic textual similarity, semantic search, paraphrase mining, text classification, clustering, and more</code></p> <p>The human-readable task the model is trained on, e.g., \"semantic textual similarity, semantic search, paraphrase mining, text classification, clustering, and more\".</p> </li> <li> <p>tags ('list[str] | None') \u2013 defaults to <code>&lt;factory&gt;</code></p> <p>A list of tags for the model, e.g., [\"sentence-transformers\", \"sentence-similarity\", \"feature-extraction\"].</p> </li> <li> <p>local_files_only ('bool') \u2013 defaults to <code>False</code></p> </li> <li> <p>generate_widget_examples (\"Literal['deprecated']\") \u2013 defaults to <code>deprecated</code></p> </li> </ul>"},{"location":"api/hf-hub/PylateModelCardData/#attributes","title":"Attributes","text":"<ul> <li> <p>base_model</p> </li> <li> <p>base_model_revision</p> </li> <li> <p>best_model_step</p> </li> <li> <p>code_carbon_callback</p> </li> <li> <p>ir_model</p> </li> <li> <p>license</p> </li> <li> <p>model</p> </li> <li> <p>model_id</p> </li> <li> <p>model_name</p> </li> <li> <p>predict_example</p> </li> <li> <p>similarities</p> </li> <li> <p>trainer</p> </li> </ul>"},{"location":"api/hf-hub/PylateModelCardData/#methods","title":"Methods","text":"add_tags compute_dataset_metrics <p>Given a dataset, compute the following: * Dataset Size * Dataset Columns * Dataset Stats     - Strings: min, mean, max word count/token length     - Integers: Counter() instance     - Floats: min, mean, max range     - List: number of elements or min, mean, max number of elements * 3 Example samples * Loss function name     - Loss function config</p> <p>Parameters</p> <ul> <li>dataset     ('Dataset | IterableDataset | None')    </li> <li>dataset_info     ('dict[str, Any]')    </li> <li>loss     ('dict[str, nn.Module] | nn.Module | None')    </li> </ul> extract_dataset_metadata format_eval_metrics <p>Format the evaluation metrics for the model card.</p> <p>The following keys will be returned: - eval_metrics: A list of dictionaries containing the class name, description, dataset name, and a markdown table   This is used to display the evaluation metrics in the model card. - metrics: A list of all metric keys. This is used in the model card metadata. - model-index: A list of dictionaries containing the task name, task type, dataset type, dataset name, metric name,   metric type, and metric value. This is used to display the evaluation metrics in the model card metadata.</p> format_training_logs get <p>Get value for a given metadata key.</p> <p>Parameters</p> <ul> <li>key     (str)    </li> <li>default     (Any)     \u2013 defaults to <code>None</code> </li> </ul> get_codecarbon_data get_default_model_name get_model_specific_metadata infer_datasets pop <p>Pop value for a given metadata key.</p> <p>Parameters</p> <ul> <li>key     (str)    </li> <li>default     (Any)     \u2013 defaults to <code>None</code> </li> </ul> register_model run_usage_snippet set_base_model set_best_model_step set_evaluation_metrics set_label_examples set_language set_license set_losses set_model_id set_widget_examples <p>A function to create widget examples from a dataset. For now, set_widget_examples is not compatible with our transform/map operations, so we make it a no-op until it is fixed</p> <p>Parameters</p> <ul> <li>dataset     ('Dataset | DatasetDict')    </li> </ul> to_dict <p>Converts CardData to a dict.</p> <p>Returns:     <code>dict</code>: CardData represented as a dictionary ready to be dumped to a YAML     block for inclusion in a README.md file.</p> to_yaml <p>Dumps CardData to a YAML block for inclusion in a README.md file.</p> <p>Args:     line_break (str, optional):         The line break to use when dumping to yaml.  Returns:     <code>str</code>: CardData represented as a YAML block.</p> <p>Parameters</p> <ul> <li>line_break     \u2013 defaults to <code>None</code> </li> </ul> tokenize try_to_set_base_model validate_datasets <p>Validate (i.e. check if the dataset IDs exist on the Hub) and process a list of dataset dictionaries.</p> <p>Args:     dataset_list (list[dict[str, Any]]): List of dataset metadata dictionaries.     infer_languages (bool | None, optional): Whether to infer languages from the dataset information.         If None (default), languages will be inferred only if <code>self.language</code> is empty.  Returns:     list[dict[str, Any]]: The validated and possibly updated list of dataset dictionaries.</p> <p>Parameters</p> <ul> <li>dataset_list     ('list[dict[str, Any]]')    </li> <li>infer_languages     ('bool | None')     \u2013 defaults to <code>None</code> </li> </ul>"},{"location":"api/indexes/PLAID/","title":"PLAID","text":"<p>PLAID index with choice between fast-plaid (Rust-based) and Stanford NLP backends.</p> <p>This class provides a unified interface for PLAID indexing that can use either: - FastPlaid: High-performance Rust-based implementation (default) - Stanford PLAID: Original Stanford NLP implementation (deprecated)</p>"},{"location":"api/indexes/PLAID/#parameters","title":"Parameters","text":"<ul> <li> <p>index_folder ('str') \u2013 defaults to <code>indexes</code></p> <p>The folder where the index will be stored.</p> </li> <li> <p>index_name ('str') \u2013 defaults to <code>colbert</code></p> <p>The name of the index.</p> </li> <li> <p>override ('bool') \u2013 defaults to <code>False</code></p> <p>Whether to override the collection if it already exists.</p> </li> <li> <p>use_fast ('bool') \u2013 defaults to <code>True</code></p> <p>If True (default), use fast-plaid backend. If False, use Stanford PLAID backend.</p> </li> <li> <p>nbits ('int') \u2013 defaults to <code>4</code></p> <p>The number of bits to use for product quantization. Lower values mean more compression and potentially faster searches but can reduce accuracy.</p> </li> <li> <p>kmeans_niters ('int') \u2013 defaults to <code>4</code></p> <p>The number of iterations for the K-means algorithm used during index creation. This influences the quality of the initial centroid assignments.</p> </li> <li> <p>max_points_per_centroid ('int') \u2013 defaults to <code>256</code></p> <p>The maximum number of points (token embeddings) that can be assigned to a single centroid during K-means. This helps in balancing the clusters.</p> </li> <li> <p>n_ivf_probe ('int') \u2013 defaults to <code>8</code></p> <p>The number of inverted file list \"probes\" to perform during the search. This parameter controls the number of clusters to search within the index for each query. Higher values improve recall but increase search time.</p> </li> <li> <p>n_full_scores ('int') \u2013 defaults to <code>8192</code></p> <p>The number of candidate documents for which full (re-ranked) scores are computed. This is a crucial parameter for accuracy; higher values lead to more accurate results but increase computation.</p> </li> <li> <p>n_samples_kmeans ('int | None') \u2013 defaults to <code>None</code></p> <p>The number of samples to use for K-means clustering. If None, it defaults to a value based on the number of documents. This parameter can be adjusted to balance between speed, memory usage and clustering quality.</p> </li> <li> <p>batch_size ('int') \u2013 defaults to <code>262144</code></p> <p>The internal batch size used for processing queries. A larger batch size might improve throughput on powerful GPUs but can consume more memory.</p> </li> <li> <p>show_progress ('bool') \u2013 defaults to <code>True</code></p> <p>If set to True, a progress bar will be displayed during search operations.</p> </li> <li> <p>device ('str | list[str] | None') \u2013 defaults to <code>None</code></p> <p>Specifies the device(s) to use for computation. If None (default) and CUDA is available, it defaults to \"cuda\". If CUDA is not available, it defaults to \"cpu\". Can be a single device string (e.g., \"cuda:0\" or \"cpu\"). Can be a list of device strings (e.g., [\"cuda:0\", \"cuda:1\"]).</p> </li> <li> <p>use_triton ('bool | None') \u2013 defaults to <code>None</code></p> <p>Whether to use triton kernels when computing kmeans using fast-plaid. Triton kernels are faster, but yields some variance due to race condition, set to false to get 100% reproducible results. If unset, will use triton kernels if possible.</p> </li> <li> <p>kwargs</p> </li> </ul>"},{"location":"api/indexes/PLAID/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from pylate import indexes, models\n\n&gt;&gt;&gt; index = indexes.PLAID(\n...    index_folder=\"test_index\",\n...    index_name=\"plaid_colbert\",\n...    override=True,\n... )\n\n&gt;&gt;&gt; model = models.ColBERT(\n...    model_name_or_path=\"lightonai/GTE-ModernColBERT-v1\",\n... )\n\n&gt;&gt;&gt; documents_embeddings = model.encode([\n...    \"Document content here...\",\n...    \"Another document...\",\n... ] * 10, is_query=False)\n\n&gt;&gt;&gt; index = index.add_documents(\n...    documents_ids=range(len(documents_embeddings)),\n...    documents_embeddings=documents_embeddings\n... )\nComputing centroids of embeddings.\nCreating FastPlaid index.\n\n&gt;&gt;&gt; queries_embeddings = model.encode(\n...     [\"search query\", \"hello world\"],\n...     is_query=True,\n... )\n\n&gt;&gt;&gt; scores = index(\n...     queries_embeddings,\n...     k=10,\n... )\n\n&gt;&gt;&gt; index = index.add_documents(\n...    documents_ids=range(len(documents_embeddings), len(documents_embeddings) * 2),\n...    documents_embeddings=documents_embeddings\n... )\n\n&gt;&gt;&gt; scores = index(\n...     queries_embeddings,\n...     k=25,\n... )\n</code></pre>"},{"location":"api/indexes/PLAID/#methods","title":"Methods","text":"call <p>Query the index for the nearest neighbors of the query embeddings.</p> <p>Parameters</p> <ul> <li>queries_embeddings     ('np.ndarray | torch.Tensor')    </li> <li>k     ('int')     \u2013 defaults to <code>10</code> </li> <li>subset     ('list[list[str]] | list[str] | None')     \u2013 defaults to <code>None</code> </li> </ul> <p>Returns</p> <p>list[list[RerankResult]]:     List of lists containing dictionaries with 'id' and 'score' keys.</p> add_documents <p>Add documents to the index.</p> <p>Parameters</p> <ul> <li>documents_ids     ('str | list[str]')    </li> <li>documents_embeddings     ('list[np.ndarray | torch.Tensor]')    </li> <li>kwargs </li> </ul> get_documents_embeddings <p>Get document embeddings by their IDs.</p> <p>Parameters</p> <ul> <li>document_ids     ('list[list[str]]')    </li> </ul> remove_documents <p>Remove documents from the index.</p> <p>Parameters</p> <ul> <li>documents_ids     ('list[str]')    </li> </ul>"},{"location":"api/indexes/Voyager/","title":"Voyager","text":"<p>Voyager index. The Voyager index is a fast and efficient index for approximate nearest neighbor search.</p> <p>To use this index, you need to install the <code>voyager</code> extra: </p> <p><code>bash pip install \"pylate[voyager]\"</code> </p> <p>or install voyager directly: </p> <p><code>bash pip install voyager</code></p>"},{"location":"api/indexes/Voyager/#parameters","title":"Parameters","text":"<ul> <li> <p>index_folder ('str') \u2013 defaults to <code>indexes</code></p> </li> <li> <p>index_name ('str') \u2013 defaults to <code>colbert</code></p> </li> <li> <p>override ('bool') \u2013 defaults to <code>False</code></p> <p>Whether to override the collection if it already exists.</p> </li> <li> <p>embedding_size ('int') \u2013 defaults to <code>128</code></p> <p>The number of dimensions of the embeddings.</p> </li> <li> <p>M ('int') \u2013 defaults to <code>64</code></p> <p>The number of subquantizers.</p> </li> <li> <p>ef_construction ('int') \u2013 defaults to <code>200</code></p> <p>The number of candidates to evaluate during the construction of the index.</p> </li> <li> <p>ef_search ('int') \u2013 defaults to <code>200</code></p> <p>The number of candidates to evaluate during the search.</p> </li> </ul>"},{"location":"api/indexes/Voyager/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from pylate import indexes, models\n\n&gt;&gt;&gt; index = indexes.Voyager(\n...    index_folder=\"test_indexes\",\n...    index_name=\"colbert\",\n...    override=True,\n...    embedding_size=128,\n... )\n\n&gt;&gt;&gt; model = models.ColBERT(\n...    model_name_or_path=\"sentence-transformers/all-MiniLM-L6-v2\",\n... )\n\n&gt;&gt;&gt; documents_embeddings = model.encode(\n...    [\"fruits are healthy.\", \"fruits are good for health.\", \"fruits are bad for health.\"],\n...    is_query=False,\n... )\n\n&gt;&gt;&gt; index = index.add_documents(\n...    documents_ids=[\"1\", \"2\", \"3\"],\n...    documents_embeddings=documents_embeddings,\n... )\n\n&gt;&gt;&gt; queries_embeddings = model.encode(\n...     [\"fruits are healthy.\", \"fruits are good for health and fun.\"],\n...     is_query=True,\n... )\n\n&gt;&gt;&gt; matches = index(queries_embeddings, k=30)\n</code></pre>"},{"location":"api/indexes/Voyager/#methods","title":"Methods","text":"call <p>Query the index for the nearest neighbors of the queries embeddings.</p> <p>Parameters</p> <ul> <li>queries_embeddings     ('np.ndarray | torch.Tensor')    </li> <li>k     ('int')     \u2013 defaults to <code>10</code> </li> </ul> add_documents <p>Add documents to the index.</p> <p>Parameters</p> <ul> <li>documents_ids     ('str | list[str]')    </li> <li>documents_embeddings     ('list[np.ndarray | torch.Tensor]')    </li> <li>batch_size     ('int')     \u2013 defaults to <code>2000</code> </li> </ul> get_documents_embeddings <p>Retrieve document embeddings for re-ranking from Voyager.</p> <p>Parameters</p> <ul> <li>document_ids     ('list[list[str]]')    </li> </ul> remove_documents <p>Remove documents from the index.</p> <p>Parameters</p> <ul> <li>documents_ids     ('list[str]')    </li> </ul>"},{"location":"api/losses/CachedContrastive/","title":"CachedContrastive","text":"<p>A cached, in-batch negatives contrastive loss for PyLate, analogous to SentenceTransformers' CachedMultipleNegativesRankingLoss. This allows large effective batch sizes by chunking the embeddings pass and caching gradients w.r.t. those embeddings.</p>"},{"location":"api/losses/CachedContrastive/#parameters","title":"Parameters","text":"<ul> <li> <p>model ('ColBERT')</p> <p>A PyLate ColBERT model</p> </li> <li> <p>score_metric ('Callable') \u2013 defaults to <code>&lt;function colbert_scores at 0x7f1e8e7b63b0&gt;</code></p> <p>ColBERT scoring function. Defaults to colbert_scores.</p> </li> <li> <p>mini_batch_size ('int') \u2013 defaults to <code>32</code></p> <p>Chunk size for the forward pass. You can keep this small to avoid OOM on large batch sizes.</p> </li> <li> <p>size_average ('bool') \u2013 defaults to <code>True</code></p> <p>Whether to average or sum the cross-entropy loss across the mini-batch.</p> </li> <li> <p>gather_across_devices ('bool') \u2013 defaults to <code>False</code></p> <p>Whether to gather the embeddings across devices to have more in batch negatives. We recommend making sure the sampling across GPUs use the same dataset in case of multi-dataset training to make sure the negatives are plausible.</p> </li> <li> <p>show_progress_bar ('bool') \u2013 defaults to <code>False</code></p> <p>Whether to show a TQDM progress bar for the embedding steps.</p> </li> <li> <p>temperature ('float') \u2013 defaults to <code>1.0</code></p> </li> </ul>"},{"location":"api/losses/CachedContrastive/#attributes","title":"Attributes","text":"<ul> <li>citation</li> </ul>"},{"location":"api/losses/CachedContrastive/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from pylate import models, losses\n\n&gt;&gt;&gt; model = models.ColBERT(\n...     model_name_or_path=\"sentence-transformers/all-MiniLM-L6-v2\", device=\"cpu\"\n... )\n\n&gt;&gt;&gt; loss = losses.CachedContrastive(model=model, mini_batch_size=1)\n\n&gt;&gt;&gt; anchors = model.tokenize([\n...     \"fruits are healthy.\", \"chips are not healthy.\"\n... ], is_query=True)\n\n&gt;&gt;&gt; positives = model.tokenize([\n...     \"fruits are good for health.\", \"chips are not good for health.\"\n... ], is_query=False)\n\n&gt;&gt;&gt; negatives = model.tokenize([\n...     \"fruits are bad for health.\", \"chips are good for health.\"\n... ], is_query=False)\n\n&gt;&gt;&gt; sentence_features = [anchors, positives, negatives]\n\n&gt;&gt;&gt; loss = loss(sentence_features=sentence_features)\n&gt;&gt;&gt; assert isinstance(loss.item(), float)\n</code></pre>"},{"location":"api/losses/CachedContrastive/#methods","title":"Methods","text":"call <p>Call self as a function.</p> <p>Parameters</p> <ul> <li>args </li> <li>kwargs </li> </ul> add_module <p>Add a child module to the current module.</p> <p>The module can be accessed as an attribute using the given name.  Args:     name (str): name of the child module. The child module can be         accessed from this module using the given name     module (Module): child module to be added to the module.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>module     (Optional[ForwardRef('Module')])    </li> </ul> apply <p>Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self.</p> <p>Typical use includes initializing the parameters of a model (see also :ref:<code>nn-init-doc</code>).  Args:     fn (:class:<code>Module</code> -&gt; None): function to be applied to each submodule  Returns:     Module: self  Example::      &gt;&gt;&gt; @torch.no_grad()     &gt;&gt;&gt; def init_weights(m):     &gt;&gt;&gt;     print(m)     &gt;&gt;&gt;     if type(m) == nn.Linear:     &gt;&gt;&gt;         m.weight.fill_(1.0)     &gt;&gt;&gt;         print(m.weight)     &gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))     &gt;&gt;&gt; net.apply(init_weights)     Linear(in_features=2, out_features=2, bias=True)     Parameter containing:     tensor([[1., 1.],             [1., 1.]], requires_grad=True)     Linear(in_features=2, out_features=2, bias=True)     Parameter containing:     tensor([[1., 1.],             [1., 1.]], requires_grad=True)     Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     )</p> <p>Parameters</p> <ul> <li>fn     (Callable[[ForwardRef('Module')], NoneType])    </li> </ul> bfloat16 <p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> buffers <p>Return an iterator over module buffers.</p> <p>Args:     recurse (bool): if True, then yields buffers of this module         and all submodules. Otherwise, yields only buffers that         are direct members of this module.  Yields:     torch.Tensor: module buffer  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for buf in model.buffers():     &gt;&gt;&gt;     print(type(buf), buf.size())      (20L,)      (20L, 1L, 5L, 5L) <p>Parameters</p> <ul> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> </ul> calculate_loss <p>Calculate the cross-entropy loss. No need to cache the gradients. Each sub-list in reps is a list of mini-batch chunk embeddings</p> <p>Parameters</p> <ul> <li>reps </li> <li>masks </li> <li>with_backward     ('bool')     \u2013 defaults to <code>False</code> </li> </ul> calculate_loss_and_cache_gradients <p>Calculate the cross-entropy loss and cache the gradients wrt. the embeddings.</p> <p>Parameters</p> <ul> <li>reps </li> <li>masks </li> </ul> children <p>Return an iterator over immediate children modules.</p> <p>Yields:     Module: a child module</p> compile <p>Compile this Module's forward using :func:<code>torch.compile</code>.</p> <p>This Module's <code>__call__</code> method is compiled and all arguments are passed as-is to :func:<code>torch.compile</code>.  See :func:<code>torch.compile</code> for details on the arguments for this function.</p> <p>Parameters</p> <ul> <li>args </li> <li>kwargs </li> </ul> cpu <p>Move all model parameters and buffers to the CPU.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> cuda <p>Move all model parameters and buffers to the GPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing the optimizer if the module will live on GPU while being optimized.  .. note::     This method modifies the module in-place.  Args:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[torch.device, int, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> double <p>Casts all floating point parameters and buffers to <code>double</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> embed_minibatch <p>Forward pass on a slice [begin:end] of sentence_feature. If 'with_grad' is False, we run under torch.no_grad. If 'copy_random_state' is True, we create and return a RandContext so that we can exactly reproduce this forward pass later.</p> <p>Parameters</p> <ul> <li>sentence_feature     ('dict[str, Tensor]')    </li> <li>begin     ('int')    </li> <li>end     ('int')    </li> <li>with_grad     ('bool')    </li> <li>copy_random_state     ('bool')    </li> <li>random_state     ('RandContext | None')     \u2013 defaults to <code>None</code> </li> </ul> embed_minibatch_iter <p>Yields chunks of embeddings (and corresponding RandContext) for the given sentence_feature, respecting the mini_batch_size limit.</p> <p>Parameters</p> <ul> <li>sentence_feature     ('dict[str, Tensor]')    </li> <li>with_grad     ('bool')    </li> <li>copy_random_state     ('bool')    </li> <li>random_states     ('list[RandContext] | None')     \u2013 defaults to <code>None</code> </li> </ul> eval <p>Set the module in evaluation mode.</p> <p>This has an effect only on certain modules. See the documentation of particular modules for details of their behaviors in training/evaluation mode, i.e. whether they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>, etc.  This is equivalent with :meth:<code>self.train(False) &lt;torch.nn.Module.train&gt;</code>.  See :ref:<code>locally-disable-grad-doc</code> for a comparison between <code>.eval()</code> and several similar mechanisms that may be confused with it.  Returns:     Module: self</p> extra_repr <p>Return the extra representation of the module.</p> <p>To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.</p> float <p>Casts all floating point parameters and buffers to <code>float</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> forward <p>Compute the CachedConstrastive loss.</p> <p>Parameters</p> <ul> <li>sentence_features     ('Iterable[dict[str, Tensor]]')    </li> <li>labels     ('Optional[Tensor]')     \u2013 defaults to <code>None</code> </li> </ul> get_buffer <p>Return the buffer given by <code>target</code> if it exists, otherwise throw an error.</p> <p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method's functionality as well as how to correctly specify <code>target</code>.  Args:     target: The fully-qualified string name of the buffer         to look for. (See <code>get_submodule</code> for how to specify a         fully-qualified string.)  Returns:     torch.Tensor: The buffer referenced by <code>target</code>  Raises:     AttributeError: If the target string references an invalid         path or resolves to something that is not a         buffer</p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> get_extra_state <p>Return any extra state to include in the module's state_dict.</p> <p>Implement this and a corresponding :func:<code>set_extra_state</code> for your module if you need to store extra state. This function is called when building the module's <code>state_dict()</code>.  Note that extra state should be picklable to ensure working serialization of the state_dict. We only provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes.  Returns:     object: Any extra state to store in the module's state_dict</p> get_parameter <p>Return the parameter given by <code>target</code> if it exists, otherwise throw an error.</p> <p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method's functionality as well as how to correctly specify <code>target</code>.  Args:     target: The fully-qualified string name of the Parameter         to look for. (See <code>get_submodule</code> for how to specify a         fully-qualified string.)  Returns:     torch.nn.Parameter: The Parameter referenced by <code>target</code>  Raises:     AttributeError: If the target string references an invalid         path or resolves to something that is not an         <code>nn.Parameter</code></p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> get_submodule <p>Return the submodule given by <code>target</code> if it exists, otherwise throw an error.</p> <p>For example, let's say you have an <code>nn.Module</code> <code>A</code> that looks like this:  .. code-block:: text      A(         (net_b): Module(             (net_c): Module(                 (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))             )             (linear): Linear(in_features=100, out_features=200, bias=True)         )     )  (The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> which has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)  To check whether or not we have the <code>linear</code> submodule, we would call <code>get_submodule(\"net_b.linear\")</code>. To check whether we have the <code>conv</code> submodule, we would call <code>get_submodule(\"net_b.net_c.conv\")</code>.  The runtime of <code>get_submodule</code> is bounded by the degree of module nesting in <code>target</code>. A query against <code>named_modules</code> achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, <code>get_submodule</code> should always be used.  Args:     target: The fully-qualified string name of the submodule         to look for. (See above example for how to specify a         fully-qualified string.)  Returns:     torch.nn.Module: The submodule referenced by <code>target</code>  Raises:     AttributeError: If at any point along the path resulting from         the target string the (sub)path resolves to a non-existent         attribute name or an object that is not an instance of <code>nn.Module</code>.</p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> half <p>Casts all floating point parameters and buffers to <code>half</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> ipu <p>Move all model parameters and buffers to the IPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing the optimizer if the module will live on IPU while being optimized.  .. note::     This method modifies the module in-place.  Arguments:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[torch.device, int, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> load_state_dict <p>Copy parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants.</p> <p>If :attr:<code>strict</code> is <code>True</code>, then the keys of :attr:<code>state_dict</code> must exactly match the keys returned by this module's :meth:<code>~torch.nn.Module.state_dict</code> function.  .. warning::     If :attr:<code>assign</code> is <code>True</code> the optimizer must be created after     the call to :attr:<code>load_state_dict</code> unless     :func:<code>~torch.__future__.get_swap_module_params_on_conversion</code> is <code>True</code>.  Args:     state_dict (dict): a dict containing parameters and         persistent buffers.     strict (bool, optional): whether to strictly enforce that the keys         in :attr:<code>state_dict</code> match the keys returned by this module's         :meth:<code>~torch.nn.Module.state_dict</code> function. Default: <code>True</code>     assign (bool, optional): When set to <code>False</code>, the properties of the tensors         in the current module are preserved whereas setting it to <code>True</code> preserves         properties of the Tensors in the state dict. The only         exception is the <code>requires_grad</code> field of :class:<code>~torch.nn.Parameter</code>         for which the value from the module is preserved. Default: <code>False</code>  Returns:     <code>NamedTuple</code> with <code>missing_keys</code> and <code>unexpected_keys</code> fields:         * <code>missing_keys</code> is a list of str containing any keys that are expected             by this module but missing from the provided <code>state_dict</code>.         * <code>unexpected_keys</code> is a list of str containing the keys that are not             expected by this module but present in the provided <code>state_dict</code>.  Note:     If a parameter or buffer is registered as <code>None</code> and its corresponding key     exists in :attr:<code>state_dict</code>, :meth:<code>load_state_dict</code> will raise a     <code>RuntimeError</code>.</p> <p>Parameters</p> <ul> <li>state_dict     (collections.abc.Mapping[str, typing.Any])    </li> <li>strict     (bool)     \u2013 defaults to <code>True</code> </li> <li>assign     (bool)     \u2013 defaults to <code>False</code> </li> </ul> modules <p>Return an iterator over all modules in the network.</p> <p>Yields:     Module: a module in the network  Note:     Duplicate modules are returned only once. In the following     example, <code>l</code> will be returned only once.  Example::      &gt;&gt;&gt; l = nn.Linear(2, 2)     &gt;&gt;&gt; net = nn.Sequential(l, l)     &gt;&gt;&gt; for idx, m in enumerate(net.modules()):     ...     print(idx, '-&gt;', m)      0 -&gt; Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     )     1 -&gt; Linear(in_features=2, out_features=2, bias=True)</p> mtia <p>Move all model parameters and buffers to the MTIA.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing the optimizer if the module will live on MTIA while being optimized.  .. note::     This method modifies the module in-place.  Arguments:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[torch.device, int, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> named_buffers <p>Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.</p> <p>Args:     prefix (str): prefix to prepend to all buffer names.     recurse (bool, optional): if True, then yields buffers of this module         and all submodules. Otherwise, yields only buffers that         are direct members of this module. Defaults to True.     remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.  Yields:     (str, torch.Tensor): Tuple containing the name and buffer  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, buf in self.named_buffers():     &gt;&gt;&gt;     if name in ['running_var']:     &gt;&gt;&gt;         print(buf.size())</p> <p>Parameters</p> <ul> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> named_children <p>Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.</p> <p>Yields:     (str, Module): Tuple containing a name and child module  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, module in model.named_children():     &gt;&gt;&gt;     if name in ['conv4', 'conv5']:     &gt;&gt;&gt;         print(module)</p> named_modules <p>Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.</p> <p>Args:     memo: a memo to store the set of modules already added to the result     prefix: a prefix that will be added to the name of the module     remove_duplicate: whether to remove the duplicated module instances in the result         or not  Yields:     (str, Module): Tuple of name and module  Note:     Duplicate modules are returned only once. In the following     example, <code>l</code> will be returned only once.  Example::      &gt;&gt;&gt; l = nn.Linear(2, 2)     &gt;&gt;&gt; net = nn.Sequential(l, l)     &gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):     ...     print(idx, '-&gt;', m)      0 -&gt; ('', Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     ))     1 -&gt; ('0', Linear(in_features=2, out_features=2, bias=True))</p> <p>Parameters</p> <ul> <li>memo     (Optional[set['Module']])     \u2013 defaults to <code>None</code> </li> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> named_parameters <p>Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.</p> <p>Args:     prefix (str): prefix to prepend to all parameter names.     recurse (bool): if True, then yields parameters of this module         and all submodules. Otherwise, yields only parameters that         are direct members of this module.     remove_duplicate (bool, optional): whether to remove the duplicated         parameters in the result. Defaults to True.  Yields:     (str, Parameter): Tuple containing the name and parameter  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, param in self.named_parameters():     &gt;&gt;&gt;     if name in ['bias']:     &gt;&gt;&gt;         print(param.size())</p> <p>Parameters</p> <ul> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> parameters <p>Return an iterator over module parameters.</p> <p>This is typically passed to an optimizer.  Args:     recurse (bool): if True, then yields parameters of this module         and all submodules. Otherwise, yields only parameters that         are direct members of this module.  Yields:     Parameter: module parameter  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for param in model.parameters():     &gt;&gt;&gt;     print(type(param), param.size())      (20L,)      (20L, 1L, 5L, 5L) <p>Parameters</p> <ul> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> </ul> register_backward_hook <p>Register a backward hook on the module.</p> <p>This function is deprecated in favor of :meth:<code>~torch.nn.Module.register_full_backward_hook</code> and the behavior of this function will change in future versions.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[tuple[torch.Tensor, ...], torch.Tensor], Union[tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]])    </li> </ul> register_buffer <p>Add a buffer to the module.</p> <p>This is typically used to register a buffer that should not be considered a model parameter. For example, BatchNorm's <code>running_mean</code> is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:<code>persistent</code> to <code>False</code>. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:<code>state_dict</code>.  Buffers can be accessed as attributes using given names.  Args:     name (str): name of the buffer. The buffer can be accessed         from this module using the given name     tensor (Tensor or None): buffer to be registered. If <code>None</code>, then operations         that run on buffers, such as :attr:<code>cuda</code>, are ignored. If <code>None</code>,         the buffer is not included in the module's :attr:<code>state_dict</code>.     persistent (bool): whether the buffer is part of this module's         :attr:<code>state_dict</code>.  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; self.register_buffer('running_mean', torch.zeros(num_features))</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>tensor     (Optional[torch.Tensor])    </li> <li>persistent     (bool)     \u2013 defaults to <code>True</code> </li> </ul> register_forward_hook <p>Register a forward hook on the module.</p> <p>The hook will be called every time after :func:<code>forward</code> has computed an output.  If <code>with_kwargs</code> is <code>False</code> or not specified, the input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the <code>forward</code>. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func:<code>forward</code> is called. The hook should have the following signature::      hook(module, args, output) -&gt; None or modified output  If <code>with_kwargs</code> is <code>True</code>, the forward hook will be passed the <code>kwargs</code> given to the forward function and be expected to return the output possibly modified. The hook should have the following signature::      hook(module, args, kwargs, output) -&gt; None or modified output  Args:     hook (Callable): The user defined hook to be registered.     prepend (bool): If <code>True</code>, the provided <code>hook</code> will be fired         before all existing <code>forward</code> hooks on this         :class:<code>torch.nn.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>forward</code> hooks on         this :class:<code>torch.nn.Module</code>. Note that global         <code>forward</code> hooks registered with         :func:<code>register_module_forward_hook</code> will fire before all hooks         registered by this method.         Default: <code>False</code>     with_kwargs (bool): If <code>True</code>, the <code>hook</code> will be passed the         kwargs given to the forward function.         Default: <code>False</code>     always_call (bool): If <code>True</code> the <code>hook</code> will be run regardless of         whether an exception is raised while calling the Module.         Default: <code>False</code>  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Union[Callable[[~T, tuple[Any, ...], Any], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Optional[Any]]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> <li>with_kwargs     (bool)     \u2013 defaults to <code>False</code> </li> <li>always_call     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_forward_pre_hook <p>Register a forward pre-hook on the module.</p> <p>The hook will be called every time before :func:<code>forward</code> is invoked.  If <code>with_kwargs</code> is false or not specified, the input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the <code>forward</code>. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned (unless that value is already a tuple). The hook should have the following signature::      hook(module, args) -&gt; None or modified input  If <code>with_kwargs</code> is true, the forward pre-hook will be passed the kwargs given to the forward function. And if the hook modifies the input, both the args and kwargs should be returned. The hook should have the following signature::      hook(module, args, kwargs) -&gt; None or a tuple of modified input and kwargs  Args:     hook (Callable): The user defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>forward_pre</code> hooks on this         :class:<code>torch.nn.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>forward_pre</code> hooks         on this :class:<code>torch.nn.Module</code>. Note that global         <code>forward_pre</code> hooks registered with         :func:<code>register_module_forward_pre_hook</code> will fire before all         hooks registered by this method.         Default: <code>False</code>     with_kwargs (bool): If true, the <code>hook</code> will be passed the kwargs         given to the forward function.         Default: <code>False</code>  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Union[Callable[[~T, tuple[Any, ...]], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any]], Optional[tuple[Any, dict[str, Any]]]]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> <li>with_kwargs     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_full_backward_hook <p>Register a backward hook on the module.</p> <p>The hook will be called every time the gradients with respect to a module are computed, and its firing rules are as follows:      1. Ordinarily, the hook fires when the gradients are computed with respect to the module inputs.     2. If none of the module inputs require gradients, the hook will fire when the gradients are computed        with respect to module outputs.     3. If none of the module outputs require gradients, then the hooks will not fire.  The hook should have the following signature::      hook(module, grad_input, grad_output) -&gt; tuple(Tensor) or None  The :attr:<code>grad_input</code> and :attr:<code>grad_output</code> are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr:<code>grad_input</code> in subsequent computations. :attr:<code>grad_input</code> will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr:<code>grad_input</code> and :attr:<code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.  For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function.  .. warning ::     Modifying inputs or outputs inplace is not allowed when using backward hooks and     will raise an error.  Args:     hook (Callable): The user-defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>backward</code> hooks on this         :class:<code>torch.nn.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>backward</code> hooks on         this :class:<code>torch.nn.Module</code>. Note that global         <code>backward</code> hooks registered with         :func:<code>register_module_full_backward_hook</code> will fire before         all hooks registered by this method.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[tuple[torch.Tensor, ...], torch.Tensor], Union[tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_full_backward_pre_hook <p>Register a backward pre-hook on the module.</p> <p>The hook will be called every time the gradients for the module are computed. The hook should have the following signature::      hook(module, grad_output) -&gt; tuple[Tensor] or None  The :attr:<code>grad_output</code> is a tuple. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the output that will be used in place of :attr:<code>grad_output</code> in subsequent computations. Entries in :attr:<code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.  For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function.  .. warning ::     Modifying inputs inplace is not allowed when using backward hooks and     will raise an error.  Args:     hook (Callable): The user-defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>backward_pre</code> hooks on this         :class:<code>torch.nn.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>backward_pre</code> hooks         on this :class:<code>torch.nn.Module</code>. Note that global         <code>backward_pre</code> hooks registered with         :func:<code>register_module_full_backward_pre_hook</code> will fire before         all hooks registered by this method.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_load_state_dict_post_hook <p>Register a post-hook to be run after module's :meth:<code>~nn.Module.load_state_dict</code> is called.</p> <p>It should have the following signature::     hook(module, incompatible_keys) -&gt; None  The <code>module</code> argument is the current module that this hook is registered on, and the <code>incompatible_keys</code> argument is a <code>NamedTuple</code> consisting of attributes <code>missing_keys</code> and <code>unexpected_keys</code>. <code>missing_keys</code> is a <code>list</code> of <code>str</code> containing the missing keys and <code>unexpected_keys</code> is a <code>list</code> of <code>str</code> containing the unexpected keys.  The given incompatible_keys can be modified inplace if needed.  Note that the checks performed when calling :func:<code>load_state_dict</code> with <code>strict=True</code> are affected by modifications the hook makes to <code>missing_keys</code> or <code>unexpected_keys</code>, as expected. Additions to either set of keys will result in an error being thrown when <code>strict=True</code>, and clearing out both missing and unexpected keys will avoid an error.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook </li> </ul> register_load_state_dict_pre_hook <p>Register a pre-hook to be run before module's :meth:<code>~nn.Module.load_state_dict</code> is called.</p> <p>It should have the following signature::     hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -&gt; None  # noqa: B950  Arguments:     hook (Callable): Callable hook that will be invoked before         loading the state dict.</p> <p>Parameters</p> <ul> <li>hook </li> </ul> register_module <p>Alias for :func:<code>add_module</code>.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>module     (Optional[ForwardRef('Module')])    </li> </ul> register_parameter <p>Add a parameter to the module.</p> <p>The parameter can be accessed as an attribute using given name.  Args:     name (str): name of the parameter. The parameter can be accessed         from this module using the given name     param (Parameter or None): parameter to be added to the module. If         <code>None</code>, then operations that run on parameters, such as :attr:<code>cuda</code>,         are ignored. If <code>None</code>, the parameter is not included in the         module's :attr:<code>state_dict</code>.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>param     (Optional[torch.nn.parameter.Parameter])    </li> </ul> register_state_dict_post_hook <p>Register a post-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method.</p> <p>It should have the following signature::     hook(module, state_dict, prefix, local_metadata) -&gt; None  The registered hooks can modify the <code>state_dict</code> inplace.</p> <p>Parameters</p> <ul> <li>hook </li> </ul> register_state_dict_pre_hook <p>Register a pre-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method.</p> <p>It should have the following signature::     hook(module, prefix, keep_vars) -&gt; None  The registered hooks can be used to perform pre-processing before the <code>state_dict</code> call is made.</p> <p>Parameters</p> <ul> <li>hook </li> </ul> requires_grad_ <p>Change if autograd should record operations on parameters in this module.</p> <p>This method sets the parameters' :attr:<code>requires_grad</code> attributes in-place.  This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).  See :ref:<code>locally-disable-grad-doc</code> for a comparison between <code>.requires_grad_()</code> and several similar mechanisms that may be confused with it.  Args:     requires_grad (bool): whether autograd should record operations on                           parameters in this module. Default: <code>True</code>.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>requires_grad     (bool)     \u2013 defaults to <code>True</code> </li> </ul> set_extra_state <p>Set extra state contained in the loaded <code>state_dict</code>.</p> <p>This function is called from :func:<code>load_state_dict</code> to handle any extra state found within the <code>state_dict</code>. Implement this function and a corresponding :func:<code>get_extra_state</code> for your module if you need to store extra state within its <code>state_dict</code>.  Args:     state (dict): Extra state from the <code>state_dict</code></p> <p>Parameters</p> <ul> <li>state     (Any)    </li> </ul> set_submodule <p>Set the submodule given by <code>target</code> if it exists, otherwise throw an error.</p> <p>.. note::     If <code>strict</code> is set to <code>False</code> (default), the method will replace an existing submodule     or create a new submodule if the parent module exists. If <code>strict</code> is set to <code>True</code>,     the method will only attempt to replace an existing submodule and throw an error if     the submodule does not exist.  For example, let's say you have an <code>nn.Module</code> <code>A</code> that looks like this:  .. code-block:: text      A(         (net_b): Module(             (net_c): Module(                 (conv): Conv2d(3, 3, 3)             )             (linear): Linear(3, 3)         )     )  (The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)  To override the <code>Conv2d</code> with a new submodule <code>Linear</code>, you could call <code>set_submodule(\"net_b.net_c.conv\", nn.Linear(1, 1))</code> where <code>strict</code> could be <code>True</code> or <code>False</code>  To add a new submodule <code>Conv2d</code> to the existing <code>net_b</code> module, you would call <code>set_submodule(\"net_b.conv\", nn.Conv2d(1, 1, 1))</code>.  In the above if you set <code>strict=True</code> and call <code>set_submodule(\"net_b.conv\", nn.Conv2d(1, 1, 1), strict=True)</code>, an AttributeError will be raised because <code>net_b</code> does not have a submodule named <code>conv</code>.  Args:     target: The fully-qualified string name of the submodule         to look for. (See above example for how to specify a         fully-qualified string.)     module: The module to set the submodule to.     strict: If <code>False</code>, the method will replace an existing submodule         or create a new submodule if the parent module exists. If <code>True</code>,         the method will only attempt to replace an existing submodule and throw an error         if the submodule doesn't already exist.  Raises:     ValueError: If the <code>target</code> string is empty or if <code>module</code> is not an instance of <code>nn.Module</code>.     AttributeError: If at any point along the path resulting from         the <code>target</code> string the (sub)path resolves to a non-existent         attribute name or an object that is not an instance of <code>nn.Module</code>.</p> <p>Parameters</p> <ul> <li>target     (str)    </li> <li>module     ('Module')    </li> <li>strict     (bool)     \u2013 defaults to <code>False</code> </li> </ul> share_memory <p>See :meth:<code>torch.Tensor.share_memory_</code>.</p> state_dict <p>Return a dictionary containing references to the whole state of the module.</p> <p>Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to <code>None</code> are not included.  .. note::     The returned object is a shallow copy. It contains references     to the module's parameters and buffers.  .. warning::     Currently <code>state_dict()</code> also accepts positional arguments for     <code>destination</code>, <code>prefix</code> and <code>keep_vars</code> in order. However,     this is being deprecated and keyword arguments will be enforced in     future releases.  .. warning::     Please avoid the use of argument <code>destination</code> as it is not     designed for end-users.  Args:     destination (dict, optional): If provided, the state of module will         be updated into the dict and the same object is returned.         Otherwise, an <code>OrderedDict</code> will be created and returned.         Default: <code>None</code>.     prefix (str, optional): a prefix added to parameter and buffer         names to compose the keys in state_dict. Default: <code>''</code>.     keep_vars (bool, optional): by default the :class:<code>~torch.Tensor</code> s         returned in the state dict are detached from autograd. If it's         set to <code>True</code>, detaching will not be performed.         Default: <code>False</code>.  Returns:     dict:         a dictionary containing a whole state of the module  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; module.state_dict().keys()     ['bias', 'weight']</p> <p>Parameters</p> <ul> <li>args </li> <li>destination     \u2013 defaults to <code>None</code> </li> <li>prefix     \u2013 defaults to ``    </li> <li>keep_vars     \u2013 defaults to <code>False</code> </li> </ul> to <p>Move and/or cast the parameters and buffers.</p> <p>This can be called as  .. function:: to(device=None, dtype=None, non_blocking=False)    :noindex:  .. function:: to(dtype, non_blocking=False)    :noindex:  .. function:: to(tensor, non_blocking=False)    :noindex:  .. function:: to(memory_format=torch.channels_last)    :noindex:  Its signature is similar to :meth:<code>torch.Tensor.to</code>, but only accepts floating point or complex :attr:<code>dtype</code>\\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr:<code>dtype</code> (if given). The integral parameters and buffers will be moved :attr:<code>device</code>, if that is given, but with dtypes unchanged. When :attr:<code>non_blocking</code> is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices.  See below for examples.  .. note::     This method modifies the module in-place.  Args:     device (:class:<code>torch.device</code>): the desired device of the parameters         and buffers in this module     dtype (:class:<code>torch.dtype</code>): the desired floating point or complex dtype of         the parameters and buffers in this module     tensor (torch.Tensor): Tensor whose dtype and device are the desired         dtype and device for all parameters and buffers in this module     memory_format (:class:<code>torch.memory_format</code>): the desired memory         format for 4D parameters and buffers in this module (keyword         only argument)  Returns:     Module: self  Examples::      &gt;&gt;&gt; # xdoctest: +IGNORE_WANT(\"non-deterministic\")     &gt;&gt;&gt; linear = nn.Linear(2, 2)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1913, -0.3420],             [-0.5113, -0.2325]])     &gt;&gt;&gt; linear.to(torch.double)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1913, -0.3420],             [-0.5113, -0.2325]], dtype=torch.float64)     &gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)     &gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")     &gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1914, -0.3420],             [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')     &gt;&gt;&gt; cpu = torch.device(\"cpu\")     &gt;&gt;&gt; linear.to(cpu)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1914, -0.3420],             [-0.5112, -0.2324]], dtype=torch.float16)      &gt;&gt;&gt; linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.3741+0.j,  0.2382+0.j],             [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)     &gt;&gt;&gt; linear(torch.ones(3, 2, dtype=torch.cdouble))     tensor([[0.6122+0.j, 0.1150+0.j],             [0.6122+0.j, 0.1150+0.j],             [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)</p> <p>Parameters</p> <ul> <li>args </li> <li>kwargs </li> </ul> to_empty <p>Move the parameters and buffers to the specified device without copying storage.</p> <p>Args:     device (:class:<code>torch.device</code>): The desired device of the parameters         and buffers in this module.     recurse (bool): Whether parameters and buffers of submodules should         be recursively moved to the specified device.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, str, torch.device, NoneType])    </li> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> </ul> train <p>Set the module in training mode.</p> <p>This has an effect only on certain modules. See the documentation of particular modules for details of their behaviors in training/evaluation mode, i.e., whether they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>, etc.  Args:     mode (bool): whether to set training mode (<code>True</code>) or evaluation                  mode (<code>False</code>). Default: <code>True</code>.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>mode     (bool)     \u2013 defaults to <code>True</code> </li> </ul> type <p>Casts all parameters and buffers to :attr:<code>dst_type</code>.</p> <p>.. note::     This method modifies the module in-place.  Args:     dst_type (type or string): the desired type  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>dst_type     (Union[torch.dtype, str])    </li> </ul> xpu <p>Move all model parameters and buffers to the XPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized.  .. note::     This method modifies the module in-place.  Arguments:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[torch.device, int, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> zero_grad <p>Reset gradients of all model parameters.</p> <p>See similar function under :class:<code>torch.optim.Optimizer</code> for more context.  Args:     set_to_none (bool): instead of setting to zero, set the grads to None.         See :meth:<code>torch.optim.Optimizer.zero_grad</code> for details.</p> <p>Parameters</p> <ul> <li>set_to_none     (bool)     \u2013 defaults to <code>True</code> </li> </ul>"},{"location":"api/losses/Contrastive/","title":"Contrastive","text":"<p>Contrastive loss. Expects as input two texts and a label of either 0 or 1. If the label == 1, then the distance between the two embeddings is reduced. If the label == 0, then the distance between the embeddings is increased.</p>"},{"location":"api/losses/Contrastive/#parameters","title":"Parameters","text":"<ul> <li> <p>model ('ColBERT')</p> <p>ColBERT model.</p> </li> <li> <p>score_metric \u2013 defaults to <code>&lt;function colbert_scores at 0x7f1e8e7b63b0&gt;</code></p> <p>ColBERT scoring function. Defaults to colbert_scores.</p> </li> <li> <p>size_average ('bool') \u2013 defaults to <code>True</code></p> <p>Average by the size of the mini-batch.</p> </li> <li> <p>gather_across_devices ('bool') \u2013 defaults to <code>False</code></p> <p>Whether to gather the embeddings across devices to have more in batch negatives. We recommend making sure the sampling across GPUs use the same dataset in case of multi-dataset training to make sure the negatives are plausible.</p> </li> <li> <p>temperature ('float') \u2013 defaults to <code>1.0</code></p> </li> </ul>"},{"location":"api/losses/Contrastive/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from pylate import models, losses\n\n&gt;&gt;&gt; model = models.ColBERT(\n...     model_name_or_path=\"sentence-transformers/all-MiniLM-L6-v2\", device=\"cpu\"\n... )\n\n&gt;&gt;&gt; loss = losses.Contrastive(model=model)\n\n&gt;&gt;&gt; anchor = model.tokenize([\n...     \"fruits are healthy.\",\n... ], is_query=True)\n\n&gt;&gt;&gt; positive = model.tokenize([\n...     \"fruits are good for health.\",\n... ], is_query=False)\n\n&gt;&gt;&gt; negative = model.tokenize([\n...     \"fruits are bad for health.\",\n... ], is_query=False)\n\n&gt;&gt;&gt; sentence_features = [anchor, positive, negative]\n\n&gt;&gt;&gt; loss = loss(sentence_features=sentence_features)\n&gt;&gt;&gt; assert isinstance(loss.item(), float)\n</code></pre>"},{"location":"api/losses/Contrastive/#methods","title":"Methods","text":"call <p>Call self as a function.</p> <p>Parameters</p> <ul> <li>args </li> <li>kwargs </li> </ul> add_module <p>Add a child module to the current module.</p> <p>The module can be accessed as an attribute using the given name.  Args:     name (str): name of the child module. The child module can be         accessed from this module using the given name     module (Module): child module to be added to the module.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>module     (Optional[ForwardRef('Module')])    </li> </ul> apply <p>Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self.</p> <p>Typical use includes initializing the parameters of a model (see also :ref:<code>nn-init-doc</code>).  Args:     fn (:class:<code>Module</code> -&gt; None): function to be applied to each submodule  Returns:     Module: self  Example::      &gt;&gt;&gt; @torch.no_grad()     &gt;&gt;&gt; def init_weights(m):     &gt;&gt;&gt;     print(m)     &gt;&gt;&gt;     if type(m) == nn.Linear:     &gt;&gt;&gt;         m.weight.fill_(1.0)     &gt;&gt;&gt;         print(m.weight)     &gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))     &gt;&gt;&gt; net.apply(init_weights)     Linear(in_features=2, out_features=2, bias=True)     Parameter containing:     tensor([[1., 1.],             [1., 1.]], requires_grad=True)     Linear(in_features=2, out_features=2, bias=True)     Parameter containing:     tensor([[1., 1.],             [1., 1.]], requires_grad=True)     Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     )</p> <p>Parameters</p> <ul> <li>fn     (Callable[[ForwardRef('Module')], NoneType])    </li> </ul> bfloat16 <p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> buffers <p>Return an iterator over module buffers.</p> <p>Args:     recurse (bool): if True, then yields buffers of this module         and all submodules. Otherwise, yields only buffers that         are direct members of this module.  Yields:     torch.Tensor: module buffer  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for buf in model.buffers():     &gt;&gt;&gt;     print(type(buf), buf.size())      (20L,)      (20L, 1L, 5L, 5L) <p>Parameters</p> <ul> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> </ul> children <p>Return an iterator over immediate children modules.</p> <p>Yields:     Module: a child module</p> compile <p>Compile this Module's forward using :func:<code>torch.compile</code>.</p> <p>This Module's <code>__call__</code> method is compiled and all arguments are passed as-is to :func:<code>torch.compile</code>.  See :func:<code>torch.compile</code> for details on the arguments for this function.</p> <p>Parameters</p> <ul> <li>args </li> <li>kwargs </li> </ul> cpu <p>Move all model parameters and buffers to the CPU.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> cuda <p>Move all model parameters and buffers to the GPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing the optimizer if the module will live on GPU while being optimized.  .. note::     This method modifies the module in-place.  Args:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[torch.device, int, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> double <p>Casts all floating point parameters and buffers to <code>double</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> eval <p>Set the module in evaluation mode.</p> <p>This has an effect only on certain modules. See the documentation of particular modules for details of their behaviors in training/evaluation mode, i.e. whether they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>, etc.  This is equivalent with :meth:<code>self.train(False) &lt;torch.nn.Module.train&gt;</code>.  See :ref:<code>locally-disable-grad-doc</code> for a comparison between <code>.eval()</code> and several similar mechanisms that may be confused with it.  Returns:     Module: self</p> extra_repr <p>Return the extra representation of the module.</p> <p>To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.</p> float <p>Casts all floating point parameters and buffers to <code>float</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> forward <p>Compute the Constrastive loss.</p> <p>Parameters</p> <ul> <li>sentence_features     ('Iterable[dict[str, Tensor]]')    </li> <li>labels     ('torch.Tensor | None')     \u2013 defaults to <code>None</code> </li> </ul> get_buffer <p>Return the buffer given by <code>target</code> if it exists, otherwise throw an error.</p> <p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method's functionality as well as how to correctly specify <code>target</code>.  Args:     target: The fully-qualified string name of the buffer         to look for. (See <code>get_submodule</code> for how to specify a         fully-qualified string.)  Returns:     torch.Tensor: The buffer referenced by <code>target</code>  Raises:     AttributeError: If the target string references an invalid         path or resolves to something that is not a         buffer</p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> get_extra_state <p>Return any extra state to include in the module's state_dict.</p> <p>Implement this and a corresponding :func:<code>set_extra_state</code> for your module if you need to store extra state. This function is called when building the module's <code>state_dict()</code>.  Note that extra state should be picklable to ensure working serialization of the state_dict. We only provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes.  Returns:     object: Any extra state to store in the module's state_dict</p> get_parameter <p>Return the parameter given by <code>target</code> if it exists, otherwise throw an error.</p> <p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method's functionality as well as how to correctly specify <code>target</code>.  Args:     target: The fully-qualified string name of the Parameter         to look for. (See <code>get_submodule</code> for how to specify a         fully-qualified string.)  Returns:     torch.nn.Parameter: The Parameter referenced by <code>target</code>  Raises:     AttributeError: If the target string references an invalid         path or resolves to something that is not an         <code>nn.Parameter</code></p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> get_submodule <p>Return the submodule given by <code>target</code> if it exists, otherwise throw an error.</p> <p>For example, let's say you have an <code>nn.Module</code> <code>A</code> that looks like this:  .. code-block:: text      A(         (net_b): Module(             (net_c): Module(                 (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))             )             (linear): Linear(in_features=100, out_features=200, bias=True)         )     )  (The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> which has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)  To check whether or not we have the <code>linear</code> submodule, we would call <code>get_submodule(\"net_b.linear\")</code>. To check whether we have the <code>conv</code> submodule, we would call <code>get_submodule(\"net_b.net_c.conv\")</code>.  The runtime of <code>get_submodule</code> is bounded by the degree of module nesting in <code>target</code>. A query against <code>named_modules</code> achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, <code>get_submodule</code> should always be used.  Args:     target: The fully-qualified string name of the submodule         to look for. (See above example for how to specify a         fully-qualified string.)  Returns:     torch.nn.Module: The submodule referenced by <code>target</code>  Raises:     AttributeError: If at any point along the path resulting from         the target string the (sub)path resolves to a non-existent         attribute name or an object that is not an instance of <code>nn.Module</code>.</p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> half <p>Casts all floating point parameters and buffers to <code>half</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> ipu <p>Move all model parameters and buffers to the IPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing the optimizer if the module will live on IPU while being optimized.  .. note::     This method modifies the module in-place.  Arguments:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[torch.device, int, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> load_state_dict <p>Copy parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants.</p> <p>If :attr:<code>strict</code> is <code>True</code>, then the keys of :attr:<code>state_dict</code> must exactly match the keys returned by this module's :meth:<code>~torch.nn.Module.state_dict</code> function.  .. warning::     If :attr:<code>assign</code> is <code>True</code> the optimizer must be created after     the call to :attr:<code>load_state_dict</code> unless     :func:<code>~torch.__future__.get_swap_module_params_on_conversion</code> is <code>True</code>.  Args:     state_dict (dict): a dict containing parameters and         persistent buffers.     strict (bool, optional): whether to strictly enforce that the keys         in :attr:<code>state_dict</code> match the keys returned by this module's         :meth:<code>~torch.nn.Module.state_dict</code> function. Default: <code>True</code>     assign (bool, optional): When set to <code>False</code>, the properties of the tensors         in the current module are preserved whereas setting it to <code>True</code> preserves         properties of the Tensors in the state dict. The only         exception is the <code>requires_grad</code> field of :class:<code>~torch.nn.Parameter</code>         for which the value from the module is preserved. Default: <code>False</code>  Returns:     <code>NamedTuple</code> with <code>missing_keys</code> and <code>unexpected_keys</code> fields:         * <code>missing_keys</code> is a list of str containing any keys that are expected             by this module but missing from the provided <code>state_dict</code>.         * <code>unexpected_keys</code> is a list of str containing the keys that are not             expected by this module but present in the provided <code>state_dict</code>.  Note:     If a parameter or buffer is registered as <code>None</code> and its corresponding key     exists in :attr:<code>state_dict</code>, :meth:<code>load_state_dict</code> will raise a     <code>RuntimeError</code>.</p> <p>Parameters</p> <ul> <li>state_dict     (collections.abc.Mapping[str, typing.Any])    </li> <li>strict     (bool)     \u2013 defaults to <code>True</code> </li> <li>assign     (bool)     \u2013 defaults to <code>False</code> </li> </ul> modules <p>Return an iterator over all modules in the network.</p> <p>Yields:     Module: a module in the network  Note:     Duplicate modules are returned only once. In the following     example, <code>l</code> will be returned only once.  Example::      &gt;&gt;&gt; l = nn.Linear(2, 2)     &gt;&gt;&gt; net = nn.Sequential(l, l)     &gt;&gt;&gt; for idx, m in enumerate(net.modules()):     ...     print(idx, '-&gt;', m)      0 -&gt; Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     )     1 -&gt; Linear(in_features=2, out_features=2, bias=True)</p> mtia <p>Move all model parameters and buffers to the MTIA.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing the optimizer if the module will live on MTIA while being optimized.  .. note::     This method modifies the module in-place.  Arguments:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[torch.device, int, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> named_buffers <p>Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.</p> <p>Args:     prefix (str): prefix to prepend to all buffer names.     recurse (bool, optional): if True, then yields buffers of this module         and all submodules. Otherwise, yields only buffers that         are direct members of this module. Defaults to True.     remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.  Yields:     (str, torch.Tensor): Tuple containing the name and buffer  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, buf in self.named_buffers():     &gt;&gt;&gt;     if name in ['running_var']:     &gt;&gt;&gt;         print(buf.size())</p> <p>Parameters</p> <ul> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> named_children <p>Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.</p> <p>Yields:     (str, Module): Tuple containing a name and child module  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, module in model.named_children():     &gt;&gt;&gt;     if name in ['conv4', 'conv5']:     &gt;&gt;&gt;         print(module)</p> named_modules <p>Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.</p> <p>Args:     memo: a memo to store the set of modules already added to the result     prefix: a prefix that will be added to the name of the module     remove_duplicate: whether to remove the duplicated module instances in the result         or not  Yields:     (str, Module): Tuple of name and module  Note:     Duplicate modules are returned only once. In the following     example, <code>l</code> will be returned only once.  Example::      &gt;&gt;&gt; l = nn.Linear(2, 2)     &gt;&gt;&gt; net = nn.Sequential(l, l)     &gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):     ...     print(idx, '-&gt;', m)      0 -&gt; ('', Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     ))     1 -&gt; ('0', Linear(in_features=2, out_features=2, bias=True))</p> <p>Parameters</p> <ul> <li>memo     (Optional[set['Module']])     \u2013 defaults to <code>None</code> </li> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> named_parameters <p>Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.</p> <p>Args:     prefix (str): prefix to prepend to all parameter names.     recurse (bool): if True, then yields parameters of this module         and all submodules. Otherwise, yields only parameters that         are direct members of this module.     remove_duplicate (bool, optional): whether to remove the duplicated         parameters in the result. Defaults to True.  Yields:     (str, Parameter): Tuple containing the name and parameter  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, param in self.named_parameters():     &gt;&gt;&gt;     if name in ['bias']:     &gt;&gt;&gt;         print(param.size())</p> <p>Parameters</p> <ul> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> parameters <p>Return an iterator over module parameters.</p> <p>This is typically passed to an optimizer.  Args:     recurse (bool): if True, then yields parameters of this module         and all submodules. Otherwise, yields only parameters that         are direct members of this module.  Yields:     Parameter: module parameter  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for param in model.parameters():     &gt;&gt;&gt;     print(type(param), param.size())      (20L,)      (20L, 1L, 5L, 5L) <p>Parameters</p> <ul> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> </ul> register_backward_hook <p>Register a backward hook on the module.</p> <p>This function is deprecated in favor of :meth:<code>~torch.nn.Module.register_full_backward_hook</code> and the behavior of this function will change in future versions.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[tuple[torch.Tensor, ...], torch.Tensor], Union[tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]])    </li> </ul> register_buffer <p>Add a buffer to the module.</p> <p>This is typically used to register a buffer that should not be considered a model parameter. For example, BatchNorm's <code>running_mean</code> is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:<code>persistent</code> to <code>False</code>. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:<code>state_dict</code>.  Buffers can be accessed as attributes using given names.  Args:     name (str): name of the buffer. The buffer can be accessed         from this module using the given name     tensor (Tensor or None): buffer to be registered. If <code>None</code>, then operations         that run on buffers, such as :attr:<code>cuda</code>, are ignored. If <code>None</code>,         the buffer is not included in the module's :attr:<code>state_dict</code>.     persistent (bool): whether the buffer is part of this module's         :attr:<code>state_dict</code>.  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; self.register_buffer('running_mean', torch.zeros(num_features))</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>tensor     (Optional[torch.Tensor])    </li> <li>persistent     (bool)     \u2013 defaults to <code>True</code> </li> </ul> register_forward_hook <p>Register a forward hook on the module.</p> <p>The hook will be called every time after :func:<code>forward</code> has computed an output.  If <code>with_kwargs</code> is <code>False</code> or not specified, the input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the <code>forward</code>. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func:<code>forward</code> is called. The hook should have the following signature::      hook(module, args, output) -&gt; None or modified output  If <code>with_kwargs</code> is <code>True</code>, the forward hook will be passed the <code>kwargs</code> given to the forward function and be expected to return the output possibly modified. The hook should have the following signature::      hook(module, args, kwargs, output) -&gt; None or modified output  Args:     hook (Callable): The user defined hook to be registered.     prepend (bool): If <code>True</code>, the provided <code>hook</code> will be fired         before all existing <code>forward</code> hooks on this         :class:<code>torch.nn.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>forward</code> hooks on         this :class:<code>torch.nn.Module</code>. Note that global         <code>forward</code> hooks registered with         :func:<code>register_module_forward_hook</code> will fire before all hooks         registered by this method.         Default: <code>False</code>     with_kwargs (bool): If <code>True</code>, the <code>hook</code> will be passed the         kwargs given to the forward function.         Default: <code>False</code>     always_call (bool): If <code>True</code> the <code>hook</code> will be run regardless of         whether an exception is raised while calling the Module.         Default: <code>False</code>  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Union[Callable[[~T, tuple[Any, ...], Any], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Optional[Any]]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> <li>with_kwargs     (bool)     \u2013 defaults to <code>False</code> </li> <li>always_call     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_forward_pre_hook <p>Register a forward pre-hook on the module.</p> <p>The hook will be called every time before :func:<code>forward</code> is invoked.  If <code>with_kwargs</code> is false or not specified, the input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the <code>forward</code>. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned (unless that value is already a tuple). The hook should have the following signature::      hook(module, args) -&gt; None or modified input  If <code>with_kwargs</code> is true, the forward pre-hook will be passed the kwargs given to the forward function. And if the hook modifies the input, both the args and kwargs should be returned. The hook should have the following signature::      hook(module, args, kwargs) -&gt; None or a tuple of modified input and kwargs  Args:     hook (Callable): The user defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>forward_pre</code> hooks on this         :class:<code>torch.nn.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>forward_pre</code> hooks         on this :class:<code>torch.nn.Module</code>. Note that global         <code>forward_pre</code> hooks registered with         :func:<code>register_module_forward_pre_hook</code> will fire before all         hooks registered by this method.         Default: <code>False</code>     with_kwargs (bool): If true, the <code>hook</code> will be passed the kwargs         given to the forward function.         Default: <code>False</code>  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Union[Callable[[~T, tuple[Any, ...]], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any]], Optional[tuple[Any, dict[str, Any]]]]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> <li>with_kwargs     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_full_backward_hook <p>Register a backward hook on the module.</p> <p>The hook will be called every time the gradients with respect to a module are computed, and its firing rules are as follows:      1. Ordinarily, the hook fires when the gradients are computed with respect to the module inputs.     2. If none of the module inputs require gradients, the hook will fire when the gradients are computed        with respect to module outputs.     3. If none of the module outputs require gradients, then the hooks will not fire.  The hook should have the following signature::      hook(module, grad_input, grad_output) -&gt; tuple(Tensor) or None  The :attr:<code>grad_input</code> and :attr:<code>grad_output</code> are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr:<code>grad_input</code> in subsequent computations. :attr:<code>grad_input</code> will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr:<code>grad_input</code> and :attr:<code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.  For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function.  .. warning ::     Modifying inputs or outputs inplace is not allowed when using backward hooks and     will raise an error.  Args:     hook (Callable): The user-defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>backward</code> hooks on this         :class:<code>torch.nn.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>backward</code> hooks on         this :class:<code>torch.nn.Module</code>. Note that global         <code>backward</code> hooks registered with         :func:<code>register_module_full_backward_hook</code> will fire before         all hooks registered by this method.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[tuple[torch.Tensor, ...], torch.Tensor], Union[tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_full_backward_pre_hook <p>Register a backward pre-hook on the module.</p> <p>The hook will be called every time the gradients for the module are computed. The hook should have the following signature::      hook(module, grad_output) -&gt; tuple[Tensor] or None  The :attr:<code>grad_output</code> is a tuple. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the output that will be used in place of :attr:<code>grad_output</code> in subsequent computations. Entries in :attr:<code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.  For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function.  .. warning ::     Modifying inputs inplace is not allowed when using backward hooks and     will raise an error.  Args:     hook (Callable): The user-defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>backward_pre</code> hooks on this         :class:<code>torch.nn.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>backward_pre</code> hooks         on this :class:<code>torch.nn.Module</code>. Note that global         <code>backward_pre</code> hooks registered with         :func:<code>register_module_full_backward_pre_hook</code> will fire before         all hooks registered by this method.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_load_state_dict_post_hook <p>Register a post-hook to be run after module's :meth:<code>~nn.Module.load_state_dict</code> is called.</p> <p>It should have the following signature::     hook(module, incompatible_keys) -&gt; None  The <code>module</code> argument is the current module that this hook is registered on, and the <code>incompatible_keys</code> argument is a <code>NamedTuple</code> consisting of attributes <code>missing_keys</code> and <code>unexpected_keys</code>. <code>missing_keys</code> is a <code>list</code> of <code>str</code> containing the missing keys and <code>unexpected_keys</code> is a <code>list</code> of <code>str</code> containing the unexpected keys.  The given incompatible_keys can be modified inplace if needed.  Note that the checks performed when calling :func:<code>load_state_dict</code> with <code>strict=True</code> are affected by modifications the hook makes to <code>missing_keys</code> or <code>unexpected_keys</code>, as expected. Additions to either set of keys will result in an error being thrown when <code>strict=True</code>, and clearing out both missing and unexpected keys will avoid an error.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook </li> </ul> register_load_state_dict_pre_hook <p>Register a pre-hook to be run before module's :meth:<code>~nn.Module.load_state_dict</code> is called.</p> <p>It should have the following signature::     hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -&gt; None  # noqa: B950  Arguments:     hook (Callable): Callable hook that will be invoked before         loading the state dict.</p> <p>Parameters</p> <ul> <li>hook </li> </ul> register_module <p>Alias for :func:<code>add_module</code>.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>module     (Optional[ForwardRef('Module')])    </li> </ul> register_parameter <p>Add a parameter to the module.</p> <p>The parameter can be accessed as an attribute using given name.  Args:     name (str): name of the parameter. The parameter can be accessed         from this module using the given name     param (Parameter or None): parameter to be added to the module. If         <code>None</code>, then operations that run on parameters, such as :attr:<code>cuda</code>,         are ignored. If <code>None</code>, the parameter is not included in the         module's :attr:<code>state_dict</code>.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>param     (Optional[torch.nn.parameter.Parameter])    </li> </ul> register_state_dict_post_hook <p>Register a post-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method.</p> <p>It should have the following signature::     hook(module, state_dict, prefix, local_metadata) -&gt; None  The registered hooks can modify the <code>state_dict</code> inplace.</p> <p>Parameters</p> <ul> <li>hook </li> </ul> register_state_dict_pre_hook <p>Register a pre-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method.</p> <p>It should have the following signature::     hook(module, prefix, keep_vars) -&gt; None  The registered hooks can be used to perform pre-processing before the <code>state_dict</code> call is made.</p> <p>Parameters</p> <ul> <li>hook </li> </ul> requires_grad_ <p>Change if autograd should record operations on parameters in this module.</p> <p>This method sets the parameters' :attr:<code>requires_grad</code> attributes in-place.  This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).  See :ref:<code>locally-disable-grad-doc</code> for a comparison between <code>.requires_grad_()</code> and several similar mechanisms that may be confused with it.  Args:     requires_grad (bool): whether autograd should record operations on                           parameters in this module. Default: <code>True</code>.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>requires_grad     (bool)     \u2013 defaults to <code>True</code> </li> </ul> set_extra_state <p>Set extra state contained in the loaded <code>state_dict</code>.</p> <p>This function is called from :func:<code>load_state_dict</code> to handle any extra state found within the <code>state_dict</code>. Implement this function and a corresponding :func:<code>get_extra_state</code> for your module if you need to store extra state within its <code>state_dict</code>.  Args:     state (dict): Extra state from the <code>state_dict</code></p> <p>Parameters</p> <ul> <li>state     (Any)    </li> </ul> set_submodule <p>Set the submodule given by <code>target</code> if it exists, otherwise throw an error.</p> <p>.. note::     If <code>strict</code> is set to <code>False</code> (default), the method will replace an existing submodule     or create a new submodule if the parent module exists. If <code>strict</code> is set to <code>True</code>,     the method will only attempt to replace an existing submodule and throw an error if     the submodule does not exist.  For example, let's say you have an <code>nn.Module</code> <code>A</code> that looks like this:  .. code-block:: text      A(         (net_b): Module(             (net_c): Module(                 (conv): Conv2d(3, 3, 3)             )             (linear): Linear(3, 3)         )     )  (The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)  To override the <code>Conv2d</code> with a new submodule <code>Linear</code>, you could call <code>set_submodule(\"net_b.net_c.conv\", nn.Linear(1, 1))</code> where <code>strict</code> could be <code>True</code> or <code>False</code>  To add a new submodule <code>Conv2d</code> to the existing <code>net_b</code> module, you would call <code>set_submodule(\"net_b.conv\", nn.Conv2d(1, 1, 1))</code>.  In the above if you set <code>strict=True</code> and call <code>set_submodule(\"net_b.conv\", nn.Conv2d(1, 1, 1), strict=True)</code>, an AttributeError will be raised because <code>net_b</code> does not have a submodule named <code>conv</code>.  Args:     target: The fully-qualified string name of the submodule         to look for. (See above example for how to specify a         fully-qualified string.)     module: The module to set the submodule to.     strict: If <code>False</code>, the method will replace an existing submodule         or create a new submodule if the parent module exists. If <code>True</code>,         the method will only attempt to replace an existing submodule and throw an error         if the submodule doesn't already exist.  Raises:     ValueError: If the <code>target</code> string is empty or if <code>module</code> is not an instance of <code>nn.Module</code>.     AttributeError: If at any point along the path resulting from         the <code>target</code> string the (sub)path resolves to a non-existent         attribute name or an object that is not an instance of <code>nn.Module</code>.</p> <p>Parameters</p> <ul> <li>target     (str)    </li> <li>module     ('Module')    </li> <li>strict     (bool)     \u2013 defaults to <code>False</code> </li> </ul> share_memory <p>See :meth:<code>torch.Tensor.share_memory_</code>.</p> state_dict <p>Return a dictionary containing references to the whole state of the module.</p> <p>Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to <code>None</code> are not included.  .. note::     The returned object is a shallow copy. It contains references     to the module's parameters and buffers.  .. warning::     Currently <code>state_dict()</code> also accepts positional arguments for     <code>destination</code>, <code>prefix</code> and <code>keep_vars</code> in order. However,     this is being deprecated and keyword arguments will be enforced in     future releases.  .. warning::     Please avoid the use of argument <code>destination</code> as it is not     designed for end-users.  Args:     destination (dict, optional): If provided, the state of module will         be updated into the dict and the same object is returned.         Otherwise, an <code>OrderedDict</code> will be created and returned.         Default: <code>None</code>.     prefix (str, optional): a prefix added to parameter and buffer         names to compose the keys in state_dict. Default: <code>''</code>.     keep_vars (bool, optional): by default the :class:<code>~torch.Tensor</code> s         returned in the state dict are detached from autograd. If it's         set to <code>True</code>, detaching will not be performed.         Default: <code>False</code>.  Returns:     dict:         a dictionary containing a whole state of the module  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; module.state_dict().keys()     ['bias', 'weight']</p> <p>Parameters</p> <ul> <li>args </li> <li>destination     \u2013 defaults to <code>None</code> </li> <li>prefix     \u2013 defaults to ``    </li> <li>keep_vars     \u2013 defaults to <code>False</code> </li> </ul> to <p>Move and/or cast the parameters and buffers.</p> <p>This can be called as  .. function:: to(device=None, dtype=None, non_blocking=False)    :noindex:  .. function:: to(dtype, non_blocking=False)    :noindex:  .. function:: to(tensor, non_blocking=False)    :noindex:  .. function:: to(memory_format=torch.channels_last)    :noindex:  Its signature is similar to :meth:<code>torch.Tensor.to</code>, but only accepts floating point or complex :attr:<code>dtype</code>\\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr:<code>dtype</code> (if given). The integral parameters and buffers will be moved :attr:<code>device</code>, if that is given, but with dtypes unchanged. When :attr:<code>non_blocking</code> is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices.  See below for examples.  .. note::     This method modifies the module in-place.  Args:     device (:class:<code>torch.device</code>): the desired device of the parameters         and buffers in this module     dtype (:class:<code>torch.dtype</code>): the desired floating point or complex dtype of         the parameters and buffers in this module     tensor (torch.Tensor): Tensor whose dtype and device are the desired         dtype and device for all parameters and buffers in this module     memory_format (:class:<code>torch.memory_format</code>): the desired memory         format for 4D parameters and buffers in this module (keyword         only argument)  Returns:     Module: self  Examples::      &gt;&gt;&gt; # xdoctest: +IGNORE_WANT(\"non-deterministic\")     &gt;&gt;&gt; linear = nn.Linear(2, 2)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1913, -0.3420],             [-0.5113, -0.2325]])     &gt;&gt;&gt; linear.to(torch.double)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1913, -0.3420],             [-0.5113, -0.2325]], dtype=torch.float64)     &gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)     &gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")     &gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1914, -0.3420],             [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')     &gt;&gt;&gt; cpu = torch.device(\"cpu\")     &gt;&gt;&gt; linear.to(cpu)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1914, -0.3420],             [-0.5112, -0.2324]], dtype=torch.float16)      &gt;&gt;&gt; linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.3741+0.j,  0.2382+0.j],             [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)     &gt;&gt;&gt; linear(torch.ones(3, 2, dtype=torch.cdouble))     tensor([[0.6122+0.j, 0.1150+0.j],             [0.6122+0.j, 0.1150+0.j],             [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)</p> <p>Parameters</p> <ul> <li>args </li> <li>kwargs </li> </ul> to_empty <p>Move the parameters and buffers to the specified device without copying storage.</p> <p>Args:     device (:class:<code>torch.device</code>): The desired device of the parameters         and buffers in this module.     recurse (bool): Whether parameters and buffers of submodules should         be recursively moved to the specified device.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, str, torch.device, NoneType])    </li> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> </ul> train <p>Set the module in training mode.</p> <p>This has an effect only on certain modules. See the documentation of particular modules for details of their behaviors in training/evaluation mode, i.e., whether they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>, etc.  Args:     mode (bool): whether to set training mode (<code>True</code>) or evaluation                  mode (<code>False</code>). Default: <code>True</code>.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>mode     (bool)     \u2013 defaults to <code>True</code> </li> </ul> type <p>Casts all parameters and buffers to :attr:<code>dst_type</code>.</p> <p>.. note::     This method modifies the module in-place.  Args:     dst_type (type or string): the desired type  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>dst_type     (Union[torch.dtype, str])    </li> </ul> xpu <p>Move all model parameters and buffers to the XPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized.  .. note::     This method modifies the module in-place.  Arguments:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[torch.device, int, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> zero_grad <p>Reset gradients of all model parameters.</p> <p>See similar function under :class:<code>torch.optim.Optimizer</code> for more context.  Args:     set_to_none (bool): instead of setting to zero, set the grads to None.         See :meth:<code>torch.optim.Optimizer.zero_grad</code> for details.</p> <p>Parameters</p> <ul> <li>set_to_none     (bool)     \u2013 defaults to <code>True</code> </li> </ul>"},{"location":"api/losses/Distillation/","title":"Distillation","text":"<p>Distillation loss for ColBERT model. The loss is computed with respect to the format of SentenceTransformer library.</p>"},{"location":"api/losses/Distillation/#parameters","title":"Parameters","text":"<ul> <li> <p>model ('ColBERT')</p> <p>SentenceTransformer model.</p> </li> <li> <p>score_metric ('Callable') \u2013 defaults to <code>&lt;function colbert_kd_scores at 0x7f1e8e7fc700&gt;</code></p> <p>Function that returns a score between two sequences of embeddings.</p> </li> <li> <p>size_average ('bool') \u2013 defaults to <code>True</code></p> <p>Average by the size of the mini-batch or perform sum.</p> </li> <li> <p>normalize_scores ('bool') \u2013 defaults to <code>True</code></p> </li> </ul>"},{"location":"api/losses/Distillation/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from pylate import models, losses\n\n&gt;&gt;&gt; model = models.ColBERT(\n...     model_name_or_path=\"sentence-transformers/all-MiniLM-L6-v2\", device=\"cpu\"\n... )\n\n&gt;&gt;&gt; distillation = losses.Distillation(model=model)\n\n&gt;&gt;&gt; query = model.tokenize([\n...     \"fruits are healthy.\",\n... ], is_query=True)\n\n&gt;&gt;&gt; documents = model.tokenize([\n...     \"fruits are good for health.\",\n...     \"fruits are bad for health.\"\n... ], is_query=False)\n\n&gt;&gt;&gt; sentence_features = [query, documents]\n\n&gt;&gt;&gt; labels = torch.tensor([\n...     [0.7, 0.3],\n... ], dtype=torch.float32)\n\n&gt;&gt;&gt; loss = distillation(sentence_features=sentence_features, labels=labels)\n\n&gt;&gt;&gt; assert isinstance(loss.item(), float)\n</code></pre>"},{"location":"api/losses/Distillation/#methods","title":"Methods","text":"call <p>Call self as a function.</p> <p>Parameters</p> <ul> <li>args </li> <li>kwargs </li> </ul> add_module <p>Add a child module to the current module.</p> <p>The module can be accessed as an attribute using the given name.  Args:     name (str): name of the child module. The child module can be         accessed from this module using the given name     module (Module): child module to be added to the module.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>module     (Optional[ForwardRef('Module')])    </li> </ul> apply <p>Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self.</p> <p>Typical use includes initializing the parameters of a model (see also :ref:<code>nn-init-doc</code>).  Args:     fn (:class:<code>Module</code> -&gt; None): function to be applied to each submodule  Returns:     Module: self  Example::      &gt;&gt;&gt; @torch.no_grad()     &gt;&gt;&gt; def init_weights(m):     &gt;&gt;&gt;     print(m)     &gt;&gt;&gt;     if type(m) == nn.Linear:     &gt;&gt;&gt;         m.weight.fill_(1.0)     &gt;&gt;&gt;         print(m.weight)     &gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))     &gt;&gt;&gt; net.apply(init_weights)     Linear(in_features=2, out_features=2, bias=True)     Parameter containing:     tensor([[1., 1.],             [1., 1.]], requires_grad=True)     Linear(in_features=2, out_features=2, bias=True)     Parameter containing:     tensor([[1., 1.],             [1., 1.]], requires_grad=True)     Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     )</p> <p>Parameters</p> <ul> <li>fn     (Callable[[ForwardRef('Module')], NoneType])    </li> </ul> bfloat16 <p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> buffers <p>Return an iterator over module buffers.</p> <p>Args:     recurse (bool): if True, then yields buffers of this module         and all submodules. Otherwise, yields only buffers that         are direct members of this module.  Yields:     torch.Tensor: module buffer  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for buf in model.buffers():     &gt;&gt;&gt;     print(type(buf), buf.size())      (20L,)      (20L, 1L, 5L, 5L) <p>Parameters</p> <ul> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> </ul> children <p>Return an iterator over immediate children modules.</p> <p>Yields:     Module: a child module</p> compile <p>Compile this Module's forward using :func:<code>torch.compile</code>.</p> <p>This Module's <code>__call__</code> method is compiled and all arguments are passed as-is to :func:<code>torch.compile</code>.  See :func:<code>torch.compile</code> for details on the arguments for this function.</p> <p>Parameters</p> <ul> <li>args </li> <li>kwargs </li> </ul> cpu <p>Move all model parameters and buffers to the CPU.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> cuda <p>Move all model parameters and buffers to the GPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing the optimizer if the module will live on GPU while being optimized.  .. note::     This method modifies the module in-place.  Args:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[torch.device, int, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> double <p>Casts all floating point parameters and buffers to <code>double</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> eval <p>Set the module in evaluation mode.</p> <p>This has an effect only on certain modules. See the documentation of particular modules for details of their behaviors in training/evaluation mode, i.e. whether they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>, etc.  This is equivalent with :meth:<code>self.train(False) &lt;torch.nn.Module.train&gt;</code>.  See :ref:<code>locally-disable-grad-doc</code> for a comparison between <code>.eval()</code> and several similar mechanisms that may be confused with it.  Returns:     Module: self</p> extra_repr <p>Return the extra representation of the module.</p> <p>To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.</p> float <p>Casts all floating point parameters and buffers to <code>float</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> forward <p>Computes the distillation loss with respect to SentenceTransformer.</p> <p>Parameters</p> <ul> <li>sentence_features     ('Iterable[dict[str, torch.Tensor]]')    </li> <li>labels     ('torch.Tensor')    </li> </ul> get_buffer <p>Return the buffer given by <code>target</code> if it exists, otherwise throw an error.</p> <p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method's functionality as well as how to correctly specify <code>target</code>.  Args:     target: The fully-qualified string name of the buffer         to look for. (See <code>get_submodule</code> for how to specify a         fully-qualified string.)  Returns:     torch.Tensor: The buffer referenced by <code>target</code>  Raises:     AttributeError: If the target string references an invalid         path or resolves to something that is not a         buffer</p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> get_extra_state <p>Return any extra state to include in the module's state_dict.</p> <p>Implement this and a corresponding :func:<code>set_extra_state</code> for your module if you need to store extra state. This function is called when building the module's <code>state_dict()</code>.  Note that extra state should be picklable to ensure working serialization of the state_dict. We only provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes.  Returns:     object: Any extra state to store in the module's state_dict</p> get_parameter <p>Return the parameter given by <code>target</code> if it exists, otherwise throw an error.</p> <p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method's functionality as well as how to correctly specify <code>target</code>.  Args:     target: The fully-qualified string name of the Parameter         to look for. (See <code>get_submodule</code> for how to specify a         fully-qualified string.)  Returns:     torch.nn.Parameter: The Parameter referenced by <code>target</code>  Raises:     AttributeError: If the target string references an invalid         path or resolves to something that is not an         <code>nn.Parameter</code></p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> get_submodule <p>Return the submodule given by <code>target</code> if it exists, otherwise throw an error.</p> <p>For example, let's say you have an <code>nn.Module</code> <code>A</code> that looks like this:  .. code-block:: text      A(         (net_b): Module(             (net_c): Module(                 (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))             )             (linear): Linear(in_features=100, out_features=200, bias=True)         )     )  (The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> which has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)  To check whether or not we have the <code>linear</code> submodule, we would call <code>get_submodule(\"net_b.linear\")</code>. To check whether we have the <code>conv</code> submodule, we would call <code>get_submodule(\"net_b.net_c.conv\")</code>.  The runtime of <code>get_submodule</code> is bounded by the degree of module nesting in <code>target</code>. A query against <code>named_modules</code> achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, <code>get_submodule</code> should always be used.  Args:     target: The fully-qualified string name of the submodule         to look for. (See above example for how to specify a         fully-qualified string.)  Returns:     torch.nn.Module: The submodule referenced by <code>target</code>  Raises:     AttributeError: If at any point along the path resulting from         the target string the (sub)path resolves to a non-existent         attribute name or an object that is not an instance of <code>nn.Module</code>.</p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> half <p>Casts all floating point parameters and buffers to <code>half</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> ipu <p>Move all model parameters and buffers to the IPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing the optimizer if the module will live on IPU while being optimized.  .. note::     This method modifies the module in-place.  Arguments:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[torch.device, int, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> load_state_dict <p>Copy parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants.</p> <p>If :attr:<code>strict</code> is <code>True</code>, then the keys of :attr:<code>state_dict</code> must exactly match the keys returned by this module's :meth:<code>~torch.nn.Module.state_dict</code> function.  .. warning::     If :attr:<code>assign</code> is <code>True</code> the optimizer must be created after     the call to :attr:<code>load_state_dict</code> unless     :func:<code>~torch.__future__.get_swap_module_params_on_conversion</code> is <code>True</code>.  Args:     state_dict (dict): a dict containing parameters and         persistent buffers.     strict (bool, optional): whether to strictly enforce that the keys         in :attr:<code>state_dict</code> match the keys returned by this module's         :meth:<code>~torch.nn.Module.state_dict</code> function. Default: <code>True</code>     assign (bool, optional): When set to <code>False</code>, the properties of the tensors         in the current module are preserved whereas setting it to <code>True</code> preserves         properties of the Tensors in the state dict. The only         exception is the <code>requires_grad</code> field of :class:<code>~torch.nn.Parameter</code>         for which the value from the module is preserved. Default: <code>False</code>  Returns:     <code>NamedTuple</code> with <code>missing_keys</code> and <code>unexpected_keys</code> fields:         * <code>missing_keys</code> is a list of str containing any keys that are expected             by this module but missing from the provided <code>state_dict</code>.         * <code>unexpected_keys</code> is a list of str containing the keys that are not             expected by this module but present in the provided <code>state_dict</code>.  Note:     If a parameter or buffer is registered as <code>None</code> and its corresponding key     exists in :attr:<code>state_dict</code>, :meth:<code>load_state_dict</code> will raise a     <code>RuntimeError</code>.</p> <p>Parameters</p> <ul> <li>state_dict     (collections.abc.Mapping[str, typing.Any])    </li> <li>strict     (bool)     \u2013 defaults to <code>True</code> </li> <li>assign     (bool)     \u2013 defaults to <code>False</code> </li> </ul> modules <p>Return an iterator over all modules in the network.</p> <p>Yields:     Module: a module in the network  Note:     Duplicate modules are returned only once. In the following     example, <code>l</code> will be returned only once.  Example::      &gt;&gt;&gt; l = nn.Linear(2, 2)     &gt;&gt;&gt; net = nn.Sequential(l, l)     &gt;&gt;&gt; for idx, m in enumerate(net.modules()):     ...     print(idx, '-&gt;', m)      0 -&gt; Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     )     1 -&gt; Linear(in_features=2, out_features=2, bias=True)</p> mtia <p>Move all model parameters and buffers to the MTIA.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing the optimizer if the module will live on MTIA while being optimized.  .. note::     This method modifies the module in-place.  Arguments:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[torch.device, int, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> named_buffers <p>Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.</p> <p>Args:     prefix (str): prefix to prepend to all buffer names.     recurse (bool, optional): if True, then yields buffers of this module         and all submodules. Otherwise, yields only buffers that         are direct members of this module. Defaults to True.     remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.  Yields:     (str, torch.Tensor): Tuple containing the name and buffer  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, buf in self.named_buffers():     &gt;&gt;&gt;     if name in ['running_var']:     &gt;&gt;&gt;         print(buf.size())</p> <p>Parameters</p> <ul> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> named_children <p>Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.</p> <p>Yields:     (str, Module): Tuple containing a name and child module  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, module in model.named_children():     &gt;&gt;&gt;     if name in ['conv4', 'conv5']:     &gt;&gt;&gt;         print(module)</p> named_modules <p>Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.</p> <p>Args:     memo: a memo to store the set of modules already added to the result     prefix: a prefix that will be added to the name of the module     remove_duplicate: whether to remove the duplicated module instances in the result         or not  Yields:     (str, Module): Tuple of name and module  Note:     Duplicate modules are returned only once. In the following     example, <code>l</code> will be returned only once.  Example::      &gt;&gt;&gt; l = nn.Linear(2, 2)     &gt;&gt;&gt; net = nn.Sequential(l, l)     &gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):     ...     print(idx, '-&gt;', m)      0 -&gt; ('', Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     ))     1 -&gt; ('0', Linear(in_features=2, out_features=2, bias=True))</p> <p>Parameters</p> <ul> <li>memo     (Optional[set['Module']])     \u2013 defaults to <code>None</code> </li> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> named_parameters <p>Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.</p> <p>Args:     prefix (str): prefix to prepend to all parameter names.     recurse (bool): if True, then yields parameters of this module         and all submodules. Otherwise, yields only parameters that         are direct members of this module.     remove_duplicate (bool, optional): whether to remove the duplicated         parameters in the result. Defaults to True.  Yields:     (str, Parameter): Tuple containing the name and parameter  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, param in self.named_parameters():     &gt;&gt;&gt;     if name in ['bias']:     &gt;&gt;&gt;         print(param.size())</p> <p>Parameters</p> <ul> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> parameters <p>Return an iterator over module parameters.</p> <p>This is typically passed to an optimizer.  Args:     recurse (bool): if True, then yields parameters of this module         and all submodules. Otherwise, yields only parameters that         are direct members of this module.  Yields:     Parameter: module parameter  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for param in model.parameters():     &gt;&gt;&gt;     print(type(param), param.size())      (20L,)      (20L, 1L, 5L, 5L) <p>Parameters</p> <ul> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> </ul> register_backward_hook <p>Register a backward hook on the module.</p> <p>This function is deprecated in favor of :meth:<code>~torch.nn.Module.register_full_backward_hook</code> and the behavior of this function will change in future versions.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[tuple[torch.Tensor, ...], torch.Tensor], Union[tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]])    </li> </ul> register_buffer <p>Add a buffer to the module.</p> <p>This is typically used to register a buffer that should not be considered a model parameter. For example, BatchNorm's <code>running_mean</code> is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:<code>persistent</code> to <code>False</code>. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:<code>state_dict</code>.  Buffers can be accessed as attributes using given names.  Args:     name (str): name of the buffer. The buffer can be accessed         from this module using the given name     tensor (Tensor or None): buffer to be registered. If <code>None</code>, then operations         that run on buffers, such as :attr:<code>cuda</code>, are ignored. If <code>None</code>,         the buffer is not included in the module's :attr:<code>state_dict</code>.     persistent (bool): whether the buffer is part of this module's         :attr:<code>state_dict</code>.  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; self.register_buffer('running_mean', torch.zeros(num_features))</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>tensor     (Optional[torch.Tensor])    </li> <li>persistent     (bool)     \u2013 defaults to <code>True</code> </li> </ul> register_forward_hook <p>Register a forward hook on the module.</p> <p>The hook will be called every time after :func:<code>forward</code> has computed an output.  If <code>with_kwargs</code> is <code>False</code> or not specified, the input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the <code>forward</code>. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func:<code>forward</code> is called. The hook should have the following signature::      hook(module, args, output) -&gt; None or modified output  If <code>with_kwargs</code> is <code>True</code>, the forward hook will be passed the <code>kwargs</code> given to the forward function and be expected to return the output possibly modified. The hook should have the following signature::      hook(module, args, kwargs, output) -&gt; None or modified output  Args:     hook (Callable): The user defined hook to be registered.     prepend (bool): If <code>True</code>, the provided <code>hook</code> will be fired         before all existing <code>forward</code> hooks on this         :class:<code>torch.nn.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>forward</code> hooks on         this :class:<code>torch.nn.Module</code>. Note that global         <code>forward</code> hooks registered with         :func:<code>register_module_forward_hook</code> will fire before all hooks         registered by this method.         Default: <code>False</code>     with_kwargs (bool): If <code>True</code>, the <code>hook</code> will be passed the         kwargs given to the forward function.         Default: <code>False</code>     always_call (bool): If <code>True</code> the <code>hook</code> will be run regardless of         whether an exception is raised while calling the Module.         Default: <code>False</code>  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Union[Callable[[~T, tuple[Any, ...], Any], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Optional[Any]]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> <li>with_kwargs     (bool)     \u2013 defaults to <code>False</code> </li> <li>always_call     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_forward_pre_hook <p>Register a forward pre-hook on the module.</p> <p>The hook will be called every time before :func:<code>forward</code> is invoked.  If <code>with_kwargs</code> is false or not specified, the input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the <code>forward</code>. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned (unless that value is already a tuple). The hook should have the following signature::      hook(module, args) -&gt; None or modified input  If <code>with_kwargs</code> is true, the forward pre-hook will be passed the kwargs given to the forward function. And if the hook modifies the input, both the args and kwargs should be returned. The hook should have the following signature::      hook(module, args, kwargs) -&gt; None or a tuple of modified input and kwargs  Args:     hook (Callable): The user defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>forward_pre</code> hooks on this         :class:<code>torch.nn.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>forward_pre</code> hooks         on this :class:<code>torch.nn.Module</code>. Note that global         <code>forward_pre</code> hooks registered with         :func:<code>register_module_forward_pre_hook</code> will fire before all         hooks registered by this method.         Default: <code>False</code>     with_kwargs (bool): If true, the <code>hook</code> will be passed the kwargs         given to the forward function.         Default: <code>False</code>  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Union[Callable[[~T, tuple[Any, ...]], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any]], Optional[tuple[Any, dict[str, Any]]]]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> <li>with_kwargs     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_full_backward_hook <p>Register a backward hook on the module.</p> <p>The hook will be called every time the gradients with respect to a module are computed, and its firing rules are as follows:      1. Ordinarily, the hook fires when the gradients are computed with respect to the module inputs.     2. If none of the module inputs require gradients, the hook will fire when the gradients are computed        with respect to module outputs.     3. If none of the module outputs require gradients, then the hooks will not fire.  The hook should have the following signature::      hook(module, grad_input, grad_output) -&gt; tuple(Tensor) or None  The :attr:<code>grad_input</code> and :attr:<code>grad_output</code> are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr:<code>grad_input</code> in subsequent computations. :attr:<code>grad_input</code> will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr:<code>grad_input</code> and :attr:<code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.  For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function.  .. warning ::     Modifying inputs or outputs inplace is not allowed when using backward hooks and     will raise an error.  Args:     hook (Callable): The user-defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>backward</code> hooks on this         :class:<code>torch.nn.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>backward</code> hooks on         this :class:<code>torch.nn.Module</code>. Note that global         <code>backward</code> hooks registered with         :func:<code>register_module_full_backward_hook</code> will fire before         all hooks registered by this method.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[tuple[torch.Tensor, ...], torch.Tensor], Union[tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_full_backward_pre_hook <p>Register a backward pre-hook on the module.</p> <p>The hook will be called every time the gradients for the module are computed. The hook should have the following signature::      hook(module, grad_output) -&gt; tuple[Tensor] or None  The :attr:<code>grad_output</code> is a tuple. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the output that will be used in place of :attr:<code>grad_output</code> in subsequent computations. Entries in :attr:<code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.  For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function.  .. warning ::     Modifying inputs inplace is not allowed when using backward hooks and     will raise an error.  Args:     hook (Callable): The user-defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>backward_pre</code> hooks on this         :class:<code>torch.nn.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>backward_pre</code> hooks         on this :class:<code>torch.nn.Module</code>. Note that global         <code>backward_pre</code> hooks registered with         :func:<code>register_module_full_backward_pre_hook</code> will fire before         all hooks registered by this method.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_load_state_dict_post_hook <p>Register a post-hook to be run after module's :meth:<code>~nn.Module.load_state_dict</code> is called.</p> <p>It should have the following signature::     hook(module, incompatible_keys) -&gt; None  The <code>module</code> argument is the current module that this hook is registered on, and the <code>incompatible_keys</code> argument is a <code>NamedTuple</code> consisting of attributes <code>missing_keys</code> and <code>unexpected_keys</code>. <code>missing_keys</code> is a <code>list</code> of <code>str</code> containing the missing keys and <code>unexpected_keys</code> is a <code>list</code> of <code>str</code> containing the unexpected keys.  The given incompatible_keys can be modified inplace if needed.  Note that the checks performed when calling :func:<code>load_state_dict</code> with <code>strict=True</code> are affected by modifications the hook makes to <code>missing_keys</code> or <code>unexpected_keys</code>, as expected. Additions to either set of keys will result in an error being thrown when <code>strict=True</code>, and clearing out both missing and unexpected keys will avoid an error.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook </li> </ul> register_load_state_dict_pre_hook <p>Register a pre-hook to be run before module's :meth:<code>~nn.Module.load_state_dict</code> is called.</p> <p>It should have the following signature::     hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -&gt; None  # noqa: B950  Arguments:     hook (Callable): Callable hook that will be invoked before         loading the state dict.</p> <p>Parameters</p> <ul> <li>hook </li> </ul> register_module <p>Alias for :func:<code>add_module</code>.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>module     (Optional[ForwardRef('Module')])    </li> </ul> register_parameter <p>Add a parameter to the module.</p> <p>The parameter can be accessed as an attribute using given name.  Args:     name (str): name of the parameter. The parameter can be accessed         from this module using the given name     param (Parameter or None): parameter to be added to the module. If         <code>None</code>, then operations that run on parameters, such as :attr:<code>cuda</code>,         are ignored. If <code>None</code>, the parameter is not included in the         module's :attr:<code>state_dict</code>.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>param     (Optional[torch.nn.parameter.Parameter])    </li> </ul> register_state_dict_post_hook <p>Register a post-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method.</p> <p>It should have the following signature::     hook(module, state_dict, prefix, local_metadata) -&gt; None  The registered hooks can modify the <code>state_dict</code> inplace.</p> <p>Parameters</p> <ul> <li>hook </li> </ul> register_state_dict_pre_hook <p>Register a pre-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method.</p> <p>It should have the following signature::     hook(module, prefix, keep_vars) -&gt; None  The registered hooks can be used to perform pre-processing before the <code>state_dict</code> call is made.</p> <p>Parameters</p> <ul> <li>hook </li> </ul> requires_grad_ <p>Change if autograd should record operations on parameters in this module.</p> <p>This method sets the parameters' :attr:<code>requires_grad</code> attributes in-place.  This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).  See :ref:<code>locally-disable-grad-doc</code> for a comparison between <code>.requires_grad_()</code> and several similar mechanisms that may be confused with it.  Args:     requires_grad (bool): whether autograd should record operations on                           parameters in this module. Default: <code>True</code>.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>requires_grad     (bool)     \u2013 defaults to <code>True</code> </li> </ul> set_extra_state <p>Set extra state contained in the loaded <code>state_dict</code>.</p> <p>This function is called from :func:<code>load_state_dict</code> to handle any extra state found within the <code>state_dict</code>. Implement this function and a corresponding :func:<code>get_extra_state</code> for your module if you need to store extra state within its <code>state_dict</code>.  Args:     state (dict): Extra state from the <code>state_dict</code></p> <p>Parameters</p> <ul> <li>state     (Any)    </li> </ul> set_submodule <p>Set the submodule given by <code>target</code> if it exists, otherwise throw an error.</p> <p>.. note::     If <code>strict</code> is set to <code>False</code> (default), the method will replace an existing submodule     or create a new submodule if the parent module exists. If <code>strict</code> is set to <code>True</code>,     the method will only attempt to replace an existing submodule and throw an error if     the submodule does not exist.  For example, let's say you have an <code>nn.Module</code> <code>A</code> that looks like this:  .. code-block:: text      A(         (net_b): Module(             (net_c): Module(                 (conv): Conv2d(3, 3, 3)             )             (linear): Linear(3, 3)         )     )  (The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)  To override the <code>Conv2d</code> with a new submodule <code>Linear</code>, you could call <code>set_submodule(\"net_b.net_c.conv\", nn.Linear(1, 1))</code> where <code>strict</code> could be <code>True</code> or <code>False</code>  To add a new submodule <code>Conv2d</code> to the existing <code>net_b</code> module, you would call <code>set_submodule(\"net_b.conv\", nn.Conv2d(1, 1, 1))</code>.  In the above if you set <code>strict=True</code> and call <code>set_submodule(\"net_b.conv\", nn.Conv2d(1, 1, 1), strict=True)</code>, an AttributeError will be raised because <code>net_b</code> does not have a submodule named <code>conv</code>.  Args:     target: The fully-qualified string name of the submodule         to look for. (See above example for how to specify a         fully-qualified string.)     module: The module to set the submodule to.     strict: If <code>False</code>, the method will replace an existing submodule         or create a new submodule if the parent module exists. If <code>True</code>,         the method will only attempt to replace an existing submodule and throw an error         if the submodule doesn't already exist.  Raises:     ValueError: If the <code>target</code> string is empty or if <code>module</code> is not an instance of <code>nn.Module</code>.     AttributeError: If at any point along the path resulting from         the <code>target</code> string the (sub)path resolves to a non-existent         attribute name or an object that is not an instance of <code>nn.Module</code>.</p> <p>Parameters</p> <ul> <li>target     (str)    </li> <li>module     ('Module')    </li> <li>strict     (bool)     \u2013 defaults to <code>False</code> </li> </ul> share_memory <p>See :meth:<code>torch.Tensor.share_memory_</code>.</p> state_dict <p>Return a dictionary containing references to the whole state of the module.</p> <p>Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to <code>None</code> are not included.  .. note::     The returned object is a shallow copy. It contains references     to the module's parameters and buffers.  .. warning::     Currently <code>state_dict()</code> also accepts positional arguments for     <code>destination</code>, <code>prefix</code> and <code>keep_vars</code> in order. However,     this is being deprecated and keyword arguments will be enforced in     future releases.  .. warning::     Please avoid the use of argument <code>destination</code> as it is not     designed for end-users.  Args:     destination (dict, optional): If provided, the state of module will         be updated into the dict and the same object is returned.         Otherwise, an <code>OrderedDict</code> will be created and returned.         Default: <code>None</code>.     prefix (str, optional): a prefix added to parameter and buffer         names to compose the keys in state_dict. Default: <code>''</code>.     keep_vars (bool, optional): by default the :class:<code>~torch.Tensor</code> s         returned in the state dict are detached from autograd. If it's         set to <code>True</code>, detaching will not be performed.         Default: <code>False</code>.  Returns:     dict:         a dictionary containing a whole state of the module  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; module.state_dict().keys()     ['bias', 'weight']</p> <p>Parameters</p> <ul> <li>args </li> <li>destination     \u2013 defaults to <code>None</code> </li> <li>prefix     \u2013 defaults to ``    </li> <li>keep_vars     \u2013 defaults to <code>False</code> </li> </ul> to <p>Move and/or cast the parameters and buffers.</p> <p>This can be called as  .. function:: to(device=None, dtype=None, non_blocking=False)    :noindex:  .. function:: to(dtype, non_blocking=False)    :noindex:  .. function:: to(tensor, non_blocking=False)    :noindex:  .. function:: to(memory_format=torch.channels_last)    :noindex:  Its signature is similar to :meth:<code>torch.Tensor.to</code>, but only accepts floating point or complex :attr:<code>dtype</code>\\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr:<code>dtype</code> (if given). The integral parameters and buffers will be moved :attr:<code>device</code>, if that is given, but with dtypes unchanged. When :attr:<code>non_blocking</code> is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices.  See below for examples.  .. note::     This method modifies the module in-place.  Args:     device (:class:<code>torch.device</code>): the desired device of the parameters         and buffers in this module     dtype (:class:<code>torch.dtype</code>): the desired floating point or complex dtype of         the parameters and buffers in this module     tensor (torch.Tensor): Tensor whose dtype and device are the desired         dtype and device for all parameters and buffers in this module     memory_format (:class:<code>torch.memory_format</code>): the desired memory         format for 4D parameters and buffers in this module (keyword         only argument)  Returns:     Module: self  Examples::      &gt;&gt;&gt; # xdoctest: +IGNORE_WANT(\"non-deterministic\")     &gt;&gt;&gt; linear = nn.Linear(2, 2)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1913, -0.3420],             [-0.5113, -0.2325]])     &gt;&gt;&gt; linear.to(torch.double)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1913, -0.3420],             [-0.5113, -0.2325]], dtype=torch.float64)     &gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)     &gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")     &gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1914, -0.3420],             [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')     &gt;&gt;&gt; cpu = torch.device(\"cpu\")     &gt;&gt;&gt; linear.to(cpu)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1914, -0.3420],             [-0.5112, -0.2324]], dtype=torch.float16)      &gt;&gt;&gt; linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.3741+0.j,  0.2382+0.j],             [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)     &gt;&gt;&gt; linear(torch.ones(3, 2, dtype=torch.cdouble))     tensor([[0.6122+0.j, 0.1150+0.j],             [0.6122+0.j, 0.1150+0.j],             [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)</p> <p>Parameters</p> <ul> <li>args </li> <li>kwargs </li> </ul> to_empty <p>Move the parameters and buffers to the specified device without copying storage.</p> <p>Args:     device (:class:<code>torch.device</code>): The desired device of the parameters         and buffers in this module.     recurse (bool): Whether parameters and buffers of submodules should         be recursively moved to the specified device.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, str, torch.device, NoneType])    </li> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> </ul> train <p>Set the module in training mode.</p> <p>This has an effect only on certain modules. See the documentation of particular modules for details of their behaviors in training/evaluation mode, i.e., whether they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>, etc.  Args:     mode (bool): whether to set training mode (<code>True</code>) or evaluation                  mode (<code>False</code>). Default: <code>True</code>.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>mode     (bool)     \u2013 defaults to <code>True</code> </li> </ul> type <p>Casts all parameters and buffers to :attr:<code>dst_type</code>.</p> <p>.. note::     This method modifies the module in-place.  Args:     dst_type (type or string): the desired type  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>dst_type     (Union[torch.dtype, str])    </li> </ul> xpu <p>Move all model parameters and buffers to the XPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized.  .. note::     This method modifies the module in-place.  Arguments:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[torch.device, int, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> zero_grad <p>Reset gradients of all model parameters.</p> <p>See similar function under :class:<code>torch.optim.Optimizer</code> for more context.  Args:     set_to_none (bool): instead of setting to zero, set the grads to None.         See :meth:<code>torch.optim.Optimizer.zero_grad</code> for details.</p> <p>Parameters</p> <ul> <li>set_to_none     (bool)     \u2013 defaults to <code>True</code> </li> </ul>"},{"location":"api/models/ColBERT/","title":"ColBERT","text":"<p>Loads or creates a ColBERT model that can be used to map sentences / text to multi-vectors embeddings.</p>"},{"location":"api/models/ColBERT/#parameters","title":"Parameters","text":"<ul> <li> <p>model_name_or_path ('str | None') \u2013 defaults to <code>None</code></p> <p>If it is a filepath on disc, it loads the model from that path. If it is not a path, it first tries to download a pre-trained SentenceTransformer model. If that fails, tries to construct a model from the Hugging Face Hub with that name.</p> </li> <li> <p>modules ('Optional[Iterable[nn.Module]]') \u2013 defaults to <code>None</code></p> <p>A list of torch Modules that should be called sequentially, can be used to create custom SentenceTransformer models from scratch.</p> </li> <li> <p>device ('str | None') \u2013 defaults to <code>None</code></p> <p>Device (like \"cuda\", \"cpu\", \"mps\", \"npu\") that should be used for computation. If None, checks if a GPU can be used.</p> </li> <li> <p>prompts ('dict[str, str] | None') \u2013 defaults to <code>None</code></p> <p>A dictionary with prompts for the model. The key is the prompt name, the value is the prompt text. The prompt text will be prepended before any text to encode. For example: <code>{\"query\": \"query: \", \"passage\": \"passage: \"}</code> or <code>{\"clustering\": \"Identify the main category based on the titles in \"}</code>.</p> </li> <li> <p>default_prompt_name ('str | None') \u2013 defaults to <code>None</code></p> <p>The name of the prompt that should be used by default. If not set, no prompt will be applied.</p> </li> <li> <p>similarity_fn_name ('Optional[str | SimilarityFunction]') \u2013 defaults to <code>None</code></p> <p>The name of the similarity function to use. Valid options are \"cosine\", \"dot\", \"euclidean\", and \"manhattan\". If not set, it is automatically set to \"cosine\" if <code>similarity</code> or <code>similarity_pairwise</code> are called while <code>model.similarity_fn_name</code> is still <code>None</code>.</p> </li> <li> <p>cache_folder ('str | None') \u2013 defaults to <code>None</code></p> <p>Path to store models. Can also be set by the SENTENCE_TRANSFORMERS_HOME environment variable.</p> </li> <li> <p>trust_remote_code ('bool') \u2013 defaults to <code>False</code></p> <p>Whether or not to allow for custom models defined on the Hub in their own modeling files. This option should only be set to True for repositories you trust and in which you have read the code, as it will execute code present on the Hub on your local machine.</p> </li> <li> <p>revision ('str | None') \u2013 defaults to <code>None</code></p> <p>The specific model version to use. It can be a branch name, a tag name, or a commit id, for a stored model on Hugging Face.</p> </li> <li> <p>local_files_only ('bool') \u2013 defaults to <code>False</code></p> <p>Whether or not to only look at local files (i.e., do not try to download the model).</p> </li> <li> <p>token ('bool | str | None') \u2013 defaults to <code>None</code></p> <p>Hugging Face authentication token to download private models.</p> </li> <li> <p>use_auth_token ('bool | str | None') \u2013 defaults to <code>None</code></p> <p>Deprecated argument. Please use <code>token</code> instead.</p> </li> <li> <p>truncate_dim ('int | None') \u2013 defaults to <code>None</code></p> <p>The dimension to truncate sentence embeddings to. <code>None</code> does no truncation. Truncation is only applicable during inference when :meth:<code>SentenceTransformer.encode</code> is called.</p> </li> <li> <p>embedding_size ('int | None') \u2013 defaults to <code>None</code></p> <p>The output size of the projection layer. Default to 128.</p> </li> <li> <p>bias ('bool') \u2013 defaults to <code>False</code></p> </li> <li> <p>query_prefix ('str | None') \u2013 defaults to <code>None</code></p> <p>Prefix to add to the queries.</p> </li> <li> <p>document_prefix ('str | None') \u2013 defaults to <code>None</code></p> <p>Prefix to add to the documents.</p> </li> <li> <p>add_special_tokens ('bool') \u2013 defaults to <code>True</code></p> <p>Add the prefix to the inputs.</p> </li> <li> <p>truncation ('bool') \u2013 defaults to <code>True</code></p> <p>Truncate the inputs to the encoder max lengths or use sliding window encoding.</p> </li> <li> <p>query_length ('int | None') \u2013 defaults to <code>None</code></p> <p>The length of the query to truncate/pad to with mask tokens. If set, will override the config value. Default to 32.</p> </li> <li> <p>document_length ('int | None') \u2013 defaults to <code>None</code></p> <p>The max length of the document to truncate. If set, will override the config value. Default to 180.</p> </li> <li> <p>do_query_expansion ('bool | None') \u2013 defaults to <code>None</code></p> <p>Whether to do query expansion. If True, will pad the query to the <code>query_length</code> with mask tokens. Default to True.</p> </li> <li> <p>attend_to_expansion_tokens ('bool | None') \u2013 defaults to <code>None</code></p> <p>Whether to attend to the expansion tokens in the attention layers model. If False, the original tokens will not only attend to the expansion tokens, only the expansion tokens will attend to the original tokens. Default is False (as in the original ColBERT codebase).</p> </li> <li> <p>skiplist_words ('list[str] | None') \u2013 defaults to <code>None</code></p> <p>A list of words to skip from the documents scoring (note that these tokens are used for encoding and are only skipped during the scoring). Default is the list of string.punctuation.</p> </li> <li> <p>model_kwargs ('dict | None') \u2013 defaults to <code>None</code></p> <p>Additional model configuration parameters to be passed to the Huggingface Transformers model. Particularly useful options are:  - <code>torch_dtype</code>: Override the default <code>torch.dtype</code> and load the model under a specific <code>dtype</code>. The     different options are:          1. <code>torch.float16</code>, <code>torch.bfloat16</code> or <code>torch.float</code>: load in a specified <code>dtype</code>,         ignoring the model's <code>config.torch_dtype</code> if one exists. If not specified - the model will get         loaded in <code>torch.float</code> (fp32).          2. <code>\"auto\"</code> - A <code>torch_dtype</code> entry in the <code>config.json</code> file of the model will be attempted         to be used. If this entry isn't found then next check the <code>dtype</code> of the first weight in the         checkpoint that's of a floating point type and use that as <code>dtype</code>. This will load the model using         the <code>dtype</code> it was saved in at the end of the training. It can't be used as an indicator of how the         model was trained. Since it could be trained in one of half precision dtypes, but saved in fp32. - <code>attn_implementation</code>: The attention implementation to use in the model (if relevant). Can be any of     <code>\"eager\"</code> (manual implementation of the attention), <code>\"sdpa\"</code> (using <code>F.scaled_dot_product_attention     &lt;https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html&gt;</code>),     or <code>\"flash_attention_2\"</code> (using <code>Dao-AILab/flash-attention     &lt;https://github.com/Dao-AILab/flash-attention&gt;</code>). By default, if available, SDPA will be used for     torch&gt;=2.1.1. The default is otherwise the manual <code>\"eager\"</code> implementation.  See the <code>PreTrainedModel.from_pretrained &lt;https://huggingface.co/docs/transformers/en/main_classes/model#transformers.PreTrainedModel.from_pretrained&gt;</code>_ documentation for more details.</p> </li> <li> <p>tokenizer_kwargs ('dict | None') \u2013 defaults to <code>None</code></p> <p>Additional tokenizer configuration parameters to be passed to the Huggingface Transformers tokenizer. See the <code>AutoTokenizer.from_pretrained &lt;https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained&gt;</code>_ documentation for more details.</p> </li> <li> <p>config_kwargs ('dict | None') \u2013 defaults to <code>None</code></p> <p>Additional model configuration parameters to be passed to the Huggingface Transformers config. See the <code>AutoConfig.from_pretrained &lt;https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoConfig.from_pretrained&gt;</code>_ documentation for more details.</p> </li> <li> <p>model_card_data ('PylateModelCardData | None') \u2013 defaults to <code>None</code></p> <p>A model card data object that contains information about the model. This is used to generate a model card when saving the model. If not set, a default model card data object is created.</p> </li> </ul>"},{"location":"api/models/ColBERT/#attributes","title":"Attributes","text":"<ul> <li> <p>device</p> <p>Get torch.device from module, assuming that the whole module has one device. In case there are no PyTorch parameters, fall back to CPU.</p> </li> <li> <p>dtype</p> </li> <li> <p>max_seq_length</p> <p>Returns the maximal input sequence length for the model. Longer inputs will be truncated.  Returns:     int: The maximal input sequence length.  Example:     ::          from sentence_transformers import SentenceTransformer          model = SentenceTransformer(\"all-mpnet-base-v2\")         print(model.max_seq_length)         # =&gt; 384</p> </li> <li> <p>similarity</p> <p>Compute the similarity between two collections of embeddings. The output will be a matrix with the similarity scores between all embeddings from the first parameter and all embeddings from the second parameter. This differs from <code>similarity_pairwise</code> which computes the similarity between each pair of embeddings. This method supports only embeddings with fp32 precision and does not accommodate quantized embeddings.  Args:     embeddings1 (Union[Tensor, ndarray]): [num_embeddings_1, embedding_dim] or [embedding_dim]-shaped numpy array or torch tensor.     embeddings2 (Union[Tensor, ndarray]): [num_embeddings_2, embedding_dim] or [embedding_dim]-shaped numpy array or torch tensor.  Returns:     Tensor: A [num_embeddings_1, num_embeddings_2]-shaped torch tensor with similarity scores.  Example:     ::          &gt;&gt;&gt; model = SentenceTransformer(\"all-mpnet-base-v2\")         &gt;&gt;&gt; sentences = [         ...     \"The weather is so nice!\",         ...     \"It's so sunny outside.\",         ...     \"He's driving to the movie theater.\",         ...     \"She's going to the cinema.\",         ... ]         &gt;&gt;&gt; embeddings = model.encode(sentences, normalize_embeddings=True)         &gt;&gt;&gt; model.similarity(embeddings, embeddings)         tensor([[1.0000, 0.7235, 0.0290, 0.1309],                 [0.7235, 1.0000, 0.0613, 0.1129],                 [0.0290, 0.0613, 1.0000, 0.5027],                 [0.1309, 0.1129, 0.5027, 1.0000]])         &gt;&gt;&gt; model.similarity_fn_name         \"cosine\"         &gt;&gt;&gt; model.similarity_fn_name = \"euclidean\"         &gt;&gt;&gt; model.similarity(embeddings, embeddings)         tensor([[-0.0000, -0.7437, -1.3935, -1.3184],                 [-0.7437, -0.0000, -1.3702, -1.3320],                 [-1.3935, -1.3702, -0.0000, -0.9973],                 [-1.3184, -1.3320, -0.9973, -0.0000]])</p> </li> <li> <p>similarity_fn_name</p> <p>Return the name of the similarity function used by :meth:<code>SentenceTransformer.similarity</code> and :meth:<code>SentenceTransformer.similarity_pairwise</code>.  Returns:     Optional[str]: The name of the similarity function. Can be None if not set, in which case it will         default to \"cosine\" when first called. Examples -------- &gt;&gt;&gt; model = ColBERT(\"bert-base-uncased\") &gt;&gt;&gt; model.similarity_fn_name     'MaxSim'</p> </li> <li> <p>similarity_pairwise</p> <p>Compute the similarity between two collections of embeddings. The output will be a vector with the similarity scores between each pair of embeddings. This method supports only embeddings with fp32 precision and does not accommodate quantized embeddings.  Args:     embeddings1 (Union[Tensor, ndarray]): [num_embeddings, embedding_dim] or [embedding_dim]-shaped numpy array or torch tensor.     embeddings2 (Union[Tensor, ndarray]): [num_embeddings, embedding_dim] or [embedding_dim]-shaped numpy array or torch tensor.  Returns:     Tensor: A [num_embeddings]-shaped torch tensor with pairwise similarity scores.  Example:     ::          &gt;&gt;&gt; model = SentenceTransformer(\"all-mpnet-base-v2\")         &gt;&gt;&gt; sentences = [         ...     \"The weather is so nice!\",         ...     \"It's so sunny outside.\",         ...     \"He's driving to the movie theater.\",         ...     \"She's going to the cinema.\",         ... ]         &gt;&gt;&gt; embeddings = model.encode(sentences, normalize_embeddings=True)         &gt;&gt;&gt; model.similarity_pairwise(embeddings[::2], embeddings[1::2])         tensor([0.7235, 0.5027])         &gt;&gt;&gt; model.similarity_fn_name         \"cosine\"         &gt;&gt;&gt; model.similarity_fn_name = \"euclidean\"         &gt;&gt;&gt; model.similarity_pairwise(embeddings[::2], embeddings[1::2])         tensor([-0.7437, -0.9973])</p> </li> <li> <p>tokenizer</p> <p>Property to get the tokenizer that is used by this model</p> </li> <li> <p>transformers_model</p> <p>Property to get the underlying transformers PreTrainedModel instance, if it exists. Note that it's possible for a model to have multiple underlying transformers models, but this property will return the first one it finds in the module hierarchy.  Returns:     PreTrainedModel or None: The underlying transformers model or None if not found.  Example:     ::          from sentence_transformers import SentenceTransformer          model = SentenceTransformer(\"all-mpnet-base-v2\")          # You can now access the underlying transformers model         transformers_model = model.transformers_model         print(type(transformers_model))         # =&gt;"},{"location":"api/models/ColBERT/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from pylate import models\n\n&gt;&gt;&gt; model = models.ColBERT(\n...     model_name_or_path=\"sentence-transformers/all-MiniLM-L6-v2\",\n...     device=\"cpu\",\n... )\n\n&gt;&gt;&gt; embeddings = model.encode(\"Hello, how are you?\")\n&gt;&gt;&gt; assert isinstance(embeddings, np.ndarray)\n\n&gt;&gt;&gt; embeddings = model.encode([\n...     \"Hello, how are you?\",\n...     \"How is the weather today?\"\n... ])\n\n&gt;&gt;&gt; assert len(embeddings) == 2\n&gt;&gt;&gt; assert isinstance(embeddings[0], np.ndarray)\n&gt;&gt;&gt; assert isinstance(embeddings[1], np.ndarray)\n\n&gt;&gt;&gt; embeddings = model.encode([\n...     [\n...         \"Hello, how are you?\",\n...         \"How is the weather today?\"\n...     ],\n...     [\n...         \"Hello, how are you?\",\n...         \"How is the weather today?\"\n...     ],\n... ])\n\n&gt;&gt;&gt; assert len(embeddings) == 2\n\n&gt;&gt;&gt; model.save_pretrained(\"test-model\")\n\n&gt;&gt;&gt; model = models.ColBERT(\"test-model\")\n\n&gt;&gt;&gt; embeddings = model.encode([\n...     \"Hello, how are you?\",\n...     \"How is the weather today?\"\n... ])\n\n&gt;&gt;&gt; assert len(embeddings) == 2\n&gt;&gt;&gt; assert isinstance(embeddings[0], np.ndarray)\n&gt;&gt;&gt; assert isinstance(embeddings[1], np.ndarray)\n</code></pre>"},{"location":"api/models/ColBERT/#methods","title":"Methods","text":"call <p>Call self as a function.</p> <p>Parameters</p> <ul> <li>args </li> <li>kwargs </li> </ul> active_adapter active_adapters <p>If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the PEFT official documentation: https://huggingface.co/docs/peft</p> <p>Gets the current active adapters of the model. In case of multi-adapter inference (combining multiple adapters for inference) returns the list of all active adapters so that users can deal with them accordingly.  For previous PEFT versions (that does not support multi-adapter inference), <code>module.active_adapter</code> will return a single string.</p> add_adapter <p>Adds a fresh new adapter to the current model for training purposes. If no adapter name is passed, a default name is assigned to the adapter to follow the convention of PEFT library (in PEFT we use \"default\" as the default adapter name).</p> <p>Requires peft as a backend to load the adapter weights and the underlying model to be compatible with PEFT.  Args:     args:         Positional arguments to pass to the underlying AutoModel <code>add_adapter</code> function. More information can be found in the transformers documentation         https://huggingface.co/docs/transformers/main/en/main_classes/peft#transformers.integrations.PeftAdapterMixin.add_adapter     *kwargs:         Keyword arguments to pass to the underlying AutoModel <code>add_adapter</code> function. More information can be found in the transformers documentation         https://huggingface.co/docs/transformers/main/en/main_classes/peft#transformers.integrations.PeftAdapterMixin.add_adapter</p> <p>Parameters</p> <ul> <li>args </li> <li>kwargs </li> </ul> add_module <p>Add a child module to the current module.</p> <p>The module can be accessed as an attribute using the given name.  Args:     name (str): name of the child module. The child module can be         accessed from this module using the given name     module (Module): child module to be added to the module.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>module     (Optional[ForwardRef('Module')])    </li> </ul> append <p>Append a given module to the end.</p> <p>Args:     module (nn.Module): module to append  Example::      &gt;&gt;&gt; import torch.nn as nn     &gt;&gt;&gt; n = nn.Sequential(nn.Linear(1, 2), nn.Linear(2, 3))     &gt;&gt;&gt; n.append(nn.Linear(3, 4))     Sequential(         (0): Linear(in_features=1, out_features=2, bias=True)         (1): Linear(in_features=2, out_features=3, bias=True)         (2): Linear(in_features=3, out_features=4, bias=True)     )</p> <p>Parameters</p> <ul> <li>module     ('Module')    </li> </ul> apply <p>Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self.</p> <p>Typical use includes initializing the parameters of a model (see also :ref:<code>nn-init-doc</code>).  Args:     fn (:class:<code>Module</code> -&gt; None): function to be applied to each submodule  Returns:     Module: self  Example::      &gt;&gt;&gt; @torch.no_grad()     &gt;&gt;&gt; def init_weights(m):     &gt;&gt;&gt;     print(m)     &gt;&gt;&gt;     if type(m) == nn.Linear:     &gt;&gt;&gt;         m.weight.fill_(1.0)     &gt;&gt;&gt;         print(m.weight)     &gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))     &gt;&gt;&gt; net.apply(init_weights)     Linear(in_features=2, out_features=2, bias=True)     Parameter containing:     tensor([[1., 1.],             [1., 1.]], requires_grad=True)     Linear(in_features=2, out_features=2, bias=True)     Parameter containing:     tensor([[1., 1.],             [1., 1.]], requires_grad=True)     Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     )</p> <p>Parameters</p> <ul> <li>fn     (Callable[[ForwardRef('Module')], NoneType])    </li> </ul> bfloat16 <p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> buffers <p>Return an iterator over module buffers.</p> <p>Args:     recurse (bool): if True, then yields buffers of this module         and all submodules. Otherwise, yields only buffers that         are direct members of this module.  Yields:     torch.Tensor: module buffer  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for buf in model.buffers():     &gt;&gt;&gt;     print(type(buf), buf.size())      (20L,)      (20L, 1L, 5L, 5L) <p>Parameters</p> <ul> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> </ul> check_peft_compatible_model children <p>Return an iterator over immediate children modules.</p> <p>Yields:     Module: a child module</p> compile <p>Compile this Module's forward using :func:<code>torch.compile</code>.</p> <p>This Module's <code>__call__</code> method is compiled and all arguments are passed as-is to :func:<code>torch.compile</code>.  See :func:<code>torch.compile</code> for details on the arguments for this function.</p> <p>Parameters</p> <ul> <li>args </li> <li>kwargs </li> </ul> cpu <p>Move all model parameters and buffers to the CPU.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> cuda <p>Move all model parameters and buffers to the GPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing the optimizer if the module will live on GPU while being optimized.  .. note::     This method modifies the module in-place.  Args:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[torch.device, int, NoneType])     \u2013 defaults to <code>None</code>      Device (like \"cuda\", \"cpu\", \"mps\", \"npu\") that should be used for computation. If None, checks if a GPU can be used.</li> </ul> delete_adapter <p>If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the PEFT official documentation: https://huggingface.co/docs/peft</p> <p>Delete an adapter's LoRA layers from the underlying model.  Args:     args:         Positional arguments to pass to the underlying AutoModel <code>delete_adapter</code> function. More information can be found in the transformers documentation         https://huggingface.co/docs/transformers/main/en/main_classes/peft#transformers.integrations.PeftAdapterMixin.delete_adapter     *kwargs:         Keyword arguments to pass to the underlying AutoModel <code>delete_adapter</code> function. More information can be found in the transformers documentation         https://huggingface.co/docs/transformers/main/en/main_classes/peft#transformers.integrations.PeftAdapterMixin.delete_adapter</p> <p>Parameters</p> <ul> <li>args </li> <li>kwargs </li> </ul> disable_adapters <p>Disable all adapters that are attached to the model. This leads to inferring with the base model only.</p> double <p>Casts all floating point parameters and buffers to <code>double</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> enable_adapters <p>Enable adapters that are attached to the model. The model will use <code>self.active_adapter()</code></p> encode <p>Computes sentence embeddings.</p> <p>Parameters</p> <ul> <li>sentences     ('str | list[str]')    </li> <li>prompt_name     ('str | None')     \u2013 defaults to <code>None</code> </li> <li>prompt     ('str | None')     \u2013 defaults to <code>None</code> </li> <li>batch_size     ('int')     \u2013 defaults to <code>32</code> </li> <li>show_progress_bar     ('bool')     \u2013 defaults to <code>None</code> </li> <li>precision     (\"Literal['float32', 'int8', 'uint8', 'binary', 'ubinary']\")     \u2013 defaults to <code>float32</code> </li> <li>convert_to_numpy     ('bool')     \u2013 defaults to <code>True</code> </li> <li>convert_to_tensor     ('bool')     \u2013 defaults to <code>False</code> </li> <li>padding     ('bool')     \u2013 defaults to <code>False</code> </li> <li>device     ('str')     \u2013 defaults to <code>None</code>      Device (like \"cuda\", \"cpu\", \"mps\", \"npu\") that should be used for computation. If None, checks if a GPU can be used.</li> <li>normalize_embeddings     ('bool')     \u2013 defaults to <code>True</code> </li> <li>is_query     ('bool')     \u2013 defaults to <code>True</code> </li> <li>pool_factor     ('int')     \u2013 defaults to <code>1</code> </li> <li>protected_tokens     ('int')     \u2013 defaults to <code>1</code> </li> </ul> encode_document <p>Computes sentence embeddings specifically optimized for document/passage representation.</p> <p>This method is a specialized version of :meth:<code>encode</code> that differs in exactly two ways:  1. If no <code>prompt_name</code> or <code>prompt</code> is provided, it uses a predefined \"document\" prompt,    if available in the model's <code>prompts</code> dictionary. 2. It sets the <code>task</code> to \"document\". If the model has a :class:<code>~sentence_transformers.models.Router</code>    module, it will use the \"document\" task type to route the input through the appropriate submodules.  .. tip::      If you are unsure whether you should use :meth:<code>encode</code>, :meth:<code>encode_query</code>, or :meth:<code>encode_document</code>,     your best bet is to use :meth:<code>encode_query</code> and :meth:<code>encode_document</code> for Information Retrieval tasks     with clear query and document/passage distinction, and use :meth:<code>encode</code> for all other tasks.      Note that :meth:<code>encode</code> is the most general method and can be used for any task, including Information     Retrieval, and that if the model was not trained with predefined prompts and/or task types, then all three     methods will return identical embeddings.  Args:     sentences (Union[str, List[str]]): The sentences to embed.     prompt_name (Optional[str], optional): The name of the prompt to use for encoding. Must be a key in the <code>prompts</code> dictionary,         which is either set in the constructor or loaded from the model configuration. For example if         <code>prompt_name</code> is \"query\" and the <code>prompts</code> is {\"query\": \"query: \", ...}, then the sentence \"What         is the capital of France?\" will be encoded as \"query: What is the capital of France?\" because the sentence         is appended to the prompt. If <code>prompt</code> is also set, this argument is ignored. Defaults to None.     prompt (Optional[str], optional): The prompt to use for encoding. For example, if the prompt is \"query: \", then the         sentence \"What is the capital of France?\" will be encoded as \"query: What is the capital of France?\"         because the sentence is appended to the prompt. If <code>prompt</code> is set, <code>prompt_name</code> is ignored. Defaults to None.     batch_size (int, optional): The batch size used for the computation. Defaults to 32.     show_progress_bar (bool, optional): Whether to output a progress bar when encode sentences. Defaults to None.     output_value (Optional[Literal[\"sentence_embedding\", \"token_embeddings\"]], optional): The type of embeddings to return:         \"sentence_embedding\" to get sentence embeddings, \"token_embeddings\" to get wordpiece token embeddings, and <code>None</code>,         to get all output values. Defaults to \"sentence_embedding\".     precision (Literal[\"float32\", \"int8\", \"uint8\", \"binary\", \"ubinary\"], optional): The precision to use for the embeddings.         Can be \"float32\", \"int8\", \"uint8\", \"binary\", or \"ubinary\". All non-float32 precisions are quantized embeddings.         Quantized embeddings are smaller in size and faster to compute, but may have a lower accuracy. They are useful for         reducing the size of the embeddings of a corpus for semantic search, among other tasks. Defaults to \"float32\".     convert_to_numpy (bool, optional): Whether the output should be a list of numpy vectors. If False, it is a list of PyTorch tensors.         Defaults to True.     convert_to_tensor (bool, optional): Whether the output should be one large tensor. Overwrites <code>convert_to_numpy</code>.         Defaults to False.     device (Union[str, List[str], None], optional): Device(s) to use for computation. Can be:          - A single device string (e.g., \"cuda:0\", \"cpu\") for single-process encoding         - A list of device strings (e.g., [\"cuda:0\", \"cuda:1\"], [\"cpu\", \"cpu\", \"cpu\", \"cpu\"]) to distribute           encoding across multiple processes         - None to auto-detect available device for single-process encoding         If a list is provided, multi-process encoding will be used. Defaults to None.     normalize_embeddings (bool, optional): Whether to normalize returned vectors to have length 1. In that case,         the faster dot-product (util.dot_score) instead of cosine similarity can be used. Defaults to False.     truncate_dim (int, optional): The dimension to truncate sentence embeddings to.         Truncation is especially interesting for <code>Matryoshka models &lt;https://sbert.net/examples/sentence_transformer/training/matryoshka/README.html&gt;</code>_,         i.e. models that are trained to still produce useful embeddings even if the embedding dimension is reduced.         Truncated embeddings require less memory and are faster to perform retrieval with, but note that inference         is just as fast, and the embedding performance is worse than the full embeddings. If None, the <code>truncate_dim</code>         from the model initialization is used. Defaults to None.     pool (Dict[Literal[\"input\", \"output\", \"processes\"], Any], optional): A pool created by <code>start_multi_process_pool()</code>         for multi-process encoding. If provided, the encoding will be distributed across multiple processes.         This is recommended for large datasets and when multiple GPUs are available. Defaults to None.     chunk_size (int, optional): Size of chunks for multi-process encoding. Only used with multiprocessing, i.e. when         <code>pool</code> is not None or <code>device</code> is a list. If None, a sensible default is calculated. Defaults to None.  Returns:     Union[List[Tensor], ndarray, Tensor]: By default, a 2d numpy array with shape [num_inputs, output_dimension] is returned.     If only one string input is provided, then the output is a 1d array with shape [output_dimension]. If <code>convert_to_tensor</code>,     a torch Tensor is returned instead. If <code>self.truncate_dim &lt;= output_dimension</code> then output_dimension is <code>self.truncate_dim</code>.  Example:     ::          from sentence_transformers import SentenceTransformer          # Load a pre-trained SentenceTransformer model         model = SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\")          # Encode some documents         documents = [             \"This research paper discusses the effects of climate change on marine life.\",             \"The article explores the history of artificial intelligence development.\",             \"This document contains technical specifications for the new product line.\",         ]          # Using document-specific encoding         embeddings = model.encode_document(documents)         print(embeddings.shape)         # (3, 768)</p> <p>Parameters</p> <ul> <li>sentences     ('str | list[str] | np.ndarray')    </li> <li>prompt_name     ('str | None')     \u2013 defaults to <code>None</code> </li> <li>prompt     ('str | None')     \u2013 defaults to <code>None</code> </li> <li>batch_size     ('int')     \u2013 defaults to <code>32</code> </li> <li>show_progress_bar     ('bool | None')     \u2013 defaults to <code>None</code> </li> <li>output_value     (\"Literal['sentence_embedding', 'token_embeddings'] | None\")     \u2013 defaults to <code>sentence_embedding</code> </li> <li>precision     (\"Literal['float32', 'int8', 'uint8', 'binary', 'ubinary']\")     \u2013 defaults to <code>float32</code> </li> <li>convert_to_numpy     ('bool')     \u2013 defaults to <code>True</code> </li> <li>convert_to_tensor     ('bool')     \u2013 defaults to <code>False</code> </li> <li>device     ('str | list[str | torch.device] | None')     \u2013 defaults to <code>None</code>      Device (like \"cuda\", \"cpu\", \"mps\", \"npu\") that should be used for computation. If None, checks if a GPU can be used.</li> <li>normalize_embeddings     ('bool')     \u2013 defaults to <code>False</code> </li> <li>truncate_dim     ('int | None')     \u2013 defaults to <code>None</code>      The dimension to truncate sentence embeddings to. <code>None</code> does no truncation. Truncation is only applicable during inference when :meth:<code>SentenceTransformer.encode</code> is called.</li> <li>pool     (\"dict[Literal['input', 'output', 'processes'], Any] | None\")     \u2013 defaults to <code>None</code> </li> <li>chunk_size     ('int | None')     \u2013 defaults to <code>None</code> </li> <li>kwargs </li> </ul> encode_multi_process <p>Encodes a list of sentences using multiple processes and GPUs via :meth:<code>SentenceTransformer.encode &lt;sentence_transformers.SentenceTransformer.encode&gt;</code>. The sentences are chunked into smaller packages and sent to individual processes, which encode them on different GPUs or CPUs. This method is only suitable for encoding large sets of sentences.</p> <p>Parameters</p> <ul> <li>sentences     ('list[str]')    </li> <li>pool     ('dict[str, object]')    </li> <li>prompt_name     ('str | None')     \u2013 defaults to <code>None</code> </li> <li>prompt     ('str | None')     \u2013 defaults to <code>None</code> </li> <li>batch_size     ('int')     \u2013 defaults to <code>32</code> </li> <li>chunk_size     ('int')     \u2013 defaults to <code>None</code> </li> <li>precision     (\"Literal['float32', 'int8', 'uint8', 'binary', 'ubinary']\")     \u2013 defaults to <code>float32</code> </li> <li>normalize_embeddings     ('bool')     \u2013 defaults to <code>True</code> </li> <li>padding     ('bool')     \u2013 defaults to <code>False</code> </li> <li>is_query     ('bool')     \u2013 defaults to <code>True</code> </li> <li>pool_factor     ('int')     \u2013 defaults to <code>1</code> </li> <li>protected_tokens     ('int')     \u2013 defaults to <code>1</code> </li> </ul> encode_query <p>Computes sentence embeddings specifically optimized for query representation.</p> <p>This method is a specialized version of :meth:<code>encode</code> that differs in exactly two ways:  1. If no <code>prompt_name</code> or <code>prompt</code> is provided, it uses a predefined \"query\" prompt,    if available in the model's <code>prompts</code> dictionary. 2. It sets the <code>task</code> to \"query\". If the model has a :class:<code>~sentence_transformers.models.Router</code>    module, it will use the \"query\" task type to route the input through the appropriate submodules.  .. tip::      If you are unsure whether you should use :meth:<code>encode</code>, :meth:<code>encode_query</code>, or :meth:<code>encode_document</code>,     your best bet is to use :meth:<code>encode_query</code> and :meth:<code>encode_document</code> for Information Retrieval tasks     with clear query and document/passage distinction, and use :meth:<code>encode</code> for all other tasks.      Note that :meth:<code>encode</code> is the most general method and can be used for any task, including Information     Retrieval, and that if the model was not trained with predefined prompts and/or task types, then all three     methods will return identical embeddings.  Args:     sentences (Union[str, List[str]]): The sentences to embed.     prompt_name (Optional[str], optional): The name of the prompt to use for encoding. Must be a key in the <code>prompts</code> dictionary,         which is either set in the constructor or loaded from the model configuration. For example if         <code>prompt_name</code> is \"query\" and the <code>prompts</code> is {\"query\": \"query: \", ...}, then the sentence \"What         is the capital of France?\" will be encoded as \"query: What is the capital of France?\" because the sentence         is appended to the prompt. If <code>prompt</code> is also set, this argument is ignored. Defaults to None.     prompt (Optional[str], optional): The prompt to use for encoding. For example, if the prompt is \"query: \", then the         sentence \"What is the capital of France?\" will be encoded as \"query: What is the capital of France?\"         because the sentence is appended to the prompt. If <code>prompt</code> is set, <code>prompt_name</code> is ignored. Defaults to None.     batch_size (int, optional): The batch size used for the computation. Defaults to 32.     show_progress_bar (bool, optional): Whether to output a progress bar when encode sentences. Defaults to None.     output_value (Optional[Literal[\"sentence_embedding\", \"token_embeddings\"]], optional): The type of embeddings to return:         \"sentence_embedding\" to get sentence embeddings, \"token_embeddings\" to get wordpiece token embeddings, and <code>None</code>,         to get all output values. Defaults to \"sentence_embedding\".     precision (Literal[\"float32\", \"int8\", \"uint8\", \"binary\", \"ubinary\"], optional): The precision to use for the embeddings.         Can be \"float32\", \"int8\", \"uint8\", \"binary\", or \"ubinary\". All non-float32 precisions are quantized embeddings.         Quantized embeddings are smaller in size and faster to compute, but may have a lower accuracy. They are useful for         reducing the size of the embeddings of a corpus for semantic search, among other tasks. Defaults to \"float32\".     convert_to_numpy (bool, optional): Whether the output should be a list of numpy vectors. If False, it is a list of PyTorch tensors.         Defaults to True.     convert_to_tensor (bool, optional): Whether the output should be one large tensor. Overwrites <code>convert_to_numpy</code>.         Defaults to False.     device (Union[str, List[str], None], optional): Device(s) to use for computation. Can be:          - A single device string (e.g., \"cuda:0\", \"cpu\") for single-process encoding         - A list of device strings (e.g., [\"cuda:0\", \"cuda:1\"], [\"cpu\", \"cpu\", \"cpu\", \"cpu\"]) to distribute           encoding across multiple processes         - None to auto-detect available device for single-process encoding         If a list is provided, multi-process encoding will be used. Defaults to None.     normalize_embeddings (bool, optional): Whether to normalize returned vectors to have length 1. In that case,         the faster dot-product (util.dot_score) instead of cosine similarity can be used. Defaults to False.     truncate_dim (int, optional): The dimension to truncate sentence embeddings to.         Truncation is especially interesting for <code>Matryoshka models &lt;https://sbert.net/examples/sentence_transformer/training/matryoshka/README.html&gt;</code>_,         i.e. models that are trained to still produce useful embeddings even if the embedding dimension is reduced.         Truncated embeddings require less memory and are faster to perform retrieval with, but note that inference         is just as fast, and the embedding performance is worse than the full embeddings. If None, the <code>truncate_dim</code>         from the model initialization is used. Defaults to None.     pool (Dict[Literal[\"input\", \"output\", \"processes\"], Any], optional): A pool created by <code>start_multi_process_pool()</code>         for multi-process encoding. If provided, the encoding will be distributed across multiple processes.         This is recommended for large datasets and when multiple GPUs are available. Defaults to None.     chunk_size (int, optional): Size of chunks for multi-process encoding. Only used with multiprocessing, i.e. when         <code>pool</code> is not None or <code>device</code> is a list. If None, a sensible default is calculated. Defaults to None.  Returns:     Union[List[Tensor], ndarray, Tensor]: By default, a 2d numpy array with shape [num_inputs, output_dimension] is returned.     If only one string input is provided, then the output is a 1d array with shape [output_dimension]. If <code>convert_to_tensor</code>,     a torch Tensor is returned instead. If <code>self.truncate_dim &lt;= output_dimension</code> then output_dimension is <code>self.truncate_dim</code>.  Example:     ::          from sentence_transformers import SentenceTransformer          # Load a pre-trained SentenceTransformer model         model = SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\")          # Encode some queries         queries = [             \"What are the effects of climate change?\",             \"History of artificial intelligence\",             \"Technical specifications product XYZ\",         ]          # Using query-specific encoding         embeddings = model.encode_query(queries)         print(embeddings.shape)         # (3, 768)</p> <p>Parameters</p> <ul> <li>sentences     ('str | list[str] | np.ndarray')    </li> <li>prompt_name     ('str | None')     \u2013 defaults to <code>None</code> </li> <li>prompt     ('str | None')     \u2013 defaults to <code>None</code> </li> <li>batch_size     ('int')     \u2013 defaults to <code>32</code> </li> <li>show_progress_bar     ('bool | None')     \u2013 defaults to <code>None</code> </li> <li>output_value     (\"Literal['sentence_embedding', 'token_embeddings'] | None\")     \u2013 defaults to <code>sentence_embedding</code> </li> <li>precision     (\"Literal['float32', 'int8', 'uint8', 'binary', 'ubinary']\")     \u2013 defaults to <code>float32</code> </li> <li>convert_to_numpy     ('bool')     \u2013 defaults to <code>True</code> </li> <li>convert_to_tensor     ('bool')     \u2013 defaults to <code>False</code> </li> <li>device     ('str | list[str | torch.device] | None')     \u2013 defaults to <code>None</code>      Device (like \"cuda\", \"cpu\", \"mps\", \"npu\") that should be used for computation. If None, checks if a GPU can be used.</li> <li>normalize_embeddings     ('bool')     \u2013 defaults to <code>False</code> </li> <li>truncate_dim     ('int | None')     \u2013 defaults to <code>None</code>      The dimension to truncate sentence embeddings to. <code>None</code> does no truncation. Truncation is only applicable during inference when :meth:<code>SentenceTransformer.encode</code> is called.</li> <li>pool     (\"dict[Literal['input', 'output', 'processes'], Any] | None\")     \u2013 defaults to <code>None</code> </li> <li>chunk_size     ('int | None')     \u2013 defaults to <code>None</code> </li> <li>kwargs </li> </ul> eval <p>Set the module in evaluation mode.</p> <p>This has an effect only on certain modules. See the documentation of particular modules for details of their behaviors in training/evaluation mode, i.e. whether they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>, etc.  This is equivalent with :meth:<code>self.train(False) &lt;torch.nn.Module.train&gt;</code>.  See :ref:<code>locally-disable-grad-doc</code> for a comparison between <code>.eval()</code> and several similar mechanisms that may be confused with it.  Returns:     Module: self</p> evaluate <p>Evaluate the model based on an evaluator</p> <p>Args:     evaluator (SentenceEvaluator): The evaluator used to evaluate the model.     output_path (str, optional): The path where the evaluator can write the results. Defaults to None.  Returns:     The evaluation results.</p> <p>Parameters</p> <ul> <li>evaluator     ('SentenceEvaluator')    </li> <li>output_path     ('str | None')     \u2013 defaults to <code>None</code> </li> </ul> extend <p>Extends the current Sequential container with layers from another Sequential container.</p> <p>Args:     sequential (Sequential): A Sequential container whose layers will be added to the current container.  Example::      &gt;&gt;&gt; import torch.nn as nn     &gt;&gt;&gt; n = nn.Sequential(nn.Linear(1, 2), nn.Linear(2, 3))     &gt;&gt;&gt; other = nn.Sequential(nn.Linear(3, 4), nn.Linear(4, 5))     &gt;&gt;&gt; n.extend(other) # or <code>n + other</code>     Sequential(         (0): Linear(in_features=1, out_features=2, bias=True)         (1): Linear(in_features=2, out_features=3, bias=True)         (2): Linear(in_features=3, out_features=4, bias=True)         (3): Linear(in_features=4, out_features=5, bias=True)     )</p> <p>Parameters</p> <ul> <li>sequential     ('Iterable[Module]')    </li> </ul> extra_repr <p>Return the extra representation of the module.</p> <p>To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.</p> fit <p>Deprecated training method from before Sentence Transformers v3.0, it is recommended to use :class:<code>~sentence_transformers.trainer.SentenceTransformerTrainer</code> instead. This method uses :class:<code>~sentence_transformers.trainer.SentenceTransformerTrainer</code> behind the scenes, but does not provide as much flexibility as the Trainer itself.</p> <p>This training approach uses a list of DataLoaders and Loss functions to train the model. Each DataLoader is sampled in turn for one batch. We sample only as many batches from each DataLoader as there are in the smallest one to make sure of equal training with each dataset, i.e. round robin sampling.  This method should produce equivalent results in v3.0+ as before v3.0, but if you encounter any issues with your existing training scripts, then you may wish to use :meth:<code>SentenceTransformer.old_fit &lt;sentence_transformers.SentenceTransformer.old_fit&gt;</code> instead. That uses the old training method from before v3.0.  Args:     train_objectives: Tuples of (DataLoader, LossFunction). Pass         more than one for multi-task learning     evaluator: An evaluator (sentence_transformers.evaluation)         evaluates the model performance during training on held-         out dev data. It is used to determine the best model         that is saved to disk.     epochs: Number of epochs for training     steps_per_epoch: Number of training steps per epoch. If set         to None (default), one epoch is equal the DataLoader         size from train_objectives.     scheduler: Learning rate scheduler. Available schedulers:         constantlr, warmupconstant, warmuplinear, warmupcosine,         warmupcosinewithhardrestarts     warmup_steps: Behavior depends on the scheduler. For         WarmupLinear (default), the learning rate is increased         from o up to the maximal learning rate. After these many         training steps, the learning rate is decreased linearly         back to zero.     optimizer_class: Optimizer     optimizer_params: Optimizer parameters     weight_decay: Weight decay for model parameters     evaluation_steps: If &gt; 0, evaluate the model using evaluator         after each number of training steps     output_path: Storage path for the model and evaluation files     save_best_model: If true, the best model (according to         evaluator) is stored at output_path     max_grad_norm: Used for gradient normalization.     use_amp: Use Automatic Mixed Precision (AMP). Only for         Pytorch &gt;= 1.6.0     callback: Callback function that is invoked after each         evaluation. It must accept the following three         parameters in this order: <code>score</code>, <code>epoch</code>, <code>steps</code>     show_progress_bar: If True, output a tqdm progress bar     checkpoint_path: Folder to save checkpoints during training     checkpoint_save_steps: Will save a checkpoint after so many         steps     checkpoint_save_total_limit: Total number of checkpoints to         store     resume_from_checkpoint: If true, searches for checkpoints         to continue training from.</p> <p>Parameters</p> <ul> <li>train_objectives     ('Iterable[tuple[DataLoader, nn.Module]]')    </li> <li>evaluator     ('SentenceEvaluator | None')     \u2013 defaults to <code>None</code> </li> <li>epochs     ('int')     \u2013 defaults to <code>1</code> </li> <li>steps_per_epoch     \u2013 defaults to <code>None</code> </li> <li>scheduler     ('str')     \u2013 defaults to <code>WarmupLinear</code> </li> <li>warmup_steps     ('int')     \u2013 defaults to <code>10000</code> </li> <li>optimizer_class     ('type[Optimizer]')     \u2013 defaults to <code>&lt;class 'torch.optim.adamw.AdamW'&gt;</code> </li> <li>optimizer_params     ('dict[str, object]')     \u2013 defaults to <code>{'lr': 2e-05}</code> </li> <li>weight_decay     ('float')     \u2013 defaults to <code>0.01</code> </li> <li>evaluation_steps     ('int')     \u2013 defaults to <code>0</code> </li> <li>output_path     ('str | None')     \u2013 defaults to <code>None</code> </li> <li>save_best_model     ('bool')     \u2013 defaults to <code>True</code> </li> <li>max_grad_norm     ('float')     \u2013 defaults to <code>1</code> </li> <li>use_amp     ('bool')     \u2013 defaults to <code>False</code> </li> <li>callback     ('Callable[[float, int, int], None]')     \u2013 defaults to <code>None</code> </li> <li>show_progress_bar     ('bool')     \u2013 defaults to <code>True</code> </li> <li>checkpoint_path     ('str | None')     \u2013 defaults to <code>None</code> </li> <li>checkpoint_save_steps     ('int')     \u2013 defaults to <code>500</code> </li> <li>checkpoint_save_total_limit     ('int')     \u2013 defaults to <code>0</code> </li> <li>resume_from_checkpoint     ('bool')     \u2013 defaults to <code>False</code> </li> </ul> float <p>Casts all floating point parameters and buffers to <code>float</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> forward <p>Runs the forward pass.</p> <p>Parameters</p> <ul> <li>input     ('dict[str, Tensor]')    </li> <li>kwargs </li> </ul> get_adapter_state_dict <p>If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the PEFT official documentation: https://huggingface.co/docs/peft</p> <p>Gets the adapter state dict that should only contain the weights tensors of the specified adapter_name adapter. If no adapter_name is passed, the active adapter is used.  Args:     args:         Positional arguments to pass to the underlying AutoModel <code>get_adapter_state_dict</code> function. More information can be found in the transformers documentation         https://huggingface.co/docs/transformers/main/en/main_classes/peft#transformers.integrations.PeftAdapterMixin.get_adapter_state_dict     *kwargs:         Keyword arguments to pass to the underlying AutoModel <code>get_adapter_state_dict</code> function. More information can be found in the transformers documentation         https://huggingface.co/docs/transformers/main/en/main_classes/peft#transformers.integrations.PeftAdapterMixin.get_adapter_state_dict</p> <p>Parameters</p> <ul> <li>args </li> <li>kwargs </li> </ul> get_backend <p>Return the backend used for inference, which can be one of \"torch\", \"onnx\", or \"openvino\".</p> <p>Returns:     str: The backend used for inference.</p> get_buffer <p>Return the buffer given by <code>target</code> if it exists, otherwise throw an error.</p> <p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method's functionality as well as how to correctly specify <code>target</code>.  Args:     target: The fully-qualified string name of the buffer         to look for. (See <code>get_submodule</code> for how to specify a         fully-qualified string.)  Returns:     torch.Tensor: The buffer referenced by <code>target</code>  Raises:     AttributeError: If the target string references an invalid         path or resolves to something that is not a         buffer</p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> get_extra_state <p>Return any extra state to include in the module's state_dict.</p> <p>Implement this and a corresponding :func:<code>set_extra_state</code> for your module if you need to store extra state. This function is called when building the module's <code>state_dict()</code>.  Note that extra state should be picklable to ensure working serialization of the state_dict. We only provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes.  Returns:     object: Any extra state to store in the module's state_dict</p> get_max_seq_length <p>Returns the maximal sequence length that the model accepts. Longer inputs will be truncated.</p> <p>Returns:     Optional[int]: The maximal sequence length that the model accepts, or None if it is not defined.</p> get_model_kwargs <p>Get the keyword arguments specific to this model for the <code>encode</code>, <code>encode_query</code>, or <code>encode_document</code> methods.</p> <p>Example:      &gt;&gt;&gt; from sentence_transformers import SentenceTransformer, SparseEncoder     &gt;&gt;&gt; SentenceTransformer(\"all-MiniLM-L6-v2\").get_model_kwargs()     []     &gt;&gt;&gt; SentenceTransformer(\"jinaai/jina-embeddings-v4\", trust_remote_code=True).get_model_kwargs()     ['task', 'truncate_dim']     &gt;&gt;&gt; SparseEncoder(\"opensearch-project/opensearch-neural-sparse-encoding-doc-v3-distill\").get_model_kwargs()     ['task']  Returns:     list[str]: A list of keyword arguments for the forward pass.</p> get_parameter <p>Return the parameter given by <code>target</code> if it exists, otherwise throw an error.</p> <p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method's functionality as well as how to correctly specify <code>target</code>.  Args:     target: The fully-qualified string name of the Parameter         to look for. (See <code>get_submodule</code> for how to specify a         fully-qualified string.)  Returns:     torch.nn.Parameter: The Parameter referenced by <code>target</code>  Raises:     AttributeError: If the target string references an invalid         path or resolves to something that is not an         <code>nn.Parameter</code></p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> get_sentence_embedding_dimension <p>Returns the number of dimensions in the output of :meth:<code>SentenceTransformer.encode &lt;sentence_transformers.SentenceTransformer.encode&gt;</code>.</p> <p>Returns:     Optional[int]: The number of dimensions in the output of <code>encode</code>. If it's not known, it's <code>None</code>.</p> get_sentence_features get_submodule <p>Return the submodule given by <code>target</code> if it exists, otherwise throw an error.</p> <p>For example, let's say you have an <code>nn.Module</code> <code>A</code> that looks like this:  .. code-block:: text      A(         (net_b): Module(             (net_c): Module(                 (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))             )             (linear): Linear(in_features=100, out_features=200, bias=True)         )     )  (The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> which has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)  To check whether or not we have the <code>linear</code> submodule, we would call <code>get_submodule(\"net_b.linear\")</code>. To check whether we have the <code>conv</code> submodule, we would call <code>get_submodule(\"net_b.net_c.conv\")</code>.  The runtime of <code>get_submodule</code> is bounded by the degree of module nesting in <code>target</code>. A query against <code>named_modules</code> achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, <code>get_submodule</code> should always be used.  Args:     target: The fully-qualified string name of the submodule         to look for. (See above example for how to specify a         fully-qualified string.)  Returns:     torch.nn.Module: The submodule referenced by <code>target</code>  Raises:     AttributeError: If at any point along the path resulting from         the target string the (sub)path resolves to a non-existent         attribute name or an object that is not an instance of <code>nn.Module</code>.</p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> gradient_checkpointing_enable half <p>Casts all floating point parameters and buffers to <code>half</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> has_peft_compatible_model insert <p>Inserts a module into the Sequential container at the specified index.</p> <p>Args:     index (int): The index to insert the module.     module (Module): The module to be inserted.  Example::      &gt;&gt;&gt; import torch.nn as nn     &gt;&gt;&gt; n = nn.Sequential(nn.Linear(1, 2), nn.Linear(2, 3))     &gt;&gt;&gt; n.insert(0, nn.Linear(3, 4))     Sequential(         (0): Linear(in_features=3, out_features=4, bias=True)         (1): Linear(in_features=1, out_features=2, bias=True)         (2): Linear(in_features=2, out_features=3, bias=True)     )</p> <p>Parameters</p> <ul> <li>index     ('int')    </li> <li>module     ('Module')    </li> </ul> insert_prefix_token <p>Inserts a prefix token at the beginning of each sequence in the input tensor.</p> <p>Parameters</p> <ul> <li>input_ids     ('torch.Tensor')    </li> <li>prefix_id     ('int')    </li> </ul> ipu <p>Move all model parameters and buffers to the IPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing the optimizer if the module will live on IPU while being optimized.  .. note::     This method modifies the module in-place.  Arguments:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[torch.device, int, NoneType])     \u2013 defaults to <code>None</code>      Device (like \"cuda\", \"cpu\", \"mps\", \"npu\") that should be used for computation. If None, checks if a GPU can be used.</li> </ul> load load_adapter <p>Load adapter weights from file or remote Hub folder.\" If you are not familiar with adapters and PEFT methods, we invite you to read more about them on PEFT official documentation: https://huggingface.co/docs/peft</p> <p>Requires peft as a backend to load the adapter weights and the underlying model to be compatible with PEFT.  Args:     args:         Positional arguments to pass to the underlying AutoModel <code>load_adapter</code> function. More information can be found in the transformers documentation         https://huggingface.co/docs/transformers/main/en/main_classes/peft#transformers.integrations.PeftAdapterMixin.load_adapter     *kwargs:         Keyword arguments to pass to the underlying AutoModel <code>load_adapter</code> function. More information can be found in the transformers documentation         https://huggingface.co/docs/transformers/main/en/main_classes/peft#transformers.integrations.PeftAdapterMixin.load_adapter</p> <p>Parameters</p> <ul> <li>args </li> <li>kwargs </li> </ul> load_state_dict <p>Copy parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants.</p> <p>If :attr:<code>strict</code> is <code>True</code>, then the keys of :attr:<code>state_dict</code> must exactly match the keys returned by this module's :meth:<code>~torch.nn.Module.state_dict</code> function.  .. warning::     If :attr:<code>assign</code> is <code>True</code> the optimizer must be created after     the call to :attr:<code>load_state_dict</code> unless     :func:<code>~torch.__future__.get_swap_module_params_on_conversion</code> is <code>True</code>.  Args:     state_dict (dict): a dict containing parameters and         persistent buffers.     strict (bool, optional): whether to strictly enforce that the keys         in :attr:<code>state_dict</code> match the keys returned by this module's         :meth:<code>~torch.nn.Module.state_dict</code> function. Default: <code>True</code>     assign (bool, optional): When set to <code>False</code>, the properties of the tensors         in the current module are preserved whereas setting it to <code>True</code> preserves         properties of the Tensors in the state dict. The only         exception is the <code>requires_grad</code> field of :class:<code>~torch.nn.Parameter</code>         for which the value from the module is preserved. Default: <code>False</code>  Returns:     <code>NamedTuple</code> with <code>missing_keys</code> and <code>unexpected_keys</code> fields:         * <code>missing_keys</code> is a list of str containing any keys that are expected             by this module but missing from the provided <code>state_dict</code>.         * <code>unexpected_keys</code> is a list of str containing the keys that are not             expected by this module but present in the provided <code>state_dict</code>.  Note:     If a parameter or buffer is registered as <code>None</code> and its corresponding key     exists in :attr:<code>state_dict</code>, :meth:<code>load_state_dict</code> will raise a     <code>RuntimeError</code>.</p> <p>Parameters</p> <ul> <li>state_dict     (collections.abc.Mapping[str, typing.Any])    </li> <li>strict     (bool)     \u2013 defaults to <code>True</code> </li> <li>assign     (bool)     \u2013 defaults to <code>False</code> </li> </ul> model_card_data_class <p>A dataclass storing data used in the model card.</p> <p>Args:     language (<code>Optional[Union[str, List[str]]]</code>): The model language, either a string or a list,         e.g. \"en\" or [\"en\", \"de\", \"nl\"]     license (<code>Optional[str]</code>): The license of the model, e.g. \"apache-2.0\", \"mit\",         or \"cc-by-nc-sa-4.0\"     model_name (<code>Optional[str]</code>): The pretty name of the model, e.g. \"SentenceTransformer based on microsoft/mpnet-base\".     model_id (<code>Optional[str]</code>): The model ID when pushing the model to the Hub,         e.g. \"tomaarsen/sbert-mpnet-base-allnli\".     train_datasets (<code>List[Dict[str, str]]</code>): A list of the names and/or Hugging Face dataset IDs of the training datasets.         e.g. [{\"name\": \"SNLI\", \"id\": \"stanfordnlp/snli\"}, {\"name\": \"MultiNLI\", \"id\": \"nyu-mll/multi_nli\"}, {\"name\": \"STSB\"}]     eval_datasets (<code>List[Dict[str, str]]</code>): A list of the names and/or Hugging Face dataset IDs of the evaluation datasets.         e.g. [{\"name\": \"SNLI\", \"id\": \"stanfordnlp/snli\"}, {\"id\": \"mteb/stsbenchmark-sts\"}]     task_name (<code>str</code>): The human-readable task the model is trained on,         e.g. \"semantic textual similarity, semantic search, paraphrase mining, text classification, clustering, and more\".     tags (<code>Optional[List[str]]</code>): A list of tags for the model,         e.g. [\"sentence-transformers\", \"sentence-similarity\", \"feature-extraction\"].     local_files_only (<code>bool</code>): If True, don't attempt to find dataset or base model information on the Hub.         Defaults to False.     generate_widget_examples (<code>bool</code>): If True, generate widget examples from the evaluation or training dataset,         and compute their similarities. Defaults to True.  .. tip::      Install <code>codecarbon &lt;https://github.com/mlco2/codecarbon&gt;</code>_ to automatically track carbon emission usage and     include it in your model cards.  Example::      &gt;&gt;&gt; model = SentenceTransformer(     ...     \"microsoft/mpnet-base\",     ...     model_card_data=SentenceTransformerModelCardData(     ...         model_id=\"tomaarsen/sbert-mpnet-base-allnli\",     ...         train_datasets=[{\"name\": \"SNLI\", \"id\": \"stanfordnlp/snli\"}, {\"name\": \"MultiNLI\", \"id\": \"nyu-mll/multi_nli\"}],     ...         eval_datasets=[{\"name\": \"SNLI\", \"id\": \"stanfordnlp/snli\"}, {\"name\": \"MultiNLI\", \"id\": \"nyu-mll/multi_nli\"}],     ...         license=\"apache-2.0\",     ...         language=\"en\",     ...     ),     ... )</p> <p>Parameters</p> <ul> <li>language     ('str | list[str] | None')     \u2013 defaults to <code>&lt;factory&gt;</code> </li> <li>license     ('str | None')     \u2013 defaults to <code>None</code> </li> <li>model_name     ('str | None')     \u2013 defaults to <code>None</code> </li> <li>model_id     ('str | None')     \u2013 defaults to <code>None</code> </li> <li>train_datasets     ('list[dict[str, str]]')     \u2013 defaults to <code>&lt;factory&gt;</code> </li> <li>eval_datasets     ('list[dict[str, str]]')     \u2013 defaults to <code>&lt;factory&gt;</code> </li> <li>task_name     ('str')     \u2013 defaults to <code>semantic textual similarity, semantic search, paraphrase mining, text classification, clustering, and more</code> </li> <li>tags     ('list[str] | None')     \u2013 defaults to <code>&lt;factory&gt;</code> </li> <li>local_files_only     ('bool')     \u2013 defaults to <code>False</code>      Whether or not to only look at local files (i.e., do not try to download the model).</li> <li>generate_widget_examples     ('bool')     \u2013 defaults to <code>True</code> </li> </ul> modules <p>Return an iterator over all modules in the network.</p> <p>Yields:     Module: a module in the network  Note:     Duplicate modules are returned only once. In the following     example, <code>l</code> will be returned only once.  Example::      &gt;&gt;&gt; l = nn.Linear(2, 2)     &gt;&gt;&gt; net = nn.Sequential(l, l)     &gt;&gt;&gt; for idx, m in enumerate(net.modules()):     ...     print(idx, '-&gt;', m)      0 -&gt; Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     )     1 -&gt; Linear(in_features=2, out_features=2, bias=True)</p> mtia <p>Move all model parameters and buffers to the MTIA.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing the optimizer if the module will live on MTIA while being optimized.  .. note::     This method modifies the module in-place.  Arguments:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[torch.device, int, NoneType])     \u2013 defaults to <code>None</code>      Device (like \"cuda\", \"cpu\", \"mps\", \"npu\") that should be used for computation. If None, checks if a GPU can be used.</li> </ul> named_buffers <p>Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.</p> <p>Args:     prefix (str): prefix to prepend to all buffer names.     recurse (bool, optional): if True, then yields buffers of this module         and all submodules. Otherwise, yields only buffers that         are direct members of this module. Defaults to True.     remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.  Yields:     (str, torch.Tensor): Tuple containing the name and buffer  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, buf in self.named_buffers():     &gt;&gt;&gt;     if name in ['running_var']:     &gt;&gt;&gt;         print(buf.size())</p> <p>Parameters</p> <ul> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> named_children <p>Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.</p> <p>Yields:     (str, Module): Tuple containing a name and child module  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, module in model.named_children():     &gt;&gt;&gt;     if name in ['conv4', 'conv5']:     &gt;&gt;&gt;         print(module)</p> named_modules <p>Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.</p> <p>Args:     memo: a memo to store the set of modules already added to the result     prefix: a prefix that will be added to the name of the module     remove_duplicate: whether to remove the duplicated module instances in the result         or not  Yields:     (str, Module): Tuple of name and module  Note:     Duplicate modules are returned only once. In the following     example, <code>l</code> will be returned only once.  Example::      &gt;&gt;&gt; l = nn.Linear(2, 2)     &gt;&gt;&gt; net = nn.Sequential(l, l)     &gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):     ...     print(idx, '-&gt;', m)      0 -&gt; ('', Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     ))     1 -&gt; ('0', Linear(in_features=2, out_features=2, bias=True))</p> <p>Parameters</p> <ul> <li>memo     (Optional[set['Module']])     \u2013 defaults to <code>None</code> </li> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> named_parameters <p>Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.</p> <p>Args:     prefix (str): prefix to prepend to all parameter names.     recurse (bool): if True, then yields parameters of this module         and all submodules. Otherwise, yields only parameters that         are direct members of this module.     remove_duplicate (bool, optional): whether to remove the duplicated         parameters in the result. Defaults to True.  Yields:     (str, Parameter): Tuple containing the name and parameter  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, param in self.named_parameters():     &gt;&gt;&gt;     if name in ['bias']:     &gt;&gt;&gt;         print(param.size())</p> <p>Parameters</p> <ul> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> old_fit <p>Deprecated training method from before Sentence Transformers v3.0, it is recommended to use :class:<code>sentence_transformers.trainer.SentenceTransformerTrainer</code> instead. This method should only be used if you encounter issues with your existing training scripts after upgrading to v3.0+.</p> <p>This training approach uses a list of DataLoaders and Loss functions to train the model. Each DataLoader is sampled in turn for one batch. We sample only as many batches from each DataLoader as there are in the smallest one to make sure of equal training with each dataset, i.e. round robin sampling.  Args:     train_objectives: Tuples of (DataLoader, LossFunction). Pass         more than one for multi-task learning     evaluator: An evaluator (sentence_transformers.evaluation)         evaluates the model performance during training on held-         out dev data. It is used to determine the best model         that is saved to disk.     epochs: Number of epochs for training     steps_per_epoch: Number of training steps per epoch. If set         to None (default), one epoch is equal the DataLoader         size from train_objectives.     scheduler: Learning rate scheduler. Available schedulers:         constantlr, warmupconstant, warmuplinear, warmupcosine,         warmupcosinewithhardrestarts     warmup_steps: Behavior depends on the scheduler. For         WarmupLinear (default), the learning rate is increased         from o up to the maximal learning rate. After these many         training steps, the learning rate is decreased linearly         back to zero.     optimizer_class: Optimizer     optimizer_params: Optimizer parameters     weight_decay: Weight decay for model parameters     evaluation_steps: If &gt; 0, evaluate the model using evaluator         after each number of training steps     output_path: Storage path for the model and evaluation files     save_best_model: If true, the best model (according to         evaluator) is stored at output_path     max_grad_norm: Used for gradient normalization.     use_amp: Use Automatic Mixed Precision (AMP). Only for         Pytorch &gt;= 1.6.0     callback: Callback function that is invoked after each         evaluation. It must accept the following three         parameters in this order: <code>score</code>, <code>epoch</code>, <code>steps</code>     show_progress_bar: If True, output a tqdm progress bar     checkpoint_path: Folder to save checkpoints during training     checkpoint_save_steps: Will save a checkpoint after so many         steps     checkpoint_save_total_limit: Total number of checkpoints to         store</p> <p>Parameters</p> <ul> <li>train_objectives     ('Iterable[tuple[DataLoader, nn.Module]]')    </li> <li>evaluator     ('SentenceEvaluator | None')     \u2013 defaults to <code>None</code> </li> <li>epochs     ('int')     \u2013 defaults to <code>1</code> </li> <li>steps_per_epoch     \u2013 defaults to <code>None</code> </li> <li>scheduler     ('str')     \u2013 defaults to <code>WarmupLinear</code> </li> <li>warmup_steps     ('int')     \u2013 defaults to <code>10000</code> </li> <li>optimizer_class     ('type[Optimizer]')     \u2013 defaults to <code>&lt;class 'torch.optim.adamw.AdamW'&gt;</code> </li> <li>optimizer_params     ('dict[str, object]')     \u2013 defaults to <code>{'lr': 2e-05}</code> </li> <li>weight_decay     ('float')     \u2013 defaults to <code>0.01</code> </li> <li>evaluation_steps     ('int')     \u2013 defaults to <code>0</code> </li> <li>output_path     ('str | None')     \u2013 defaults to <code>None</code> </li> <li>save_best_model     ('bool')     \u2013 defaults to <code>True</code> </li> <li>max_grad_norm     ('float')     \u2013 defaults to <code>1</code> </li> <li>use_amp     ('bool')     \u2013 defaults to <code>False</code> </li> <li>callback     ('Callable[[float, int, int], None]')     \u2013 defaults to <code>None</code> </li> <li>show_progress_bar     ('bool')     \u2013 defaults to <code>True</code> </li> <li>checkpoint_path     ('str | None')     \u2013 defaults to <code>None</code> </li> <li>checkpoint_save_steps     ('int')     \u2013 defaults to <code>500</code> </li> <li>checkpoint_save_total_limit     ('int')     \u2013 defaults to <code>0</code> </li> </ul> parameters <p>Return an iterator over module parameters.</p> <p>This is typically passed to an optimizer.  Args:     recurse (bool): if True, then yields parameters of this module         and all submodules. Otherwise, yields only parameters that         are direct members of this module.  Yields:     Parameter: module parameter  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for param in model.parameters():     &gt;&gt;&gt;     print(type(param), param.size())      (20L,)      (20L, 1L, 5L, 5L) <p>Parameters</p> <ul> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> </ul> pool_embeddings_hierarchical <p>Pools the embeddings hierarchically by clustering and averaging them.</p> <p>Parameters</p> <ul> <li>documents_embeddings     ('list[torch.Tensor]')    </li> <li>pool_factor     ('int')     \u2013 defaults to <code>1</code> </li> <li>protected_tokens     ('int')     \u2013 defaults to <code>1</code> </li> </ul> <p>Returns</p> <p>list[torch.Tensor]:     A list of pooled embeddings for each document.</p> pop <p>Pop <code>key</code> from self.</p> <p>Parameters</p> <ul> <li>key     ('Union[int, slice]')    </li> </ul> push_to_hub <p>Uploads all elements of this Sentence Transformer to a new HuggingFace Hub repository.</p> <p>Args:     repo_id (str): Repository name for your model in the Hub, including the user or organization.     token (str, optional): An authentication token (See https://huggingface.co/settings/token)     private (bool, optional): Set to true, for hosting a private model     safe_serialization (bool, optional): If true, save the model using safetensors. If false, save the model the traditional PyTorch way     commit_message (str, optional): Message to commit while pushing.     local_model_path (str, optional): Path of the model locally. If set, this file path will be uploaded. Otherwise, the current model will be uploaded     exist_ok (bool, optional): If true, saving to an existing repository is OK. If false, saving only to a new repository is possible     replace_model_card (bool, optional): If true, replace an existing model card in the hub with the automatically created model card     train_datasets (List[str], optional): Datasets used to train the model. If set, the datasets will be added to the model card in the Hub.     revision (str, optional): Branch to push the uploaded files to     create_pr (bool, optional): If True, create a pull request instead of pushing directly to the main branch  Returns:     str: The url of the commit of your model in the repository on the Hugging Face Hub.</p> <p>Parameters</p> <ul> <li>repo_id     ('str')    </li> <li>token     ('str | None')     \u2013 defaults to <code>None</code>      Hugging Face authentication token to download private models.</li> <li>private     ('bool | None')     \u2013 defaults to <code>None</code> </li> <li>safe_serialization     ('bool')     \u2013 defaults to <code>True</code> </li> <li>commit_message     ('str | None')     \u2013 defaults to <code>None</code> </li> <li>local_model_path     ('str | None')     \u2013 defaults to <code>None</code> </li> <li>exist_ok     ('bool')     \u2013 defaults to <code>False</code> </li> <li>replace_model_card     ('bool')     \u2013 defaults to <code>False</code> </li> <li>train_datasets     ('list[str] | None')     \u2013 defaults to <code>None</code> </li> <li>revision     ('str | None')     \u2013 defaults to <code>None</code>      The specific model version to use. It can be a branch name, a tag name, or a commit id, for a stored model on Hugging Face.</li> <li>create_pr     ('bool')     \u2013 defaults to <code>False</code> </li> </ul> register_backward_hook <p>Register a backward hook on the module.</p> <p>This function is deprecated in favor of :meth:<code>~torch.nn.Module.register_full_backward_hook</code> and the behavior of this function will change in future versions.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[tuple[torch.Tensor, ...], torch.Tensor], Union[tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]])    </li> </ul> register_buffer <p>Add a buffer to the module.</p> <p>This is typically used to register a buffer that should not be considered a model parameter. For example, BatchNorm's <code>running_mean</code> is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:<code>persistent</code> to <code>False</code>. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:<code>state_dict</code>.  Buffers can be accessed as attributes using given names.  Args:     name (str): name of the buffer. The buffer can be accessed         from this module using the given name     tensor (Tensor or None): buffer to be registered. If <code>None</code>, then operations         that run on buffers, such as :attr:<code>cuda</code>, are ignored. If <code>None</code>,         the buffer is not included in the module's :attr:<code>state_dict</code>.     persistent (bool): whether the buffer is part of this module's         :attr:<code>state_dict</code>.  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; self.register_buffer('running_mean', torch.zeros(num_features))</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>tensor     (Optional[torch.Tensor])    </li> <li>persistent     (bool)     \u2013 defaults to <code>True</code> </li> </ul> register_forward_hook <p>Register a forward hook on the module.</p> <p>The hook will be called every time after :func:<code>forward</code> has computed an output.  If <code>with_kwargs</code> is <code>False</code> or not specified, the input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the <code>forward</code>. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func:<code>forward</code> is called. The hook should have the following signature::      hook(module, args, output) -&gt; None or modified output  If <code>with_kwargs</code> is <code>True</code>, the forward hook will be passed the <code>kwargs</code> given to the forward function and be expected to return the output possibly modified. The hook should have the following signature::      hook(module, args, kwargs, output) -&gt; None or modified output  Args:     hook (Callable): The user defined hook to be registered.     prepend (bool): If <code>True</code>, the provided <code>hook</code> will be fired         before all existing <code>forward</code> hooks on this         :class:<code>torch.nn.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>forward</code> hooks on         this :class:<code>torch.nn.Module</code>. Note that global         <code>forward</code> hooks registered with         :func:<code>register_module_forward_hook</code> will fire before all hooks         registered by this method.         Default: <code>False</code>     with_kwargs (bool): If <code>True</code>, the <code>hook</code> will be passed the         kwargs given to the forward function.         Default: <code>False</code>     always_call (bool): If <code>True</code> the <code>hook</code> will be run regardless of         whether an exception is raised while calling the Module.         Default: <code>False</code>  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Union[Callable[[~T, tuple[Any, ...], Any], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Optional[Any]]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> <li>with_kwargs     (bool)     \u2013 defaults to <code>False</code> </li> <li>always_call     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_forward_pre_hook <p>Register a forward pre-hook on the module.</p> <p>The hook will be called every time before :func:<code>forward</code> is invoked.  If <code>with_kwargs</code> is false or not specified, the input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the <code>forward</code>. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned (unless that value is already a tuple). The hook should have the following signature::      hook(module, args) -&gt; None or modified input  If <code>with_kwargs</code> is true, the forward pre-hook will be passed the kwargs given to the forward function. And if the hook modifies the input, both the args and kwargs should be returned. The hook should have the following signature::      hook(module, args, kwargs) -&gt; None or a tuple of modified input and kwargs  Args:     hook (Callable): The user defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>forward_pre</code> hooks on this         :class:<code>torch.nn.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>forward_pre</code> hooks         on this :class:<code>torch.nn.Module</code>. Note that global         <code>forward_pre</code> hooks registered with         :func:<code>register_module_forward_pre_hook</code> will fire before all         hooks registered by this method.         Default: <code>False</code>     with_kwargs (bool): If true, the <code>hook</code> will be passed the kwargs         given to the forward function.         Default: <code>False</code>  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Union[Callable[[~T, tuple[Any, ...]], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any]], Optional[tuple[Any, dict[str, Any]]]]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> <li>with_kwargs     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_full_backward_hook <p>Register a backward hook on the module.</p> <p>The hook will be called every time the gradients with respect to a module are computed, and its firing rules are as follows:      1. Ordinarily, the hook fires when the gradients are computed with respect to the module inputs.     2. If none of the module inputs require gradients, the hook will fire when the gradients are computed        with respect to module outputs.     3. If none of the module outputs require gradients, then the hooks will not fire.  The hook should have the following signature::      hook(module, grad_input, grad_output) -&gt; tuple(Tensor) or None  The :attr:<code>grad_input</code> and :attr:<code>grad_output</code> are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr:<code>grad_input</code> in subsequent computations. :attr:<code>grad_input</code> will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr:<code>grad_input</code> and :attr:<code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.  For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function.  .. warning ::     Modifying inputs or outputs inplace is not allowed when using backward hooks and     will raise an error.  Args:     hook (Callable): The user-defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>backward</code> hooks on this         :class:<code>torch.nn.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>backward</code> hooks on         this :class:<code>torch.nn.Module</code>. Note that global         <code>backward</code> hooks registered with         :func:<code>register_module_full_backward_hook</code> will fire before         all hooks registered by this method.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[tuple[torch.Tensor, ...], torch.Tensor], Union[tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_full_backward_pre_hook <p>Register a backward pre-hook on the module.</p> <p>The hook will be called every time the gradients for the module are computed. The hook should have the following signature::      hook(module, grad_output) -&gt; tuple[Tensor] or None  The :attr:<code>grad_output</code> is a tuple. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the output that will be used in place of :attr:<code>grad_output</code> in subsequent computations. Entries in :attr:<code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.  For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function.  .. warning ::     Modifying inputs inplace is not allowed when using backward hooks and     will raise an error.  Args:     hook (Callable): The user-defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>backward_pre</code> hooks on this         :class:<code>torch.nn.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>backward_pre</code> hooks         on this :class:<code>torch.nn.Module</code>. Note that global         <code>backward_pre</code> hooks registered with         :func:<code>register_module_full_backward_pre_hook</code> will fire before         all hooks registered by this method.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_load_state_dict_post_hook <p>Register a post-hook to be run after module's :meth:<code>~nn.Module.load_state_dict</code> is called.</p> <p>It should have the following signature::     hook(module, incompatible_keys) -&gt; None  The <code>module</code> argument is the current module that this hook is registered on, and the <code>incompatible_keys</code> argument is a <code>NamedTuple</code> consisting of attributes <code>missing_keys</code> and <code>unexpected_keys</code>. <code>missing_keys</code> is a <code>list</code> of <code>str</code> containing the missing keys and <code>unexpected_keys</code> is a <code>list</code> of <code>str</code> containing the unexpected keys.  The given incompatible_keys can be modified inplace if needed.  Note that the checks performed when calling :func:<code>load_state_dict</code> with <code>strict=True</code> are affected by modifications the hook makes to <code>missing_keys</code> or <code>unexpected_keys</code>, as expected. Additions to either set of keys will result in an error being thrown when <code>strict=True</code>, and clearing out both missing and unexpected keys will avoid an error.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook </li> </ul> register_load_state_dict_pre_hook <p>Register a pre-hook to be run before module's :meth:<code>~nn.Module.load_state_dict</code> is called.</p> <p>It should have the following signature::     hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -&gt; None  # noqa: B950  Arguments:     hook (Callable): Callable hook that will be invoked before         loading the state dict.</p> <p>Parameters</p> <ul> <li>hook </li> </ul> register_module <p>Alias for :func:<code>add_module</code>.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>module     (Optional[ForwardRef('Module')])    </li> </ul> register_parameter <p>Add a parameter to the module.</p> <p>The parameter can be accessed as an attribute using given name.  Args:     name (str): name of the parameter. The parameter can be accessed         from this module using the given name     param (Parameter or None): parameter to be added to the module. If         <code>None</code>, then operations that run on parameters, such as :attr:<code>cuda</code>,         are ignored. If <code>None</code>, the parameter is not included in the         module's :attr:<code>state_dict</code>.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>param     (Optional[torch.nn.parameter.Parameter])    </li> </ul> register_state_dict_post_hook <p>Register a post-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method.</p> <p>It should have the following signature::     hook(module, state_dict, prefix, local_metadata) -&gt; None  The registered hooks can modify the <code>state_dict</code> inplace.</p> <p>Parameters</p> <ul> <li>hook </li> </ul> register_state_dict_pre_hook <p>Register a pre-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method.</p> <p>It should have the following signature::     hook(module, prefix, keep_vars) -&gt; None  The registered hooks can be used to perform pre-processing before the <code>state_dict</code> call is made.</p> <p>Parameters</p> <ul> <li>hook </li> </ul> requires_grad_ <p>Change if autograd should record operations on parameters in this module.</p> <p>This method sets the parameters' :attr:<code>requires_grad</code> attributes in-place.  This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).  See :ref:<code>locally-disable-grad-doc</code> for a comparison between <code>.requires_grad_()</code> and several similar mechanisms that may be confused with it.  Args:     requires_grad (bool): whether autograd should record operations on                           parameters in this module. Default: <code>True</code>.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>requires_grad     (bool)     \u2013 defaults to <code>True</code> </li> </ul> save <p>Saves a model and its configuration files to a directory, so that it can be loaded with <code>SentenceTransformer(path)</code> again.</p> <p>Args:     path (str): Path on disc where the model will be saved.     model_name (str, optional): Optional model name.     create_model_card (bool, optional): If True, create a README.md with basic information about this model.     train_datasets (list[str], optional): Optional list with the names of the datasets used to train the model.     safe_serialization (bool, optional): If True, save the model using safetensors. If False, save the model         the traditional (but unsafe) PyTorch way.</p> <p>Parameters</p> <ul> <li>path     ('str')    </li> <li>model_name     ('str | None')     \u2013 defaults to <code>None</code> </li> <li>create_model_card     ('bool')     \u2013 defaults to <code>True</code> </li> <li>train_datasets     ('list[str] | None')     \u2013 defaults to <code>None</code> </li> <li>safe_serialization     ('bool')     \u2013 defaults to <code>True</code> </li> </ul> save_pretrained <p>Saves a model and its configuration files to a directory, so that it can be loaded with <code>SentenceTransformer(path)</code> again.</p> <p>Args:     path (str): Path on disk where the model will be saved.     model_name (str, optional): Optional model name.     create_model_card (bool, optional): If True, create a README.md with basic information about this model.     train_datasets (List[str], optional): Optional list with the names of the datasets used to train the model.     safe_serialization (bool, optional): If True, save the model using safetensors. If False, save the model         the traditional (but unsafe) PyTorch way.</p> <p>Parameters</p> <ul> <li>path     ('str')    </li> <li>model_name     ('str | None')     \u2013 defaults to <code>None</code> </li> <li>create_model_card     ('bool')     \u2013 defaults to <code>True</code> </li> <li>train_datasets     ('list[str] | None')     \u2013 defaults to <code>None</code> </li> <li>safe_serialization     ('bool')     \u2013 defaults to <code>True</code> </li> </ul> save_to_hub <p>DEPRECATED, use <code>push_to_hub</code> instead.</p> <p>Uploads all elements of this Sentence Transformer to a new HuggingFace Hub repository.  Args:     repo_id (str): Repository name for your model in the Hub, including the user or organization.     token (str, optional): An authentication token (See https://huggingface.co/settings/token)     private (bool, optional): Set to true, for hosting a private model     safe_serialization (bool, optional): If true, save the model using safetensors. If false, save the model the traditional PyTorch way     commit_message (str, optional): Message to commit while pushing.     local_model_path (str, optional): Path of the model locally. If set, this file path will be uploaded. Otherwise, the current model will be uploaded     exist_ok (bool, optional): If true, saving to an existing repository is OK. If false, saving only to a new repository is possible     replace_model_card (bool, optional): If true, replace an existing model card in the hub with the automatically created model card     train_datasets (List[str], optional): Datasets used to train the model. If set, the datasets will be added to the model card in the Hub.  Returns:     str: The url of the commit of your model in the repository on the Hugging Face Hub.</p> <p>Parameters</p> <ul> <li>repo_id     ('str')    </li> <li>organization     ('str | None')     \u2013 defaults to <code>None</code> </li> <li>token     ('str | None')     \u2013 defaults to <code>None</code>      Hugging Face authentication token to download private models.</li> <li>private     ('bool | None')     \u2013 defaults to <code>None</code> </li> <li>safe_serialization     ('bool')     \u2013 defaults to <code>True</code> </li> <li>commit_message     ('str')     \u2013 defaults to <code>Add new SentenceTransformer model.</code> </li> <li>local_model_path     ('str | None')     \u2013 defaults to <code>None</code> </li> <li>exist_ok     ('bool')     \u2013 defaults to <code>False</code> </li> <li>replace_model_card     ('bool')     \u2013 defaults to <code>False</code> </li> <li>train_datasets     ('list[str] | None')     \u2013 defaults to <code>None</code> </li> </ul> set_adapter <p>Sets a specific adapter by forcing the model to use a that adapter and disable the other adapters.</p> <p>Args:     args:         Positional arguments to pass to the underlying AutoModel <code>set_adapter</code> function. More information can be found in the transformers documentation         https://huggingface.co/docs/transformers/main/en/main_classes/peft#transformers.integrations.PeftAdapterMixin.set_adapter     *kwargs:         Keyword arguments to pass to the underlying AutoModel <code>set_adapter</code> function. More information can be found in the transformers documentation         https://huggingface.co/docs/transformers/main/en/main_classes/peft#transformers.integrations.PeftAdapterMixin.set_adapter</p> <p>Parameters</p> <ul> <li>args </li> <li>kwargs </li> </ul> set_extra_state <p>Set extra state contained in the loaded <code>state_dict</code>.</p> <p>This function is called from :func:<code>load_state_dict</code> to handle any extra state found within the <code>state_dict</code>. Implement this function and a corresponding :func:<code>get_extra_state</code> for your module if you need to store extra state within its <code>state_dict</code>.  Args:     state (dict): Extra state from the <code>state_dict</code></p> <p>Parameters</p> <ul> <li>state     (Any)    </li> </ul> set_pooling_include_prompt <p>Sets the <code>include_prompt</code> attribute in the pooling layer in the model, if there is one.</p> <p>This is useful for INSTRUCTOR models, as the prompt should be excluded from the pooling strategy for these models.  Args:     include_prompt (bool): Whether to include the prompt in the pooling layer.  Returns:     None</p> <p>Parameters</p> <ul> <li>include_prompt     ('bool')    </li> </ul> set_submodule <p>Set the submodule given by <code>target</code> if it exists, otherwise throw an error.</p> <p>.. note::     If <code>strict</code> is set to <code>False</code> (default), the method will replace an existing submodule     or create a new submodule if the parent module exists. If <code>strict</code> is set to <code>True</code>,     the method will only attempt to replace an existing submodule and throw an error if     the submodule does not exist.  For example, let's say you have an <code>nn.Module</code> <code>A</code> that looks like this:  .. code-block:: text      A(         (net_b): Module(             (net_c): Module(                 (conv): Conv2d(3, 3, 3)             )             (linear): Linear(3, 3)         )     )  (The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)  To override the <code>Conv2d</code> with a new submodule <code>Linear</code>, you could call <code>set_submodule(\"net_b.net_c.conv\", nn.Linear(1, 1))</code> where <code>strict</code> could be <code>True</code> or <code>False</code>  To add a new submodule <code>Conv2d</code> to the existing <code>net_b</code> module, you would call <code>set_submodule(\"net_b.conv\", nn.Conv2d(1, 1, 1))</code>.  In the above if you set <code>strict=True</code> and call <code>set_submodule(\"net_b.conv\", nn.Conv2d(1, 1, 1), strict=True)</code>, an AttributeError will be raised because <code>net_b</code> does not have a submodule named <code>conv</code>.  Args:     target: The fully-qualified string name of the submodule         to look for. (See above example for how to specify a         fully-qualified string.)     module: The module to set the submodule to.     strict: If <code>False</code>, the method will replace an existing submodule         or create a new submodule if the parent module exists. If <code>True</code>,         the method will only attempt to replace an existing submodule and throw an error         if the submodule doesn't already exist.  Raises:     ValueError: If the <code>target</code> string is empty or if <code>module</code> is not an instance of <code>nn.Module</code>.     AttributeError: If at any point along the path resulting from         the <code>target</code> string the (sub)path resolves to a non-existent         attribute name or an object that is not an instance of <code>nn.Module</code>.</p> <p>Parameters</p> <ul> <li>target     (str)    </li> <li>module     ('Module')    </li> <li>strict     (bool)     \u2013 defaults to <code>False</code> </li> </ul> share_memory <p>See :meth:<code>torch.Tensor.share_memory_</code>.</p> skiplist_mask <p>Create a mask for the set of input_ids that are in the skiplist.</p> <p>Parameters</p> <ul> <li>input_ids     ('torch.Tensor')    </li> <li>skiplist     ('list[int]')    </li> </ul> smart_batching_collate <p>Transforms a batch from a SmartBatchingDataset to a batch of tensors for the model Here, batch is a list of InputExample instances: [InputExample(...), ...]</p> <p>Args:     batch: a batch from a SmartBatchingDataset  Returns:     a batch of tensors for the model</p> <p>Parameters</p> <ul> <li>batch     ('list[InputExample]')    </li> </ul> start_multi_process_pool <p>Starts a multi-process pool to process the encoding with several independent processes. This method is recommended if you want to encode on multiple GPUs or CPUs. It is advised to start only one process per GPU. This method works together with encode_multi_process and stop_multi_process_pool.</p> <p>Parameters</p> <ul> <li>target_devices     ('list[str]')     \u2013 defaults to <code>None</code> </li> </ul> <p>Returns</p> <p>dict:     A dictionary with the target processes, an input queue, and an output queue.</p> state_dict <p>Return a dictionary containing references to the whole state of the module.</p> <p>Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to <code>None</code> are not included.  .. note::     The returned object is a shallow copy. It contains references     to the module's parameters and buffers.  .. warning::     Currently <code>state_dict()</code> also accepts positional arguments for     <code>destination</code>, <code>prefix</code> and <code>keep_vars</code> in order. However,     this is being deprecated and keyword arguments will be enforced in     future releases.  .. warning::     Please avoid the use of argument <code>destination</code> as it is not     designed for end-users.  Args:     destination (dict, optional): If provided, the state of module will         be updated into the dict and the same object is returned.         Otherwise, an <code>OrderedDict</code> will be created and returned.         Default: <code>None</code>.     prefix (str, optional): a prefix added to parameter and buffer         names to compose the keys in state_dict. Default: <code>''</code>.     keep_vars (bool, optional): by default the :class:<code>~torch.Tensor</code> s         returned in the state dict are detached from autograd. If it's         set to <code>True</code>, detaching will not be performed.         Default: <code>False</code>.  Returns:     dict:         a dictionary containing a whole state of the module  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; module.state_dict().keys()     ['bias', 'weight']</p> <p>Parameters</p> <ul> <li>args </li> <li>destination     \u2013 defaults to <code>None</code> </li> <li>prefix     \u2013 defaults to ``    </li> <li>keep_vars     \u2013 defaults to <code>False</code> </li> </ul> stop_multi_process_pool <p>Stops all processes started with start_multi_process_pool.</p> <p>Args:     pool (Dict[str, object]): A dictionary containing the input queue, output queue, and process list.  Returns:     None</p> <ul> <li>pool     (\"dict[Literal['input', 'output', 'processes'], Any]\")    </li> </ul> to <p>Move and/or cast the parameters and buffers.</p> <p>This can be called as  .. function:: to(device=None, dtype=None, non_blocking=False)    :noindex:  .. function:: to(dtype, non_blocking=False)    :noindex:  .. function:: to(tensor, non_blocking=False)    :noindex:  .. function:: to(memory_format=torch.channels_last)    :noindex:  Its signature is similar to :meth:<code>torch.Tensor.to</code>, but only accepts floating point or complex :attr:<code>dtype</code>\\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr:<code>dtype</code> (if given). The integral parameters and buffers will be moved :attr:<code>device</code>, if that is given, but with dtypes unchanged. When :attr:<code>non_blocking</code> is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices.  See below for examples.  .. note::     This method modifies the module in-place.  Args:     device (:class:<code>torch.device</code>): the desired device of the parameters         and buffers in this module     dtype (:class:<code>torch.dtype</code>): the desired floating point or complex dtype of         the parameters and buffers in this module     tensor (torch.Tensor): Tensor whose dtype and device are the desired         dtype and device for all parameters and buffers in this module     memory_format (:class:<code>torch.memory_format</code>): the desired memory         format for 4D parameters and buffers in this module (keyword         only argument)  Returns:     Module: self  Examples::      &gt;&gt;&gt; # xdoctest: +IGNORE_WANT(\"non-deterministic\")     &gt;&gt;&gt; linear = nn.Linear(2, 2)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1913, -0.3420],             [-0.5113, -0.2325]])     &gt;&gt;&gt; linear.to(torch.double)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1913, -0.3420],             [-0.5113, -0.2325]], dtype=torch.float64)     &gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)     &gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")     &gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1914, -0.3420],             [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')     &gt;&gt;&gt; cpu = torch.device(\"cpu\")     &gt;&gt;&gt; linear.to(cpu)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1914, -0.3420],             [-0.5112, -0.2324]], dtype=torch.float16)      &gt;&gt;&gt; linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.3741+0.j,  0.2382+0.j],             [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)     &gt;&gt;&gt; linear(torch.ones(3, 2, dtype=torch.cdouble))     tensor([[0.6122+0.j, 0.1150+0.j],             [0.6122+0.j, 0.1150+0.j],             [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)</p> <p>Parameters</p> <ul> <li>args </li> <li>kwargs </li> </ul> to_empty <p>Move the parameters and buffers to the specified device without copying storage.</p> <p>Args:     device (:class:<code>torch.device</code>): The desired device of the parameters         and buffers in this module.     recurse (bool): Whether parameters and buffers of submodules should         be recursively moved to the specified device.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, str, torch.device, NoneType])       Device (like \"cuda\", \"cpu\", \"mps\", \"npu\") that should be used for computation. If None, checks if a GPU can be used.</li> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> </ul> tokenize <p>Tokenizes the input texts.</p> <p>Args:     texts (Union[list[str], list[dict], list[tuple[str, str]]]): A list of texts to be tokenized.     is_query (bool): Flag to indicate if the texts are queries. Defaults to True.     pad (bool): Flag to indicate if elements should be padded to max length. Defaults to False.  Returns:     dict[str, torch.Tensor]: A dictionary of tensors with the tokenized texts, including \"input_ids\",         \"attention_mask\", and optionally \"token_type_ids\".</p> <p>Parameters</p> <ul> <li>texts     ('list[str] | list[dict] | list[tuple[str, str]]')    </li> <li>is_query     ('bool')     \u2013 defaults to <code>True</code> </li> <li>pad     ('bool')     \u2013 defaults to <code>False</code> </li> <li>task     ('str | None')     \u2013 defaults to <code>None</code> </li> </ul> train <p>Set the module in training mode.</p> <p>This has an effect only on certain modules. See the documentation of particular modules for details of their behaviors in training/evaluation mode, i.e., whether they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>, etc.  Args:     mode (bool): whether to set training mode (<code>True</code>) or evaluation                  mode (<code>False</code>). Default: <code>True</code>.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>mode     (bool)     \u2013 defaults to <code>True</code> </li> </ul> truncate_sentence_embeddings <p>In this context, :meth:<code>SentenceTransformer.encode &lt;sentence_transformers.SentenceTransformer.encode&gt;</code> outputs sentence embeddings truncated at dimension <code>truncate_dim</code>.</p> <p>This may be useful when you are using the same model for different applications where different dimensions are needed.  Args:     truncate_dim (int, optional): The dimension to truncate sentence embeddings to. <code>None</code> does no truncation.  Example:     ::          from sentence_transformers import SentenceTransformer          model = SentenceTransformer(\"all-mpnet-base-v2\")          with model.truncate_sentence_embeddings(truncate_dim=16):             embeddings_truncated = model.encode([\"hello there\", \"hiya\"])         assert embeddings_truncated.shape[-1] == 16</p> <p>Parameters</p> <ul> <li>truncate_dim     ('int | None')       The dimension to truncate sentence embeddings to. <code>None</code> does no truncation. Truncation is only applicable during inference when :meth:<code>SentenceTransformer.encode</code> is called.</li> </ul> type <p>Casts all parameters and buffers to :attr:<code>dst_type</code>.</p> <p>.. note::     This method modifies the module in-place.  Args:     dst_type (type or string): the desired type  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>dst_type     (Union[torch.dtype, str])    </li> </ul> xpu <p>Move all model parameters and buffers to the XPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized.  .. note::     This method modifies the module in-place.  Arguments:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[torch.device, int, NoneType])     \u2013 defaults to <code>None</code>      Device (like \"cuda\", \"cpu\", \"mps\", \"npu\") that should be used for computation. If None, checks if a GPU can be used.</li> </ul> zero_grad <p>Reset gradients of all model parameters.</p> <p>See similar function under :class:<code>torch.optim.Optimizer</code> for more context.  Args:     set_to_none (bool): instead of setting to zero, set the grads to None.         See :meth:<code>torch.optim.Optimizer.zero_grad</code> for details.</p> <p>Parameters</p> <ul> <li>set_to_none     (bool)     \u2013 defaults to <code>True</code> </li> </ul>"},{"location":"api/models/Dense/","title":"Dense","text":"<p>Performs linear projection on the token embeddings to a lower dimension.</p>"},{"location":"api/models/Dense/#parameters","title":"Parameters","text":"<ul> <li> <p>in_features ('int')</p> <p>Size of the embeddings in output of the transformer.</p> </li> <li> <p>out_features ('int')</p> <p>Size of the output embeddings after linear projection</p> </li> <li> <p>bias ('bool') \u2013 defaults to <code>True</code></p> <p>Add a bias vector</p> </li> <li> <p>activation_function \u2013 defaults to <code>Identity()</code></p> </li> <li> <p>init_weight ('torch.Tensor') \u2013 defaults to <code>None</code></p> <p>Initial value for the matrix of the linear layer</p> </li> <li> <p>init_bias ('torch.Tensor') \u2013 defaults to <code>None</code></p> <p>Initial value for the bias of the linear layer.</p> </li> <li> <p>use_residual ('bool') \u2013 defaults to <code>False</code></p> <p>Whether to use residual for the linear layer.</p> </li> </ul>"},{"location":"api/models/Dense/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from pylate import models\n\n&gt;&gt;&gt; model = models.Dense(\n...     in_features=768,\n...     out_features=128,\n... )\n\n&gt;&gt;&gt; features = {\n...     \"token_embeddings\": torch.randn(2, 768),\n... }\n\n&gt;&gt;&gt; projected_features = model(features)\n\n&gt;&gt;&gt; assert projected_features[\"token_embeddings\"].shape == (2, 128)\n&gt;&gt;&gt; assert isinstance(model, DenseSentenceTransformer)\n</code></pre>"},{"location":"api/models/Dense/#methods","title":"Methods","text":"call <p>Call self as a function.</p> <p>Parameters</p> <ul> <li>args </li> <li>kwargs </li> </ul> add_module <p>Add a child module to the current module.</p> <p>The module can be accessed as an attribute using the given name.  Args:     name (str): name of the child module. The child module can be         accessed from this module using the given name     module (Module): child module to be added to the module.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>module     (Optional[ForwardRef('Module')])    </li> </ul> apply <p>Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self.</p> <p>Typical use includes initializing the parameters of a model (see also :ref:<code>nn-init-doc</code>).  Args:     fn (:class:<code>Module</code> -&gt; None): function to be applied to each submodule  Returns:     Module: self  Example::      &gt;&gt;&gt; @torch.no_grad()     &gt;&gt;&gt; def init_weights(m):     &gt;&gt;&gt;     print(m)     &gt;&gt;&gt;     if type(m) == nn.Linear:     &gt;&gt;&gt;         m.weight.fill_(1.0)     &gt;&gt;&gt;         print(m.weight)     &gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))     &gt;&gt;&gt; net.apply(init_weights)     Linear(in_features=2, out_features=2, bias=True)     Parameter containing:     tensor([[1., 1.],             [1., 1.]], requires_grad=True)     Linear(in_features=2, out_features=2, bias=True)     Parameter containing:     tensor([[1., 1.],             [1., 1.]], requires_grad=True)     Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     )</p> <p>Parameters</p> <ul> <li>fn     (Callable[[ForwardRef('Module')], NoneType])    </li> </ul> bfloat16 <p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> buffers <p>Return an iterator over module buffers.</p> <p>Args:     recurse (bool): if True, then yields buffers of this module         and all submodules. Otherwise, yields only buffers that         are direct members of this module.  Yields:     torch.Tensor: module buffer  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for buf in model.buffers():     &gt;&gt;&gt;     print(type(buf), buf.size())      (20L,)      (20L, 1L, 5L, 5L) <p>Parameters</p> <ul> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> </ul> children <p>Return an iterator over immediate children modules.</p> <p>Yields:     Module: a child module</p> compile <p>Compile this Module's forward using :func:<code>torch.compile</code>.</p> <p>This Module's <code>__call__</code> method is compiled and all arguments are passed as-is to :func:<code>torch.compile</code>.  See :func:<code>torch.compile</code> for details on the arguments for this function.</p> <p>Parameters</p> <ul> <li>args </li> <li>kwargs </li> </ul> cpu <p>Move all model parameters and buffers to the CPU.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> cuda <p>Move all model parameters and buffers to the GPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing the optimizer if the module will live on GPU while being optimized.  .. note::     This method modifies the module in-place.  Args:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[torch.device, int, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> double <p>Casts all floating point parameters and buffers to <code>double</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> eval <p>Set the module in evaluation mode.</p> <p>This has an effect only on certain modules. See the documentation of particular modules for details of their behaviors in training/evaluation mode, i.e. whether they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>, etc.  This is equivalent with :meth:<code>self.train(False) &lt;torch.nn.Module.train&gt;</code>.  See :ref:<code>locally-disable-grad-doc</code> for a comparison between <code>.eval()</code> and several similar mechanisms that may be confused with it.  Returns:     Module: self</p> extra_repr <p>Return the extra representation of the module.</p> <p>To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.</p> float <p>Casts all floating point parameters and buffers to <code>float</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> forward <p>Performs linear projection on the token embeddings.</p> <p>Parameters</p> <ul> <li>features     ('dict[str, torch.Tensor]')    </li> </ul> from_sentence_transformers <p>Converts a SentenceTransformer Dense model to a Dense model. Our Dense model does not have the activation function.</p> <ul> <li>dense     ('DenseSentenceTransformer')    </li> </ul> from_stanford_weights <p>Load the weight of the Dense layer using weights from a stanford-nlp checkpoint.</p> <p>Parameters</p> <ul> <li>model_name_or_path     ('str | os.PathLike')    </li> <li>cache_folder     ('str | os.PathLike | None')     \u2013 defaults to <code>None</code> </li> <li>revision     ('str | None')     \u2013 defaults to <code>None</code> </li> <li>local_files_only     ('bool | None')     \u2013 defaults to <code>None</code> </li> <li>token     ('str | bool | None')     \u2013 defaults to <code>None</code> </li> <li>use_auth_token     ('str | bool | None')     \u2013 defaults to <code>None</code> </li> </ul> get_buffer <p>Return the buffer given by <code>target</code> if it exists, otherwise throw an error.</p> <p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method's functionality as well as how to correctly specify <code>target</code>.  Args:     target: The fully-qualified string name of the buffer         to look for. (See <code>get_submodule</code> for how to specify a         fully-qualified string.)  Returns:     torch.Tensor: The buffer referenced by <code>target</code>  Raises:     AttributeError: If the target string references an invalid         path or resolves to something that is not a         buffer</p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> get_config_dict <p>Returns a dictionary of the configuration parameters of the module.</p> <p>These parameters are used to save the module's configuration when saving the model to disk, and again used to initialize the module when loading it from a pre-trained model. The keys used in the dictionary are defined in the <code>config_keys</code> class variable.  Returns:     dict[str, Any]: A dictionary of the configuration parameters of the module.</p> get_extra_state <p>Return any extra state to include in the module's state_dict.</p> <p>Implement this and a corresponding :func:<code>set_extra_state</code> for your module if you need to store extra state. This function is called when building the module's <code>state_dict()</code>.  Note that extra state should be picklable to ensure working serialization of the state_dict. We only provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes.  Returns:     object: Any extra state to store in the module's state_dict</p> get_parameter <p>Return the parameter given by <code>target</code> if it exists, otherwise throw an error.</p> <p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method's functionality as well as how to correctly specify <code>target</code>.  Args:     target: The fully-qualified string name of the Parameter         to look for. (See <code>get_submodule</code> for how to specify a         fully-qualified string.)  Returns:     torch.nn.Parameter: The Parameter referenced by <code>target</code>  Raises:     AttributeError: If the target string references an invalid         path or resolves to something that is not an         <code>nn.Parameter</code></p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> get_sentence_embedding_dimension get_submodule <p>Return the submodule given by <code>target</code> if it exists, otherwise throw an error.</p> <p>For example, let's say you have an <code>nn.Module</code> <code>A</code> that looks like this:  .. code-block:: text      A(         (net_b): Module(             (net_c): Module(                 (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))             )             (linear): Linear(in_features=100, out_features=200, bias=True)         )     )  (The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> which has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)  To check whether or not we have the <code>linear</code> submodule, we would call <code>get_submodule(\"net_b.linear\")</code>. To check whether we have the <code>conv</code> submodule, we would call <code>get_submodule(\"net_b.net_c.conv\")</code>.  The runtime of <code>get_submodule</code> is bounded by the degree of module nesting in <code>target</code>. A query against <code>named_modules</code> achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, <code>get_submodule</code> should always be used.  Args:     target: The fully-qualified string name of the submodule         to look for. (See above example for how to specify a         fully-qualified string.)  Returns:     torch.nn.Module: The submodule referenced by <code>target</code>  Raises:     AttributeError: If at any point along the path resulting from         the target string the (sub)path resolves to a non-existent         attribute name or an object that is not an instance of <code>nn.Module</code>.</p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> half <p>Casts all floating point parameters and buffers to <code>half</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> ipu <p>Move all model parameters and buffers to the IPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing the optimizer if the module will live on IPU while being optimized.  .. note::     This method modifies the module in-place.  Arguments:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[torch.device, int, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> load <p>Load a Dense layer.</p> <ul> <li>input_path </li> </ul> load_config <p>Load the configuration of the module from a model checkpoint. The checkpoint can be either a local directory or a model id on Hugging Face. The configuration is loaded from a JSON file, which contains the parameters used to initialize the module.</p> <p>Args:     model_name_or_path (str): The path to the model directory or the name of the model on Hugging Face.     subfolder (str, optional): The subfolder within the model directory to load from, e.g. <code>\"1_Pooling\"</code>.         Defaults to <code>\"\"</code>.     config_filename (str | None, optional): The name of the configuration file to load.         If None, uses the default configuration file name defined in the <code>config_file_name</code> class variable.         Defaults to None.     token (bool | str | None, optional): The token to use for authentication when loading from Hugging Face.         If None, tries to use a token saved using <code>huggingface-cli login</code> or the <code>HF_TOKEN</code> environment variable.         Defaults to None.     cache_folder (str | None, optional): The folder to use for caching the model files.         If None, uses the default cache folder for Hugging Face, <code>~/.cache/huggingface</code>. Defaults to None.     revision (str | None, optional): The revision of the model to load.         If None, uses the latest revision. Defaults to None.     local_files_only (bool, optional): Whether to only load local files. Defaults to False.  Returns:     dict[str, Any]: A dictionary of the configuration parameters of the module.</p> <p>Parameters</p> <ul> <li>model_name_or_path     ('str')    </li> <li>subfolder     ('str')     \u2013 defaults to ``    </li> <li>config_filename     ('str | None')     \u2013 defaults to <code>None</code> </li> <li>token     ('bool | str | None')     \u2013 defaults to <code>None</code> </li> <li>cache_folder     ('str | None')     \u2013 defaults to <code>None</code> </li> <li>revision     ('str | None')     \u2013 defaults to <code>None</code> </li> <li>local_files_only     ('bool')     \u2013 defaults to <code>False</code> </li> </ul> load_dir_path <p>A utility function to load a directory from a model checkpoint. The checkpoint can be either a local directory or a model id on Hugging Face.</p> <p>Args:     model_name_or_path (str): The path to the model directory or the name of the model on Hugging Face.     subfolder (str, optional): The subfolder within the model directory to load from, e.g. <code>\"1_Pooling\"</code>.         Defaults to <code>\"\"</code>.     token (bool | str | None, optional): The token to use for authentication when loading from Hugging Face.         If None, tries to use a token saved using <code>huggingface-cli login</code> or the <code>HF_TOKEN</code> environment variable.         Defaults to None.     cache_folder (str | None, optional): The folder to use for caching the model files.         If None, uses the default cache folder for Hugging Face, <code>~/.cache/huggingface</code>. Defaults to None.     revision (str | None, optional): The revision of the model to load.         If None, uses the latest revision. Defaults to None.     local_files_only (bool, optional): Whether to only load local files. Defaults to False.  Returns:     str: The path to the loaded directory.</p> <p>Parameters</p> <ul> <li>model_name_or_path     ('str')    </li> <li>subfolder     ('str')     \u2013 defaults to ``    </li> <li>token     ('bool | str | None')     \u2013 defaults to <code>None</code> </li> <li>cache_folder     ('str | None')     \u2013 defaults to <code>None</code> </li> <li>revision     ('str | None')     \u2013 defaults to <code>None</code> </li> <li>local_files_only     ('bool')     \u2013 defaults to <code>False</code> </li> </ul> load_file_path <p>A utility function to load a file from a model checkpoint. The checkpoint can be either a local directory or a model id on Hugging Face. The file is loaded from the specified subfolder within the model directory.</p> <p>Args:     model_name_or_path (str): The path to the model directory or the name of the model on Hugging Face.     filename (str): The name of the file to load.     subfolder (str, optional): The subfolder within the model directory to load from, e.g. <code>\"1_Pooling\"</code>.         Defaults to <code>\"\"</code>.     token (bool | str | None, optional): The token to use for authentication when loading from Hugging Face.         If None, tries to use a token saved using <code>huggingface-cli login</code> or the <code>HF_TOKEN</code> environment variable.         Defaults to None.     cache_folder (str | None, optional): The folder to use for caching the model files.         If None, uses the default cache folder for Hugging Face, <code>~/.cache/huggingface</code>. Defaults to None.     revision (str | None, optional): The revision of the model to load.         If None, uses the latest revision. Defaults to None.     local_files_only (bool, optional): Whether to only load local files. Defaults to False.  Returns:     str | None: The path to the loaded file, or None if the file was not found.</p> <p>Parameters</p> <ul> <li>model_name_or_path     ('str')    </li> <li>filename     ('str')    </li> <li>subfolder     ('str')     \u2013 defaults to ``    </li> <li>token     ('bool | str | None')     \u2013 defaults to <code>None</code> </li> <li>cache_folder     ('str | None')     \u2013 defaults to <code>None</code> </li> <li>revision     ('str | None')     \u2013 defaults to <code>None</code> </li> <li>local_files_only     ('bool')     \u2013 defaults to <code>False</code> </li> </ul> load_state_dict <p>Copy parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants.</p> <p>If :attr:<code>strict</code> is <code>True</code>, then the keys of :attr:<code>state_dict</code> must exactly match the keys returned by this module's :meth:<code>~torch.nn.Module.state_dict</code> function.  .. warning::     If :attr:<code>assign</code> is <code>True</code> the optimizer must be created after     the call to :attr:<code>load_state_dict</code> unless     :func:<code>~torch.__future__.get_swap_module_params_on_conversion</code> is <code>True</code>.  Args:     state_dict (dict): a dict containing parameters and         persistent buffers.     strict (bool, optional): whether to strictly enforce that the keys         in :attr:<code>state_dict</code> match the keys returned by this module's         :meth:<code>~torch.nn.Module.state_dict</code> function. Default: <code>True</code>     assign (bool, optional): When set to <code>False</code>, the properties of the tensors         in the current module are preserved whereas setting it to <code>True</code> preserves         properties of the Tensors in the state dict. The only         exception is the <code>requires_grad</code> field of :class:<code>~torch.nn.Parameter</code>         for which the value from the module is preserved. Default: <code>False</code>  Returns:     <code>NamedTuple</code> with <code>missing_keys</code> and <code>unexpected_keys</code> fields:         * <code>missing_keys</code> is a list of str containing any keys that are expected             by this module but missing from the provided <code>state_dict</code>.         * <code>unexpected_keys</code> is a list of str containing the keys that are not             expected by this module but present in the provided <code>state_dict</code>.  Note:     If a parameter or buffer is registered as <code>None</code> and its corresponding key     exists in :attr:<code>state_dict</code>, :meth:<code>load_state_dict</code> will raise a     <code>RuntimeError</code>.</p> <p>Parameters</p> <ul> <li>state_dict     (collections.abc.Mapping[str, typing.Any])    </li> <li>strict     (bool)     \u2013 defaults to <code>True</code> </li> <li>assign     (bool)     \u2013 defaults to <code>False</code> </li> </ul> load_torch_weights <p>A utility function to load the PyTorch weights of a model from a checkpoint. The checkpoint can be either a local directory or a model id on Hugging Face. The weights are loaded from either a <code>model.safetensors</code> file or a <code>pytorch_model.bin</code> file, depending on which one is available. This method either loads the weights into the model or returns the weights as a state dictionary.</p> <p>Args:     model_name_or_path (str): The path to the model directory or the name of the model on Hugging Face.     subfolder (str, optional): The subfolder within the model directory to load from, e.g. <code>\"2_Dense\"</code>.         Defaults to <code>\"\"</code>.     token (bool | str | None, optional): The token to use for authentication when loading from Hugging Face.         If None, tries to use a token saved using <code>huggingface-cli login</code> or the <code>HF_TOKEN</code> environment variable.         Defaults to None.     cache_folder (str | None, optional): The folder to use for caching the model files.         If None, uses the default cache folder for Hugging Face, <code>~/.cache/huggingface</code>. Defaults to None.     revision (str | None, optional): The revision of the model to load.         If None, uses the latest revision. Defaults to None.     local_files_only (bool, optional): Whether to only load local files. Defaults to False.     model (Self | None, optional): The model to load the weights into. If None, returns the weights as a state         dictionary. Defaults to None.  Raises:     ValueError: If neither a <code>model.safetensors</code> file nor a <code>pytorch_model.bin</code> file is found in the model         checkpoint in the <code>subfolder</code>.  Returns:     Self | dict[str, torch.Tensor]: The model with the loaded weights or the weights as a state dictionary,         depending on the value of the <code>model</code> argument.</p> <p>Parameters</p> <ul> <li>model_name_or_path     ('str')    </li> <li>subfolder     ('str')     \u2013 defaults to ``    </li> <li>token     ('bool | str | None')     \u2013 defaults to <code>None</code> </li> <li>cache_folder     ('str | None')     \u2013 defaults to <code>None</code> </li> <li>revision     ('str | None')     \u2013 defaults to <code>None</code> </li> <li>local_files_only     ('bool')     \u2013 defaults to <code>False</code> </li> <li>model     ('Self | None')     \u2013 defaults to <code>None</code> </li> </ul> modules <p>Return an iterator over all modules in the network.</p> <p>Yields:     Module: a module in the network  Note:     Duplicate modules are returned only once. In the following     example, <code>l</code> will be returned only once.  Example::      &gt;&gt;&gt; l = nn.Linear(2, 2)     &gt;&gt;&gt; net = nn.Sequential(l, l)     &gt;&gt;&gt; for idx, m in enumerate(net.modules()):     ...     print(idx, '-&gt;', m)      0 -&gt; Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     )     1 -&gt; Linear(in_features=2, out_features=2, bias=True)</p> mtia <p>Move all model parameters and buffers to the MTIA.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing the optimizer if the module will live on MTIA while being optimized.  .. note::     This method modifies the module in-place.  Arguments:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[torch.device, int, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> named_buffers <p>Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.</p> <p>Args:     prefix (str): prefix to prepend to all buffer names.     recurse (bool, optional): if True, then yields buffers of this module         and all submodules. Otherwise, yields only buffers that         are direct members of this module. Defaults to True.     remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.  Yields:     (str, torch.Tensor): Tuple containing the name and buffer  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, buf in self.named_buffers():     &gt;&gt;&gt;     if name in ['running_var']:     &gt;&gt;&gt;         print(buf.size())</p> <p>Parameters</p> <ul> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> named_children <p>Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.</p> <p>Yields:     (str, Module): Tuple containing a name and child module  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, module in model.named_children():     &gt;&gt;&gt;     if name in ['conv4', 'conv5']:     &gt;&gt;&gt;         print(module)</p> named_modules <p>Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.</p> <p>Args:     memo: a memo to store the set of modules already added to the result     prefix: a prefix that will be added to the name of the module     remove_duplicate: whether to remove the duplicated module instances in the result         or not  Yields:     (str, Module): Tuple of name and module  Note:     Duplicate modules are returned only once. In the following     example, <code>l</code> will be returned only once.  Example::      &gt;&gt;&gt; l = nn.Linear(2, 2)     &gt;&gt;&gt; net = nn.Sequential(l, l)     &gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):     ...     print(idx, '-&gt;', m)      0 -&gt; ('', Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     ))     1 -&gt; ('0', Linear(in_features=2, out_features=2, bias=True))</p> <p>Parameters</p> <ul> <li>memo     (Optional[set['Module']])     \u2013 defaults to <code>None</code> </li> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> named_parameters <p>Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.</p> <p>Args:     prefix (str): prefix to prepend to all parameter names.     recurse (bool): if True, then yields parameters of this module         and all submodules. Otherwise, yields only parameters that         are direct members of this module.     remove_duplicate (bool, optional): whether to remove the duplicated         parameters in the result. Defaults to True.  Yields:     (str, Parameter): Tuple containing the name and parameter  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, param in self.named_parameters():     &gt;&gt;&gt;     if name in ['bias']:     &gt;&gt;&gt;         print(param.size())</p> <p>Parameters</p> <ul> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> parameters <p>Return an iterator over module parameters.</p> <p>This is typically passed to an optimizer.  Args:     recurse (bool): if True, then yields parameters of this module         and all submodules. Otherwise, yields only parameters that         are direct members of this module.  Yields:     Parameter: module parameter  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for param in model.parameters():     &gt;&gt;&gt;     print(type(param), param.size())      (20L,)      (20L, 1L, 5L, 5L) <p>Parameters</p> <ul> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> </ul> register_backward_hook <p>Register a backward hook on the module.</p> <p>This function is deprecated in favor of :meth:<code>~torch.nn.Module.register_full_backward_hook</code> and the behavior of this function will change in future versions.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[tuple[torch.Tensor, ...], torch.Tensor], Union[tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]])    </li> </ul> register_buffer <p>Add a buffer to the module.</p> <p>This is typically used to register a buffer that should not be considered a model parameter. For example, BatchNorm's <code>running_mean</code> is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:<code>persistent</code> to <code>False</code>. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:<code>state_dict</code>.  Buffers can be accessed as attributes using given names.  Args:     name (str): name of the buffer. The buffer can be accessed         from this module using the given name     tensor (Tensor or None): buffer to be registered. If <code>None</code>, then operations         that run on buffers, such as :attr:<code>cuda</code>, are ignored. If <code>None</code>,         the buffer is not included in the module's :attr:<code>state_dict</code>.     persistent (bool): whether the buffer is part of this module's         :attr:<code>state_dict</code>.  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; self.register_buffer('running_mean', torch.zeros(num_features))</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>tensor     (Optional[torch.Tensor])    </li> <li>persistent     (bool)     \u2013 defaults to <code>True</code> </li> </ul> register_forward_hook <p>Register a forward hook on the module.</p> <p>The hook will be called every time after :func:<code>forward</code> has computed an output.  If <code>with_kwargs</code> is <code>False</code> or not specified, the input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the <code>forward</code>. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func:<code>forward</code> is called. The hook should have the following signature::      hook(module, args, output) -&gt; None or modified output  If <code>with_kwargs</code> is <code>True</code>, the forward hook will be passed the <code>kwargs</code> given to the forward function and be expected to return the output possibly modified. The hook should have the following signature::      hook(module, args, kwargs, output) -&gt; None or modified output  Args:     hook (Callable): The user defined hook to be registered.     prepend (bool): If <code>True</code>, the provided <code>hook</code> will be fired         before all existing <code>forward</code> hooks on this         :class:<code>torch.nn.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>forward</code> hooks on         this :class:<code>torch.nn.Module</code>. Note that global         <code>forward</code> hooks registered with         :func:<code>register_module_forward_hook</code> will fire before all hooks         registered by this method.         Default: <code>False</code>     with_kwargs (bool): If <code>True</code>, the <code>hook</code> will be passed the         kwargs given to the forward function.         Default: <code>False</code>     always_call (bool): If <code>True</code> the <code>hook</code> will be run regardless of         whether an exception is raised while calling the Module.         Default: <code>False</code>  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Union[Callable[[~T, tuple[Any, ...], Any], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Optional[Any]]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> <li>with_kwargs     (bool)     \u2013 defaults to <code>False</code> </li> <li>always_call     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_forward_pre_hook <p>Register a forward pre-hook on the module.</p> <p>The hook will be called every time before :func:<code>forward</code> is invoked.  If <code>with_kwargs</code> is false or not specified, the input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the <code>forward</code>. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned (unless that value is already a tuple). The hook should have the following signature::      hook(module, args) -&gt; None or modified input  If <code>with_kwargs</code> is true, the forward pre-hook will be passed the kwargs given to the forward function. And if the hook modifies the input, both the args and kwargs should be returned. The hook should have the following signature::      hook(module, args, kwargs) -&gt; None or a tuple of modified input and kwargs  Args:     hook (Callable): The user defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>forward_pre</code> hooks on this         :class:<code>torch.nn.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>forward_pre</code> hooks         on this :class:<code>torch.nn.Module</code>. Note that global         <code>forward_pre</code> hooks registered with         :func:<code>register_module_forward_pre_hook</code> will fire before all         hooks registered by this method.         Default: <code>False</code>     with_kwargs (bool): If true, the <code>hook</code> will be passed the kwargs         given to the forward function.         Default: <code>False</code>  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Union[Callable[[~T, tuple[Any, ...]], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any]], Optional[tuple[Any, dict[str, Any]]]]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> <li>with_kwargs     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_full_backward_hook <p>Register a backward hook on the module.</p> <p>The hook will be called every time the gradients with respect to a module are computed, and its firing rules are as follows:      1. Ordinarily, the hook fires when the gradients are computed with respect to the module inputs.     2. If none of the module inputs require gradients, the hook will fire when the gradients are computed        with respect to module outputs.     3. If none of the module outputs require gradients, then the hooks will not fire.  The hook should have the following signature::      hook(module, grad_input, grad_output) -&gt; tuple(Tensor) or None  The :attr:<code>grad_input</code> and :attr:<code>grad_output</code> are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr:<code>grad_input</code> in subsequent computations. :attr:<code>grad_input</code> will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr:<code>grad_input</code> and :attr:<code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.  For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function.  .. warning ::     Modifying inputs or outputs inplace is not allowed when using backward hooks and     will raise an error.  Args:     hook (Callable): The user-defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>backward</code> hooks on this         :class:<code>torch.nn.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>backward</code> hooks on         this :class:<code>torch.nn.Module</code>. Note that global         <code>backward</code> hooks registered with         :func:<code>register_module_full_backward_hook</code> will fire before         all hooks registered by this method.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[tuple[torch.Tensor, ...], torch.Tensor], Union[tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_full_backward_pre_hook <p>Register a backward pre-hook on the module.</p> <p>The hook will be called every time the gradients for the module are computed. The hook should have the following signature::      hook(module, grad_output) -&gt; tuple[Tensor] or None  The :attr:<code>grad_output</code> is a tuple. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the output that will be used in place of :attr:<code>grad_output</code> in subsequent computations. Entries in :attr:<code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.  For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function.  .. warning ::     Modifying inputs inplace is not allowed when using backward hooks and     will raise an error.  Args:     hook (Callable): The user-defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>backward_pre</code> hooks on this         :class:<code>torch.nn.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>backward_pre</code> hooks         on this :class:<code>torch.nn.Module</code>. Note that global         <code>backward_pre</code> hooks registered with         :func:<code>register_module_full_backward_pre_hook</code> will fire before         all hooks registered by this method.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_load_state_dict_post_hook <p>Register a post-hook to be run after module's :meth:<code>~nn.Module.load_state_dict</code> is called.</p> <p>It should have the following signature::     hook(module, incompatible_keys) -&gt; None  The <code>module</code> argument is the current module that this hook is registered on, and the <code>incompatible_keys</code> argument is a <code>NamedTuple</code> consisting of attributes <code>missing_keys</code> and <code>unexpected_keys</code>. <code>missing_keys</code> is a <code>list</code> of <code>str</code> containing the missing keys and <code>unexpected_keys</code> is a <code>list</code> of <code>str</code> containing the unexpected keys.  The given incompatible_keys can be modified inplace if needed.  Note that the checks performed when calling :func:<code>load_state_dict</code> with <code>strict=True</code> are affected by modifications the hook makes to <code>missing_keys</code> or <code>unexpected_keys</code>, as expected. Additions to either set of keys will result in an error being thrown when <code>strict=True</code>, and clearing out both missing and unexpected keys will avoid an error.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook </li> </ul> register_load_state_dict_pre_hook <p>Register a pre-hook to be run before module's :meth:<code>~nn.Module.load_state_dict</code> is called.</p> <p>It should have the following signature::     hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -&gt; None  # noqa: B950  Arguments:     hook (Callable): Callable hook that will be invoked before         loading the state dict.</p> <p>Parameters</p> <ul> <li>hook </li> </ul> register_module <p>Alias for :func:<code>add_module</code>.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>module     (Optional[ForwardRef('Module')])    </li> </ul> register_parameter <p>Add a parameter to the module.</p> <p>The parameter can be accessed as an attribute using given name.  Args:     name (str): name of the parameter. The parameter can be accessed         from this module using the given name     param (Parameter or None): parameter to be added to the module. If         <code>None</code>, then operations that run on parameters, such as :attr:<code>cuda</code>,         are ignored. If <code>None</code>, the parameter is not included in the         module's :attr:<code>state_dict</code>.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>param     (Optional[torch.nn.parameter.Parameter])    </li> </ul> register_state_dict_post_hook <p>Register a post-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method.</p> <p>It should have the following signature::     hook(module, state_dict, prefix, local_metadata) -&gt; None  The registered hooks can modify the <code>state_dict</code> inplace.</p> <p>Parameters</p> <ul> <li>hook </li> </ul> register_state_dict_pre_hook <p>Register a pre-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method.</p> <p>It should have the following signature::     hook(module, prefix, keep_vars) -&gt; None  The registered hooks can be used to perform pre-processing before the <code>state_dict</code> call is made.</p> <p>Parameters</p> <ul> <li>hook </li> </ul> requires_grad_ <p>Change if autograd should record operations on parameters in this module.</p> <p>This method sets the parameters' :attr:<code>requires_grad</code> attributes in-place.  This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).  See :ref:<code>locally-disable-grad-doc</code> for a comparison between <code>.requires_grad_()</code> and several similar mechanisms that may be confused with it.  Args:     requires_grad (bool): whether autograd should record operations on                           parameters in this module. Default: <code>True</code>.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>requires_grad     (bool)     \u2013 defaults to <code>True</code> </li> </ul> save <p>Save the module to disk. This method should be overridden by subclasses to implement the specific behavior of the module.</p> <p>Args:     output_path (str): The path to the directory where the module should be saved.     args: Additional arguments that can be used to pass additional information to the save method.     safe_serialization (bool, optional): Whether to use the safetensors format for saving the model weights.         Defaults to True.     *kwargs: Additional keyword arguments that can be used to pass additional information to the save method.</p> <p>Parameters</p> <ul> <li>output_path     ('str')    </li> <li>args </li> <li>safe_serialization     ('bool')     \u2013 defaults to <code>True</code> </li> <li>kwargs </li> </ul> save_config <p>Save the configuration of the module to a JSON file.</p> <p>Args:     output_path (str): The path to the directory where the configuration file should be saved.     filename (str | None, optional): The name of the configuration file. If None, uses the default configuration         file name defined in the <code>config_file_name</code> class variable. Defaults to None.  Returns:     None</p> <p>Parameters</p> <ul> <li>output_path     ('str')    </li> <li>filename     ('str | None')     \u2013 defaults to <code>None</code> </li> </ul> save_torch_weights <p>Save the PyTorch weights of the module to disk.</p> <p>Args:     output_path (str): The path to the directory where the weights should be saved.     safe_serialization (bool, optional): Whether to use the safetensors format for saving the model weights.         Defaults to True.  Returns:     None</p> <p>Parameters</p> <ul> <li>output_path     ('str')    </li> <li>safe_serialization     ('bool')     \u2013 defaults to <code>True</code> </li> </ul> set_extra_state <p>Set extra state contained in the loaded <code>state_dict</code>.</p> <p>This function is called from :func:<code>load_state_dict</code> to handle any extra state found within the <code>state_dict</code>. Implement this function and a corresponding :func:<code>get_extra_state</code> for your module if you need to store extra state within its <code>state_dict</code>.  Args:     state (dict): Extra state from the <code>state_dict</code></p> <p>Parameters</p> <ul> <li>state     (Any)    </li> </ul> set_submodule <p>Set the submodule given by <code>target</code> if it exists, otherwise throw an error.</p> <p>.. note::     If <code>strict</code> is set to <code>False</code> (default), the method will replace an existing submodule     or create a new submodule if the parent module exists. If <code>strict</code> is set to <code>True</code>,     the method will only attempt to replace an existing submodule and throw an error if     the submodule does not exist.  For example, let's say you have an <code>nn.Module</code> <code>A</code> that looks like this:  .. code-block:: text      A(         (net_b): Module(             (net_c): Module(                 (conv): Conv2d(3, 3, 3)             )             (linear): Linear(3, 3)         )     )  (The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)  To override the <code>Conv2d</code> with a new submodule <code>Linear</code>, you could call <code>set_submodule(\"net_b.net_c.conv\", nn.Linear(1, 1))</code> where <code>strict</code> could be <code>True</code> or <code>False</code>  To add a new submodule <code>Conv2d</code> to the existing <code>net_b</code> module, you would call <code>set_submodule(\"net_b.conv\", nn.Conv2d(1, 1, 1))</code>.  In the above if you set <code>strict=True</code> and call <code>set_submodule(\"net_b.conv\", nn.Conv2d(1, 1, 1), strict=True)</code>, an AttributeError will be raised because <code>net_b</code> does not have a submodule named <code>conv</code>.  Args:     target: The fully-qualified string name of the submodule         to look for. (See above example for how to specify a         fully-qualified string.)     module: The module to set the submodule to.     strict: If <code>False</code>, the method will replace an existing submodule         or create a new submodule if the parent module exists. If <code>True</code>,         the method will only attempt to replace an existing submodule and throw an error         if the submodule doesn't already exist.  Raises:     ValueError: If the <code>target</code> string is empty or if <code>module</code> is not an instance of <code>nn.Module</code>.     AttributeError: If at any point along the path resulting from         the <code>target</code> string the (sub)path resolves to a non-existent         attribute name or an object that is not an instance of <code>nn.Module</code>.</p> <p>Parameters</p> <ul> <li>target     (str)    </li> <li>module     ('Module')    </li> <li>strict     (bool)     \u2013 defaults to <code>False</code> </li> </ul> share_memory <p>See :meth:<code>torch.Tensor.share_memory_</code>.</p> state_dict <p>Return a dictionary containing references to the whole state of the module.</p> <p>Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to <code>None</code> are not included.  .. note::     The returned object is a shallow copy. It contains references     to the module's parameters and buffers.  .. warning::     Currently <code>state_dict()</code> also accepts positional arguments for     <code>destination</code>, <code>prefix</code> and <code>keep_vars</code> in order. However,     this is being deprecated and keyword arguments will be enforced in     future releases.  .. warning::     Please avoid the use of argument <code>destination</code> as it is not     designed for end-users.  Args:     destination (dict, optional): If provided, the state of module will         be updated into the dict and the same object is returned.         Otherwise, an <code>OrderedDict</code> will be created and returned.         Default: <code>None</code>.     prefix (str, optional): a prefix added to parameter and buffer         names to compose the keys in state_dict. Default: <code>''</code>.     keep_vars (bool, optional): by default the :class:<code>~torch.Tensor</code> s         returned in the state dict are detached from autograd. If it's         set to <code>True</code>, detaching will not be performed.         Default: <code>False</code>.  Returns:     dict:         a dictionary containing a whole state of the module  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; module.state_dict().keys()     ['bias', 'weight']</p> <p>Parameters</p> <ul> <li>args </li> <li>destination     \u2013 defaults to <code>None</code> </li> <li>prefix     \u2013 defaults to ``    </li> <li>keep_vars     \u2013 defaults to <code>False</code> </li> </ul> to <p>Move and/or cast the parameters and buffers.</p> <p>This can be called as  .. function:: to(device=None, dtype=None, non_blocking=False)    :noindex:  .. function:: to(dtype, non_blocking=False)    :noindex:  .. function:: to(tensor, non_blocking=False)    :noindex:  .. function:: to(memory_format=torch.channels_last)    :noindex:  Its signature is similar to :meth:<code>torch.Tensor.to</code>, but only accepts floating point or complex :attr:<code>dtype</code>\\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr:<code>dtype</code> (if given). The integral parameters and buffers will be moved :attr:<code>device</code>, if that is given, but with dtypes unchanged. When :attr:<code>non_blocking</code> is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices.  See below for examples.  .. note::     This method modifies the module in-place.  Args:     device (:class:<code>torch.device</code>): the desired device of the parameters         and buffers in this module     dtype (:class:<code>torch.dtype</code>): the desired floating point or complex dtype of         the parameters and buffers in this module     tensor (torch.Tensor): Tensor whose dtype and device are the desired         dtype and device for all parameters and buffers in this module     memory_format (:class:<code>torch.memory_format</code>): the desired memory         format for 4D parameters and buffers in this module (keyword         only argument)  Returns:     Module: self  Examples::      &gt;&gt;&gt; # xdoctest: +IGNORE_WANT(\"non-deterministic\")     &gt;&gt;&gt; linear = nn.Linear(2, 2)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1913, -0.3420],             [-0.5113, -0.2325]])     &gt;&gt;&gt; linear.to(torch.double)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1913, -0.3420],             [-0.5113, -0.2325]], dtype=torch.float64)     &gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)     &gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")     &gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1914, -0.3420],             [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')     &gt;&gt;&gt; cpu = torch.device(\"cpu\")     &gt;&gt;&gt; linear.to(cpu)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1914, -0.3420],             [-0.5112, -0.2324]], dtype=torch.float16)      &gt;&gt;&gt; linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.3741+0.j,  0.2382+0.j],             [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)     &gt;&gt;&gt; linear(torch.ones(3, 2, dtype=torch.cdouble))     tensor([[0.6122+0.j, 0.1150+0.j],             [0.6122+0.j, 0.1150+0.j],             [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)</p> <p>Parameters</p> <ul> <li>args </li> <li>kwargs </li> </ul> to_empty <p>Move the parameters and buffers to the specified device without copying storage.</p> <p>Args:     device (:class:<code>torch.device</code>): The desired device of the parameters         and buffers in this module.     recurse (bool): Whether parameters and buffers of submodules should         be recursively moved to the specified device.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, str, torch.device, NoneType])    </li> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> </ul> train <p>Set the module in training mode.</p> <p>This has an effect only on certain modules. See the documentation of particular modules for details of their behaviors in training/evaluation mode, i.e., whether they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>, etc.  Args:     mode (bool): whether to set training mode (<code>True</code>) or evaluation                  mode (<code>False</code>). Default: <code>True</code>.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>mode     (bool)     \u2013 defaults to <code>True</code> </li> </ul> type <p>Casts all parameters and buffers to :attr:<code>dst_type</code>.</p> <p>.. note::     This method modifies the module in-place.  Args:     dst_type (type or string): the desired type  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>dst_type     (Union[torch.dtype, str])    </li> </ul> xpu <p>Move all model parameters and buffers to the XPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized.  .. note::     This method modifies the module in-place.  Arguments:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[torch.device, int, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> zero_grad <p>Reset gradients of all model parameters.</p> <p>See similar function under :class:<code>torch.optim.Optimizer</code> for more context.  Args:     set_to_none (bool): instead of setting to zero, set the grads to None.         See :meth:<code>torch.optim.Optimizer.zero_grad</code> for details.</p> <p>Parameters</p> <ul> <li>set_to_none     (bool)     \u2013 defaults to <code>True</code> </li> </ul>"},{"location":"api/rank/RerankResult/","title":"RerankResult","text":"<p>Rerank result for ranking.</p>"},{"location":"api/rank/RerankResult/#methods","title":"Methods","text":"clear <p>D.clear() -&gt; None.  Remove all items from D.</p> copy <p>D.copy() -&gt; a shallow copy of D</p> fromkeys <p>Create a new dictionary with keys from iterable and values set to value.</p> <p>Parameters</p> <ul> <li>iterable </li> <li>value     \u2013 defaults to <code>None</code> </li> </ul> get <p>Return the value for key if key is in the dictionary, else default.</p> <p>Parameters</p> <ul> <li>key </li> <li>default     \u2013 defaults to <code>None</code> </li> </ul> items <p>D.items() -&gt; a set-like object providing a view on D's items</p> keys <p>D.keys() -&gt; a set-like object providing a view on D's keys</p> pop <p>D.pop(k[,d]) -&gt; v, remove specified key and return the corresponding value.</p> <p>If the key is not found, return the default if given; otherwise, raise a KeyError.</p> popitem <p>Remove and return a (key, value) pair as a 2-tuple.</p> <p>Pairs are returned in LIFO (last-in, first-out) order. Raises KeyError if the dict is empty.</p> setdefault <p>Insert key with a value of default if key is not in the dictionary.</p> <p>Return the value for key if key is in the dictionary, else default.</p> <p>Parameters</p> <ul> <li>key </li> <li>default     \u2013 defaults to <code>None</code> </li> </ul> update <p>D.update([E, ]**F) -&gt; None.  Update D from dict/iterable E and F. If E is present and has a .keys() method, then does:  for k in E: D[k] = E[k] If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v In either case, this is followed by: for k in F:  D[k] = F[k]</p> values <p>D.values() -&gt; an object providing a view on D's values</p>"},{"location":"api/rank/rerank/","title":"rerank","text":"<p>Rerank the documents based on the queries embeddings.</p>"},{"location":"api/rank/rerank/#parameters","title":"Parameters","text":"<ul> <li> <p>documents_ids ('list[list[int | str]]')</p> <p>The documents ids.</p> </li> <li> <p>queries_embeddings ('list[list[float | int] | np.ndarray | torch.Tensor]')</p> <p>The queries embeddings which is a dictionary of queries and their embeddings.</p> </li> <li> <p>documents_embeddings ('list[list[float | int] | np.ndarray | torch.Tensor]')</p> <p>The documents embeddings which is a dictionary of documents ids and their embeddings.</p> </li> <li> <p>device ('str') \u2013 defaults to <code>None</code></p> <p>The device to use for the reranking. If None, the device of the queries embeddings will be used.</p> </li> </ul>"},{"location":"api/rank/rerank/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from pylate import models, rank\n\n&gt;&gt;&gt; model = models.ColBERT(\n...     model_name_or_path=\"sentence-transformers/all-MiniLM-L6-v2\", device=\"cpu\"\n... )\n\n&gt;&gt;&gt; queries = [\n...     \"query A\",\n...     \"query B\",\n... ]\n\n&gt;&gt;&gt; documents = [\n...     [\"document A\", \"document B\"],\n...     [\"document 1\", \"document C\", \"document B\"],\n... ]\n\n&gt;&gt;&gt; documents_ids = [\n...    [1, 2],\n...    [1, 3, 2],\n... ]\n\n&gt;&gt;&gt; queries_embeddings = model.encode(\n...     queries,\n...     is_query=True,\n...     batch_size=1,\n... )\n\n&gt;&gt;&gt; documents_embeddings = model.encode(\n...     documents,\n...     is_query=False,\n...     batch_size=1,\n... )\n\n&gt;&gt;&gt; reranked_documents = rank.rerank(\n...     documents_ids=documents_ids,\n...     queries_embeddings=queries_embeddings,\n...     documents_embeddings=documents_embeddings,\n... )\n\n&gt;&gt;&gt; assert isinstance(reranked_documents, list)\n&gt;&gt;&gt; assert len(reranked_documents) == 2\n&gt;&gt;&gt; assert len(reranked_documents[0]) == 2\n&gt;&gt;&gt; assert len(reranked_documents[1]) == 3\n&gt;&gt;&gt; assert isinstance(reranked_documents[0], list)\n&gt;&gt;&gt; assert isinstance(reranked_documents[0][0], dict)\n&gt;&gt;&gt; assert \"id\" in reranked_documents[0][0]\n&gt;&gt;&gt; assert \"score\" in reranked_documents[0][0]\n</code></pre>"},{"location":"api/retrieve/ColBERT/","title":"ColBERT","text":"<p>ColBERT retriever.</p>"},{"location":"api/retrieve/ColBERT/#parameters","title":"Parameters","text":"<ul> <li>index ('Voyager | PLAID')</li> </ul>"},{"location":"api/retrieve/ColBERT/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from pylate import indexes, models, retrieve\n\n&gt;&gt;&gt; model = models.ColBERT(\n...     model_name_or_path=\"sentence-transformers/all-MiniLM-L6-v2\",\n...     device=\"cpu\",\n... )\n\n&gt;&gt;&gt; documents_ids = [f\"document_id_{i}\" for i in range(20)]\n&gt;&gt;&gt; documents = [f\"This is the content of document {i}.\" for i in range(20)]\n\n&gt;&gt;&gt; documents_embeddings = model.encode(\n...     sentences=documents,\n...     batch_size=1,\n...     is_query=False,\n... )\n\n&gt;&gt;&gt; index = indexes.PLAID(\n...     index_folder=\"test_indexes\",\n...     index_name=\"colbert\",\n...     override=True,\n... )\n\n&gt;&gt;&gt; index = index.add_documents(\n...     documents_ids=documents_ids,\n...     documents_embeddings=documents_embeddings,\n... )\nComputing centroids of embeddings.\nCreating FastPlaid index.\n\n&gt;&gt;&gt; retriever = retrieve.ColBERT(index=index)\n\n&gt;&gt;&gt; queries_embeddings = model.encode(\n...     [\"fruits are healthy.\", \"fruits are good for health.\"],\n...     batch_size=1,\n...     is_query=True,\n... )\n\n&gt;&gt;&gt; results = retriever.retrieve(\n...     queries_embeddings=queries_embeddings,\n...     k=2,\n...     device=\"cpu\",\n... )\n\n&gt;&gt;&gt; assert isinstance(results, list)\n&gt;&gt;&gt; assert len(results) == 2\n\n&gt;&gt;&gt; queries_embeddings = model.encode(\n...     \"fruits are healthy.\",\n...     batch_size=1,\n...     is_query=True,\n... )\n\n&gt;&gt;&gt; results = retriever.retrieve(\n...     queries_embeddings=queries_embeddings,\n...     k=2,\n...     device=\"cpu\",\n... )\n\n&gt;&gt;&gt; assert isinstance(results, list)\n&gt;&gt;&gt; assert len(results) == 1\n\n&gt;&gt;&gt; results = retriever.retrieve(\n...     queries_embeddings=queries_embeddings,\n...     k=2,\n...     device=\"cpu\",\n...     subset=[\"document_id_10\"],\n... )\n</code></pre>"},{"location":"api/retrieve/ColBERT/#methods","title":"Methods","text":"retrieve <p>Retrieve documents for a list of queries.</p> <p>Parameters</p> <ul> <li>queries_embeddings     ('list[list | np.ndarray | torch.Tensor]')    </li> <li>k     ('int')     \u2013 defaults to <code>10</code> </li> <li>k_token     ('int')     \u2013 defaults to <code>100</code> </li> <li>device     ('str | None')     \u2013 defaults to <code>None</code> </li> <li>batch_size     ('int')     \u2013 defaults to <code>50</code> </li> <li>subset     ('list[list[str]] | list[str] | None')     \u2013 defaults to <code>None</code> </li> </ul>"},{"location":"api/scores/SimilarityFunction/","title":"SimilarityFunction","text":"<p>Enum class for supported score functions. The following functions are supported: - <code>SimilarityFunction.MAXSIM</code> (<code>\"MaxSim\"</code>): Max similarity</p>"},{"location":"api/scores/SimilarityFunction/#parameters","title":"Parameters","text":"<ul> <li> <p>value</p> </li> <li> <p>names \u2013 defaults to <code>None</code></p> </li> <li> <p>module \u2013 defaults to <code>None</code></p> </li> <li> <p>qualname \u2013 defaults to <code>None</code></p> </li> <li> <p>type \u2013 defaults to <code>None</code></p> </li> <li> <p>start \u2013 defaults to <code>1</code></p> </li> </ul>"},{"location":"api/scores/colbert-kd-scores/","title":"colbert_kd_scores","text":"<p>Computes the ColBERT scores between queries and documents embeddings. This scoring function is dedicated to the knowledge distillation pipeline.</p>"},{"location":"api/scores/colbert-kd-scores/#parameters","title":"Parameters","text":"<ul> <li> <p>queries_embeddings ('list | np.ndarray | torch.Tensor')</p> </li> <li> <p>documents_embeddings ('list | np.ndarray | torch.Tensor')</p> </li> <li> <p>queries_mask ('torch.Tensor') \u2013 defaults to <code>None</code></p> </li> <li> <p>documents_mask ('torch.Tensor') \u2013 defaults to <code>None</code></p> </li> </ul>"},{"location":"api/scores/colbert-kd-scores/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; import torch\n\n&gt;&gt;&gt; queries_embeddings = torch.tensor([\n...     [[1.], [0.], [0.], [0.]],\n...     [[0.], [2.], [0.], [0.]],\n...     [[0.], [0.], [3.], [0.]],\n... ])\n\n&gt;&gt;&gt; documents_embeddings = torch.tensor([\n...     [[[10.], [0.], [1.]], [[20.], [0.], [1.]], [[30.], [0.], [1.]]],\n...     [[[0.], [100.], [1.]], [[0.], [200.], [1.]], [[0.], [300.], [1.]]],\n...     [[[1.], [0.], [1000.]], [[1.], [0.], [2000.]], [[10.], [0.], [3000.]]],\n... ])\n&gt;&gt;&gt; documents_mask = torch.tensor([\n...     [[0., 1., 1.], [1., 1., 1.], [1., 1., 1.]],\n...     [[1., 1., 1.], [1., 1., 1.], [1., 1., 1.]],\n...     [[1., 1., 1.], [1., 1., 1.], [1., 1., 1.]],\n... ])\n&gt;&gt;&gt; query_mask = torch.tensor([\n...     [1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 0., 1.]\n... ])\n&gt;&gt;&gt; colbert_kd_scores(\n...     queries_embeddings=queries_embeddings,\n...     documents_embeddings=documents_embeddings,\n...     queries_mask=query_mask,\n...     documents_mask=documents_mask,\n... )\ntensor([[ 1.,  20.,  30.],\n        [200., 400., 600.],\n        [  0.,   0.,   0.]])\n</code></pre>"},{"location":"api/scores/colbert-scores-pairwise/","title":"colbert_scores_pairwise","text":"<p>Computes the ColBERT score for each query-document pair. The score is computed as the sum of maximum similarities between the query and the document for corresponding pairs.</p>"},{"location":"api/scores/colbert-scores-pairwise/#parameters","title":"Parameters","text":"<ul> <li> <p>queries_embeddings ('torch.Tensor')</p> <p>The first tensor. The queries embeddings. Shape: (batch_size, num tokens queries, embedding_size)</p> </li> <li> <p>documents_embeddings ('torch.Tensor')</p> <p>The second tensor. The documents embeddings. Shape: (batch_size, num tokens documents, embedding_size)</p> </li> </ul>"},{"location":"api/scores/colbert-scores-pairwise/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; import torch\n\n&gt;&gt;&gt; queries_embeddings = torch.tensor([\n...     [[1.], [0.], [0.], [0.]],\n...     [[0.], [2.], [0.], [0.]],\n...     [[0.], [0.], [3.], [0.]],\n... ])\n\n&gt;&gt;&gt; documents_embeddings = torch.tensor([\n...     [[10.], [0.], [1.]],\n...     [[0.], [100.], [1.]],\n...     [[1.], [0.], [1000.]],\n... ])\n\n&gt;&gt;&gt; scores = colbert_scores_pairwise(\n...     queries_embeddings=queries_embeddings,\n...     documents_embeddings=documents_embeddings\n... )\n\n&gt;&gt;&gt; scores\ntensor([  10.,  200., 3000.])\n</code></pre>"},{"location":"api/scores/colbert-scores/","title":"colbert_scores","text":"<p>Computes the ColBERT scores between queries and documents embeddings. The score is computed as the sum of maximum similarities between the query and the document.</p>"},{"location":"api/scores/colbert-scores/#parameters","title":"Parameters","text":"<ul> <li> <p>queries_embeddings ('list | np.ndarray | torch.Tensor')</p> <p>The first tensor. The queries embeddings. Shape: (batch_size, num tokens queries, embedding_size)</p> </li> <li> <p>documents_embeddings ('list | np.ndarray | torch.Tensor')</p> <p>The second tensor. The documents embeddings. Shape: (batch_size, num tokens documents, embedding_size)</p> </li> <li> <p>queries_mask ('torch.Tensor | None') \u2013 defaults to <code>None</code></p> <p>The mask for the queries embeddings. Shape: (batch_size, num tokens queries)</p> </li> <li> <p>documents_mask ('torch.Tensor | None') \u2013 defaults to <code>None</code></p> <p>The mask for the documents embeddings. Shape: (batch_size, num tokens documents)</p> </li> </ul>"},{"location":"api/scores/colbert-scores/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; import torch\n\n&gt;&gt;&gt; queries_embeddings = torch.tensor([\n...     [[1.], [0.], [0.], [0.]],\n...     [[0.], [2.], [0.], [0.]],\n...     [[0.], [0.], [3.], [0.]],\n... ])\n\n&gt;&gt;&gt; documents_embeddings = torch.tensor([\n...     [[10.], [0.], [1.]],\n...     [[0.], [100.], [10.]],\n...     [[1.], [0.], [1000.]],\n... ])\n\n&gt;&gt;&gt; documents_mask = torch.tensor([\n...     [1., 1., 1.],\n...     [1., 0., 1.],\n...     [1., 1., 1.],\n... ])\n&gt;&gt;&gt; query_mask = torch.tensor([\n...     [1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 0., 1.]\n... ])\n\n&gt;&gt;&gt; scores = colbert_scores(\n...     queries_embeddings=queries_embeddings,\n...     documents_embeddings=documents_embeddings,\n...     queries_mask=query_mask,\n...     documents_mask=documents_mask,\n... )\n\n&gt;&gt;&gt; scores\ntensor([[  10.,  10., 1000.],\n        [  20.,  20., 2000.],\n        [  0.,  0., 0.]])\n</code></pre>"},{"location":"api/utils/ColBERTCollator/","title":"ColBERTCollator","text":"<p>Collator for ColBERT model.</p>"},{"location":"api/utils/ColBERTCollator/#parameters","title":"Parameters","text":"<ul> <li> <p>tokenize_fn ('Callable')</p> <p>The function to tokenize the input text.</p> </li> <li> <p>valid_label_columns ('list[str] | None') \u2013 defaults to <code>None</code></p> <p>The name of the columns that contain the labels: scores or labels.</p> </li> <li> <p>router_mapping ('dict[str, str] | dict[str, dict[str, str]] | None') \u2013 defaults to <code>None</code></p> <p>The mapping of the columns to the router.</p> </li> <li> <p>prompts ('dict[str, str] | dict[str, dict[str, str]] | None') \u2013 defaults to <code>None</code></p> <p>The prompts to use for the columns.</p> </li> <li> <p>include_prompt_lengths ('bool') \u2013 defaults to <code>False</code></p> <p>Whether to include the prompt lengths in the batch.</p> </li> <li> <p>all_special_ids ('set[int] | None') \u2013 defaults to <code>None</code></p> <p>The special ids to use for the tokenization.</p> </li> <li> <p>_prompt_length_mapping ('dict[str, int] | None') \u2013 defaults to <code>None</code></p> </li> <li> <p>_warned_columns ('set[tuple[str]] | None') \u2013 defaults to <code>None</code></p> </li> </ul>"},{"location":"api/utils/ColBERTCollator/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from pylate import models, utils\n\n&gt;&gt;&gt; model = models.ColBERT(\n...     model_name_or_path=\"sentence-transformers/all-MiniLM-L6-v2\", device=\"cpu\"\n... )\n\n&gt;&gt;&gt; collator = utils.ColBERTCollator(\n...     tokenize_fn=model.tokenize,\n... )\n\n&gt;&gt;&gt; features = [\n...     {\n...         \"query\": \"fruits are healthy.\",\n...         \"positive\": \"fruits are good for health.\",\n...         \"negative\": \"fruits are bad for health.\",\n...         \"label\": [0.7, 0.3]\n...     }\n... ]\n\n&gt;&gt;&gt; features = collator(features=features)\n\n&gt;&gt;&gt; fields = [\n...     \"query_input_ids\",\n...     \"positive_input_ids\",\n...     \"negative_input_ids\",\n...     \"query_attention_mask\",\n...     \"positive_attention_mask\",\n...     \"negative_attention_mask\",\n...     \"query_token_type_ids\",\n...     \"positive_token_type_ids\",\n...     \"negative_token_type_ids\",\n... ]\n\n&gt;&gt;&gt; for field in fields:\n...     assert field in features\n...     assert isinstance(features[field], torch.Tensor)\n...     assert features[field].ndim == 2\n</code></pre>"},{"location":"api/utils/ColBERTCollator/#methods","title":"Methods","text":"call <p>Collate a list of features into a batch.</p> <p>Parameters</p> <ul> <li>features     ('list[dict]')    </li> </ul> maybe_warn_about_column_order <p>Warn the user if the columns are likely not in the expected order.</p> <p>Parameters</p> <ul> <li>column_names     ('list[str]')    </li> </ul>"},{"location":"api/utils/KDProcessing/","title":"KDProcessing","text":"<p>Dataset processing class for knowledge distillation training.</p>"},{"location":"api/utils/KDProcessing/#parameters","title":"Parameters","text":"<ul> <li> <p>queries ('datasets.Dataset | datasets.DatasetDict')</p> <p>Queries dataset.</p> </li> <li> <p>documents ('datasets.Dataset | datasets.DatasetDict')</p> <p>Documents dataset.</p> </li> <li> <p>split ('str') \u2013 defaults to <code>train</code></p> <p>Split to use for the queries and documents datasets. Used only if the queries and documents are of type <code>datasets.DatasetDict</code>.</p> </li> <li> <p>n_ways ('int') \u2013 defaults to <code>32</code></p> <p>Number of scores to keep for the distillation.</p> </li> </ul>"},{"location":"api/utils/KDProcessing/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from datasets import load_dataset\n&gt;&gt;&gt; from pylate import utils\n\n&gt;&gt;&gt; train = load_dataset(\n...    path=\"lightonai/lighton-ms-marco-mini\",\n...    name=\"train\",\n...    split=\"train\",\n... )\n\n&gt;&gt;&gt; queries = load_dataset(\n...    path=\"lightonai/lighton-ms-marco-mini\",\n...    name=\"queries\",\n...    split=\"train\",\n... )\n\n&gt;&gt;&gt; documents = load_dataset(\n...    path=\"lightonai/lighton-ms-marco-mini\",\n...    name=\"documents\",\n...    split=\"train\",\n... )\n\n&gt;&gt;&gt; train.set_transform(\n...    utils.KDProcessing(\n...        queries=queries, documents=documents\n...    ).transform,\n... )\n\n&gt;&gt;&gt; for sample in train:\n...     assert \"documents\" in sample and isinstance(sample[\"documents\"], list)\n...     assert \"query\" in sample and isinstance(sample[\"query\"], str)\n...     assert \"scores\" in sample and isinstance(sample[\"scores\"], list)\n</code></pre>"},{"location":"api/utils/KDProcessing/#methods","title":"Methods","text":"map <p>Process a single example.</p> <p>Parameters</p> <ul> <li>example     ('dict')    </li> </ul> transform <p>Update the input dataset with the queries and documents.</p> <p>Parameters</p> <ul> <li>examples     ('dict')    </li> </ul>"},{"location":"api/utils/all-gather-with-gradients/","title":"all_gather_with_gradients","text":"<p>Gathers a tensor from each distributed rank into a list. All the tensors will retain gradients. This is the same as <code>all_gather</code>, but all the tensors will retain gradients and is used to compute contrastive with local queries only to lower the memory usage, see https://github.com/mlfoundations/open_clip/issues/616</p> <ul> <li> <p>If torch.distributed is available and initialized, gather all the tensors (with gradients) from each rank into a list </p> </li> <li> <p>If torch.distributed is either unavailable, uninitialized, or   <code>world_size == 1</code>, it returns a list containing only the   original tensor and throws a warning to notify the user (helpful when using a single GPU setup).</p> </li> </ul>"},{"location":"api/utils/all-gather-with-gradients/#parameters","title":"Parameters","text":"<ul> <li>tensor ('torch.Tensor')</li> </ul>"},{"location":"api/utils/all-gather/","title":"all_gather","text":"<p>Gathers a tensor from each distributed rank into a list. The tensor for the local rank is the original one, with the gradients while the others have no gradients.</p> <ul> <li> <p>If torch.distributed is available and initialized:   1. Creates a list of tensors (each sized like the input <code>tensor</code>).   2. Gathers tensors from each rank into that list.   3. Replaces the local tensor in the list with the original tensor that retains gradients. </p> </li> <li> <p>If torch.distributed is either unavailable, uninitialized, or   <code>world_size == 1</code>, it returns a list containing only the   original tensor and throws a warning to notify the user (helpful when using a single GPU setup).</p> </li> </ul>"},{"location":"api/utils/all-gather/#parameters","title":"Parameters","text":"<ul> <li>tensor ('torch.Tensor')</li> </ul>"},{"location":"api/utils/convert-to-tensor/","title":"convert_to_tensor","text":"<p>Converts a list or numpy array to a torch tensor.</p>"},{"location":"api/utils/convert-to-tensor/#parameters","title":"Parameters","text":"<ul> <li> <p>x ('torch.Tensor | np.ndarray | list[torch.Tensor | np.ndarray | list | float]')</p> <p>The input data. It can be a torch tensor, a numpy array, or a list of torch tensors, numpy arrays, or lists.</p> </li> </ul>"},{"location":"api/utils/convert-to-tensor/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import torch\n\n&gt;&gt;&gt; x = torch.tensor([[1., 1., 1.], [2., 2., 2.]])\n&gt;&gt;&gt; convert_to_tensor(x)\ntensor([[1., 1., 1.],\n        [2., 2., 2.]])\n\n&gt;&gt;&gt; x = np.array([[1., 1., 1.], [2., 2., 2.]], dtype=np.float32)\n&gt;&gt;&gt; convert_to_tensor(x)\ntensor([[1., 1., 1.],\n        [2., 2., 2.]])\n\n&gt;&gt;&gt; x = []\n&gt;&gt;&gt; convert_to_tensor(x)\ntensor([])\n\n&gt;&gt;&gt; x = [np.array([1., 1., 1.])]\n&gt;&gt;&gt; convert_to_tensor(x)\ntensor([[1., 1., 1.]])\n\n&gt;&gt;&gt; x = [[1., 1., 1.]]\n&gt;&gt;&gt; convert_to_tensor(x)\ntensor([[1., 1., 1.]])\n\n&gt;&gt;&gt; x = [torch.tensor([1., 1., 1.]), torch.tensor([2., 2., 2.])]\n&gt;&gt;&gt; convert_to_tensor(x)\ntensor([[1., 1., 1.],\n        [2., 2., 2.]])\n\n&gt;&gt;&gt; x = np.array([], dtype=np.float32)\n&gt;&gt;&gt; convert_to_tensor(x)\ntensor([])\n</code></pre>"},{"location":"api/utils/get-rank/","title":"get_rank","text":"<p>Returns the current rank in a distributed training.</p>"},{"location":"api/utils/get-world-size/","title":"get_world_size","text":"<p>Returns the world size in a distributed training.</p>"},{"location":"api/utils/iter-batch/","title":"iter_batch","text":"<p>Iterate over a list of elements by batch.</p>"},{"location":"api/utils/iter-batch/#parameters","title":"Parameters","text":"<ul> <li> <p>X ('list[str]')</p> </li> <li> <p>batch_size ('int')</p> </li> <li> <p>tqdm_bar ('bool') \u2013 defaults to <code>True</code></p> </li> <li> <p>desc ('str') \u2013 defaults to ``</p> </li> </ul>"},{"location":"api/utils/iter-batch/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from pylate import utils\n\n&gt;&gt;&gt; X = [\n...  \"element 0\",\n...  \"element 1\",\n...  \"element 2\",\n...  \"element 3\",\n...  \"element 4\",\n... ]\n\n&gt;&gt;&gt; n_samples = 0\n&gt;&gt;&gt; for batch in utils.iter_batch(X, batch_size=2):\n...     n_samples += len(batch)\n\n&gt;&gt;&gt; n_samples\n5\n</code></pre>"},{"location":"documentation/datasets/","title":"Datasets","text":"<p>PyLate is designed to be compatible with Hugging Face datasets, enabling seamless integration for tasks like knowledge distillation and contrastive model training. Below are examples of how to load and prepare datasets for these specific training objectives.</p>"},{"location":"documentation/datasets/#contrastive-dataset","title":"Contrastive Dataset","text":"<p>Contrastive training requires datasets that include a query, a positive document (relevant to the query), and a negative document (irrelevant to the query). This is the standard triplet format used by Sentence Transformers, making PyLate's contrastive training compatible with all existing triplet datasets.</p>"},{"location":"documentation/datasets/#loading-a-pre-built-contrastive-dataset","title":"Loading a pre-built contrastive dataset","text":"<p>You can directly download an existing contrastive dataset from Hugging Face's hub, such as the msmarco-bm25 triplet dataset.</p> <pre><code>from datasets import load_dataset\n\ndataset = load_dataset(\"sentence-transformers/msmarco-bm25\", \"triplet\", split=\"train\")\n\ntrain_dataset, test_dataset = dataset.train_test_split(test_size=0.001)\n</code></pre> <p>Then we can shuffle the dataset:</p> <pre><code>train_dataset = train_dataset.shuffle(seed=42)\n</code></pre> <p>And select a subset of the dataset if needed:</p> <pre><code>train_dataset = train_dataset.select(range(10_000))\n</code></pre>"},{"location":"documentation/datasets/#creating-a-contrastive-dataset-from-list","title":"Creating a contrastive dataset from list","text":"<p>If you want to create a custom contrastive dataset, you can do so by manually specifying the query, positive, and negative samples.</p> <pre><code>from datasets import Dataset\n\ndataset = [\n    {\n        \"query\": \"example query 1\",\n        \"positive\": \"example positive document 1\",\n        \"negative\": \"example negative document 1\",\n    },\n    {\n        \"query\": \"example query 2\",\n        \"positive\": \"example positive document 2\",\n        \"negative\": \"example negative document 2\",\n    },\n    {\n        \"query\": \"example query 3\",\n        \"positive\": \"example positive document 3\",\n        \"negative\": \"example negative document 3\",\n    },\n]\n\ndataset = Dataset.from_list(mapping=dataset)\n\ntrain_dataset, test_dataset = dataset.train_test_split(test_size=0.3)\n</code></pre> Tip <p>Note that PyLate supports more than one negative per query, simply add the additional negatives after the first one in the row. <pre><code>{\n        \"query\": \"example query 1\",\n        \"positive\": \"example positive document 1\",\n        \"negative_1\": \"example negative document 1\",\n        \"negative_2\": \"example negative document 2\",\n}\n</code></pre></p>"},{"location":"documentation/datasets/#loading-a-contrastive-dataset-from-a-local-parquet-file","title":"Loading a contrastive dataset from a local parquet file","text":"<p>To load a local dataset stored in a Parquet file:</p> <pre><code>from datasets import load_dataset\n\ndataset = load_dataset(\n    path=\"parquet\",\n    data_files=\"dataset.parquet\",\n    split=\"train\"\n)\n\ntrain_dataset, test_dataset = dataset.train_test_split(test_size=0.001)\n</code></pre>"},{"location":"documentation/datasets/#knowledge-distillation-dataset","title":"Knowledge distillation dataset","text":"<p>For fine-tuning a model using knowledge distillation loss, three distinct dataset files are required: train, queries, and documents.</p> Info <p>Each file contains unique and complementary information necessary for the distillation process:</p> <ul> <li><code>train</code>: Contains three columns: <code>['query_id', 'document_ids', 'scores']</code><ul> <li><code>query_id</code> refers to the query identifier.</li> <li><code>document_ids</code> is a list of document IDs relevant to the query.</li> <li><code>scores</code> corresponds to the relevance scores between the query and each document.</li> </ul> </li> </ul>"},{"location":"documentation/datasets/#train","title":"Train","text":"<p>Example entry:</p> <pre><code>{\n    \"query_id\": 54528,\n    \"document_ids\": [\n        6862419,\n        335116,\n        339186,\n        7509316,\n        7361291,\n        7416534,\n        5789936,\n        5645247,\n    ],\n    \"scores\": [\n        0.4546215673141326,\n        0.6575686537173476,\n        0.26825184192900203,\n        0.5256195579370395,\n        0.879939718687207,\n        0.7894968184862693,\n        0.6450100468854655,\n        0.5823844608171467,\n    ],\n}\n</code></pre> Warning <p>Ensure that the length of <code>document_ids</code> matches the length of <code>scores</code>.</p>"},{"location":"documentation/datasets/#queries","title":"Queries","text":"<ul> <li><code>queries</code>: Contains two columns: <code>['query_id', 'text']</code></li> </ul> <p>Example entry:</p> <pre><code>{\"query_id\": 749480, \"text\": \"example query 1\"}\n</code></pre>"},{"location":"documentation/datasets/#documents","title":"Documents","text":"<ul> <li><code>documents</code>: contains two columns: <code>['document_ids', 'text']</code></li> </ul> <p>Example entry:</p> <pre><code>{\n    \"document_id\": 136062,\n    \"text\": \"example document 1\",\n}\n</code></pre>"},{"location":"documentation/datasets/#loading-a-pre-built-knowledge-distillation-dataset","title":"Loading a pre-built knowledge distillation dataset","text":"<p>You can directly download an existing knowledge distillation dataset from Hugging Face's hub, such as the English MS MARCO dataset with BGE M3 scores or the French version. Simply load the different files by giving the respective names to the <code>load_dataset</code> function:</p> <pre><code>from datasets import load_dataset\n\ntrain = load_dataset(\n    \"lightonai/ms-marco-en-bge\",\n    \"train\",\n    split=\"train\",\n)\n\nqueries = load_dataset(\n    \"lightonai/ms-marco-en-bge\",\n    \"queries\",\n    split=\"train\",\n)\n\ndocuments = load_dataset(\n    \"lightonai/ms-marco-en-bge\",\n    \"documents\",\n    split=\"train\",\n)\n</code></pre>"},{"location":"documentation/datasets/#knowledge-distillation-dataset-from-list","title":"Knowledge distillation dataset from list","text":"<p>You can also create custom datasets from list in Python. This example demonstrates how to build the <code>train</code>, <code>queries</code>, and <code>documents</code> datasets</p> <pre><code>from datasets import Dataset\n\ndataset = [\n    {\n        \"query_id\": 54528,\n        \"document_ids\": [\n            6862419,\n            335116,\n            339186,\n            7509316,\n            7361291,\n            7416534,\n            5789936,\n            5645247,\n        ],\n        \"scores\": [\n            0.4546215673141326,\n            0.6575686537173476,\n            0.26825184192900203,\n            0.5256195579370395,\n            0.879939718687207,\n            0.7894968184862693,\n            0.6450100468854655,\n            0.5823844608171467,\n        ],\n    },\n    {\n        \"query_id\": 749480,\n        \"document_ids\": [\n            6862419,\n            335116,\n            339186,\n            7509316,\n            7361291,\n            7416534,\n            5789936,\n            5645247,\n        ],\n        \"scores\": [\n            0.2546215673141326,\n            0.7575686537173476,\n            0.96825184192900203,\n            0.0256195579370395,\n            0.779939718687207,\n            0.2894968184862693,\n            0.1450100468854655,\n            0.7823844608171467,\n        ],\n    },\n]\n\n\ndataset = Dataset.from_list(mapping=dataset)\n\ndocuments = [\n    {\"document_id\": 6862419, \"text\": \"example document 1\"},\n    {\"document_id\": 335116, \"text\": \"example document 2\"},\n    {\"document_id\": 339186, \"text\": \"example document 3\"},\n    {\"document_id\": 7509316, \"text\": \"example document 4\"},\n    {\"document_id\": 7361291, \"text\": \"example document 5\"},\n    {\"document_id\": 7416534, \"text\": \"example document 6\"},\n    {\"document_id\": 5789936, \"text\": \"example document 7\"},\n    {\"document_id\": 5645247, \"text\": \"example document 8\"},\n]\n\nqueries = [\n    {\"query_id\": 749480, \"text\": \"example query 1\"},\n    {\"query_id\": 54528, \"text\": \"example query 2\"},\n]\n\ndocuments = Dataset.from_list(mapping=documents)\n\nqueries = Dataset.from_list(mapping=queries)\n</code></pre>"},{"location":"documentation/evaluation/","title":"Evaluation","text":""},{"location":"documentation/evaluation/#retrieval-evaluation","title":"Retrieval evaluation","text":"<p>This guide demonstrates an end-to-end pipeline to evaluate the performance of the ColBERT model on retrieval tasks. The pipeline involves three key steps: indexing documents, retrieving top-k documents for a given set of queries, and evaluating the retrieval results using standard metrics.</p>"},{"location":"documentation/evaluation/#beir-retrieval-evaluation-pipeline","title":"BEIR Retrieval Evaluation Pipeline","text":"<pre><code>from pylate import evaluation, indexes, models, retrieve\n\n# Step 1: Initialize the ColBERT model\n\ndataset = \"scifact\" # Choose the dataset you want to evaluate\nmodel = models.ColBERT(\n    model_name_or_path=\"lightonai/GTE-ModernColBERT-v1\",\n    device=\"cuda\" # \"cpu\" or \"cuda\" or \"mps\"\n)\n\n# Step 2: Create a Voyager index\nindex = indexes.Voyager(\n    index_folder=\"pylate-index\",\n    index_name=dataset,\n    override=True,  # Overwrite any existing index\n)\n\n# Step 3: Load the documents, queries, and relevance judgments (qrels)\ndocuments, queries, qrels = evaluation.load_beir(\n    dataset,  # Specify the dataset (e.g., \"scifact\")\n    split=\"test\",  # Specify the split (e.g., \"test\")\n)\n\n# Step 4: Encode the documents\ndocuments_embeddings = model.encode(\n    [document[\"text\"] for document in documents],\n    batch_size=32,\n    is_query=False,  # Indicate that these are documents\n    show_progress_bar=True,\n)\n\n# Step 5: Add document embeddings to the index\nindex.add_documents(\n    documents_ids=[document[\"id\"] for document in documents],\n    documents_embeddings=documents_embeddings,\n)\n\n# Step 6: Encode the queries\nqueries_embeddings = model.encode(\n    queries,\n    batch_size=32,\n    is_query=True,  # Indicate that these are queries\n    show_progress_bar=True,\n)\n\n# Step 7: Retrieve top-k documents\nretriever = retrieve.ColBERT(index=index)\nscores = retriever.retrieve(\n    queries_embeddings=queries_embeddings,\n    k=100,  # Retrieve the top 100 matches for each query\n)\n\n# Step 8: Evaluate the retrieval results\nresults = evaluation.evaluate(\n    scores=scores,\n    qrels=qrels,\n    queries=queries,\n    metrics=[f\"ndcg@{k}\" for k in [1, 3, 5, 10, 100]] # NDCG for different k values\n    + [f\"hits@{k}\" for k in [1, 3, 5, 10, 100]]       # Hits at different k values\n    + [\"map\"]                                         # Mean Average Precision (MAP)\n    + [\"recall@10\", \"recall@100\"]                     # Recall at k\n    + [\"precision@10\", \"precision@100\"],              # Precision at k\n)\n\nprint(results)\n</code></pre> <p>The output is a dictionary containing various evaluation metrics. Here\u2019s a sample output:</p> <pre><code>{\n    \"ndcg@1\": 0.47333333333333333,\n    \"ndcg@3\": 0.543862513095773,\n    \"ndcg@5\": 0.5623210323686343,\n    \"ndcg@10\": 0.5891793972249917,\n    \"ndcg@100\": 0.5891793972249917,\n    \"hits@1\": 0.47333333333333333,\n    \"hits@3\": 0.64,\n    \"hits@5\": 0.7033333333333334,\n    \"hits@10\": 0.8,\n    \"hits@100\": 0.8,\n    \"map\": 0.5442202380952381,\n    \"recall@10\": 0.7160555555555556,\n    \"recall@100\": 0.7160555555555556,\n    \"precision@10\": 0.08,\n    \"precision@100\": 0.008000000000000002,\n}\n</code></pre> Info <ol> <li> <p>is_query flag: Always set is_query=True when encoding queries and is_query=False when encoding documents. This ensures the model applies the correct prefixes for queries and documents.</p> </li> <li> <p>Evaluation metrics: The pipeline supports a wide range of evaluation metrics, including NDCG, hits, MAP, recall, and precision, with different cutoff points.</p> </li> <li> <p>Relevance judgments (qrels): The qrels are used to calculate how well the retrieved documents match the ground truth.</p> </li> </ol>"},{"location":"documentation/evaluation/#beir-datasets","title":"BEIR datasets","text":"<p>The following table lists the datasets available in the BEIR benchmark along with their names, types, number of queries, corpus size, and relevance degree per query. Source: BEIR Datasets</p> Table Dataset BEIR-Name Type Queries Corpus MSMARCO msmarco train, dev, test 6,980 8,840,000 TREC-COVID trec-covid test 50 171,000 NFCorpus nfcorpus train, dev, test 323 3,600 BioASQ bioasq train, test 500 14,910,000 NQ nq train, test 3,452 2,680,000 HotpotQA hotpotqa train, dev, test 7,405 5,230,000 FiQA-2018 fiqa train, dev, test 648 57,000 Signal-1M(RT) signal1m test 97 2,860,000 TREC-NEWS trec-news test 57 595,000 Robust04 robust04 test 249 528,000 ArguAna arguana test 1,406 8,670 Touche-2020 webis-touche2020 test 49 382,000 CQADupstack cqadupstack test 13,145 457,000 Quora quora dev, test 10,000 523,000 DBPedia dbpedia-entity dev, test 400 4,630,000 SCIDOCS scidocs test 1,000 25,000 FEVER fever train, dev, test 6,666 5,420,000 Climate-FEVER climate-fever test 1,535 5,420,000 SciFact scifact train, test 300 5,000"},{"location":"documentation/evaluation/#custom-datasets","title":"Custom datasets","text":"<p>You can also run evaluation on your custom dataset using the following structure:</p> <ul> <li><code>corpus.jsonl</code>: each row contains a json element with two properties: <code>['_id', 'text']</code><ul> <li><code>_id</code> refers to the document identifier.</li> <li><code>text</code> contains the text of the document.</li> <li>(an additional <code>title</code> field can also be added if necessary)</li> </ul> </li> <li><code>queries.jsonl</code>: each row contains a json element with two properties: <code>['_id', 'text']</code><ul> <li><code>_id</code> refers to the query identifier.</li> <li><code>text</code> contains the text of the query.</li> </ul> </li> <li><code>qrels</code> folder contains tsv files with three columns: <code>['query-id', 'doc-id', 'score']</code><ul> <li><code>query-id</code> refers to the query identifier.</li> <li><code>doc-id</code> refers to the document identifier.</li> <li><code>score</code> contains the relation between the query and the document (1 if relevant, else 0) The name of the tsv corresponds to the split (e.g, \"dev\").</li> </ul> </li> </ul> <p>You can then use the same pipeline as with BEIR datasets by changing the loading of the data in step 3:</p> <pre><code>documents, queries, qrels = evaluation.load_custom_dataset(\n    \"custom_dataset\", split=\"dev\"\n)\n</code></pre>"},{"location":"documentation/evaluation/#metrics","title":"Metrics","text":"<p>PyLate evaluation is based on Ranx Python library to compute standard Information Retrieval metrics. The following metrics are supported:</p> Table Metric Alias @k Hits hits Yes Hit Rate / Success hit_rate Yes Precision precision Yes Recall recall Yes F1 f1 Yes R-Precision r_precision No Bpref bpref No Rank-biased Precision rbp No Mean Reciprocal Rank mrr Yes Mean Average Precision map Yes DCG dcg Yes DCG Burges dcg_burges Yes NDCG ndcg Yes NDCG Burges ndcg_burges Yes <p>For any details about the metrics, please refer to Ranx documentation.</p> <p>Sample code to evaluate the retrieval results using specific metrics:</p> <pre><code>results = evaluation.evaluate(\n    scores=scores,\n    qrels=qrels,\n    queries=queries,\n    metrics=[f\"ndcg@{k}\" for k in [1, 3, 5, 10, 100]] # NDCG for different k values\n    + [f\"hits@{k}\" for k in [1, 3, 5, 10, 100]]       # Hits at different k values\n    + [\"map\"]                                         # Mean Average Precision (MAP)\n    + [\"recall@10\", \"recall@100\"]                     # Recall at k\n    + [\"precision@10\", \"precision@100\"],              # Precision at k\n)\n</code></pre>"},{"location":"documentation/fastapi/","title":"Serve the embeddings of a PyLate model using FastAPI","text":"<p>The <code>server.py</code> script (located in the <code>server</code> folder) allows to create a FastAPI server to serve the embeddings of a PyLate model. To use it, you need to install the api dependencies: <code>pip install \"pylate[api]\"</code> Then, run <code>python server.py</code> to launch the server.</p> <p>You can then send requests to the API like so: <pre><code>curl -X POST http://localhost:8002/v1/embeddings \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"input\": [\"Query 1\", \"Query 2\"],\n    \"model\": \"lightonai/GTE-ModernColBERT-v1\",\n    \"is_query\": false\n  }'\n</code></pre> If you want to encode queries, simply set <code>\u00ecs_query</code> to <code>True</code>.</p> Tip <p>Note that the server leverages batched, so you can do batch processing by sending multiple separate calls and it will create batches dynamically to fill up the GPU.</p> <p>For now, the server only support one loaded model, which you can define by using the <code>--model</code> argument when launching the server.</p>"},{"location":"documentation/retrieval/","title":"ColBERT-based Retrieval with PLAID","text":"<p>PyLate provides a streamlined interface to index and retrieve documents using ColBERT models, powered by our high-performance PLAID index.</p>"},{"location":"documentation/retrieval/#the-plaid-index","title":"The PLAID Index","text":"<p>PyLate leverages PLAID, a purpose-built index for fast ColBERT retrieval and specifically FastPLAID, an optimized implementation that delivers significant performance improvements over the original PLAID.</p>"},{"location":"documentation/retrieval/#indexing","title":"Indexing","text":"<p>The following example demonstrates the end-to-end process of creating a PLAID index from a collection of documents.</p>"},{"location":"documentation/retrieval/#step-by-step-index-creation","title":"Step-by-Step Index Creation","text":"<p>Here's how to load a model, initialize an index, and populate it with your documents.</p> <pre><code>from pylate import indexes, models\n\n# A sample collection of documents to index\ndocuments_ids = [\"doc_001\", \"doc_002\", \"doc_003\", \"doc_004\", \"doc_005\", \"doc_006\", \"doc_007\", \"doc_008\"]\n\ndocuments = [\n    \"The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. It was designed by Gustave Eiffel and completed in 1889 as the entrance to the 1889 World's Fair. It is a globally recognized symbol of France and one of the most visited paid monuments in the world. The tower is 330 meters tall and has three levels for visitors.\",\n    \"The Louvre is the world's largest art museum and a historic monument in Paris, France. It is located on the Right Bank of the Seine River in the 1st arrondissement. A central landmark of the city, the Louvre is home to some of the most famous works of art, including the Mona Lisa and the Venus de Milo. Its distinctive glass pyramid was added in 1989.\",\n    \"ColBERT is a highly effective neural retrieval model based on late interaction. It was introduced by Omar Khattab and Matei Zaharia. Unlike traditional dense retrieval models that compute a single vector for the entire query and document, ColBERT computes a contextualized embedding for each token of the query and document, and then performs a fast, parallelized late interaction between them.\",\n    \"Paris is known for its cafes, fashion, and the Seine River. The city's rich history dates back to the Roman era, and it has since become a major center for art, culture, and gastronomy. Landmarks like the Notre-Dame Cathedral, Arc de Triomphe, and the Sacr\u00e9-C\u0153ur Basilica add to its architectural beauty.\",\n    \"Deep Learning based information retrieval models have advanced the state of the art in search. These models, often trained on massive datasets, can understand the nuanced semantics of language, moving beyond simple keyword matching. Techniques like Siamese networks, Transformers, and contrastive learning are commonly used to build these powerful retrieval systems.\",\n    \"The University of Paris, known as the Sorbonne, is one of the world's oldest universities. It was founded in the mid-12th century and gained a strong reputation for academic excellence. The university has played a pivotal role in the intellectual history of Europe and has been a hub for philosophers, scientists, and writers for centuries.\",\n    \"The Arc de Triomphe honors those who fought and died for France in the French Revolutionary and Napoleonic Wars. Located at the western end of the Champs-\u00c9lys\u00e9es, it is a key landmark of Paris and a symbol of national pride. The monument stands at the center of Place Charles de Gaulle, a busy roundabout.\",\n    \"The Seine is a major river in northern France, flowing through Paris and into the English Channel. It has been a vital waterway for commerce and a source of inspiration for countless artists. Many of Paris's famous monuments and buildings are situated along its banks, including the Notre-Dame Cathedral and the Louvre Museum.\"\n]\n\n# --- Step 1: Load the ColBERT Model ---\nmodel = models.ColBERT(\n    model_name_or_path=\"lightonai/GTE-ModernColBERT-v1\",\n)\n\n# --- Step 2: Initialize the PLAID Index ---\nindex = indexes.PLAID(\n    index_folder=\"pylate-colbert-index\",\n    index_name=\"my_documents\",\n    override=True,\n)\n\n# --- Step 3: Encode Documents into Embeddings ---\ndocuments_embeddings = model.encode(\n    documents,\n    batch_size=32,\n    is_query=False,\n    show_progress_bar=True,\n)\n\n# --- Step 4: Add Document Embeddings to the Index ---\nindex.add_documents(\n    documents_ids=documents_ids,\n    documents_embeddings=documents_embeddings,\n)\n\nprint(\"Indexing complete!\")\n</code></pre>"},{"location":"documentation/retrieval/#persisting-and-re-loading-an-index","title":"Persisting and Re-loading an Index","text":"<p>You only need to build the index once. For subsequent uses, you can load the existing index directly from disk. By setting <code>override=False</code>, you ensure that the existing index is preserved and adding documents will append to it rather than replacing it.</p> <pre><code>loaded_index = indexes.PLAID(\n    index_folder=\"pylate-colbert-index\",\n    index_name=\"my_documents\",\n    override=False,\n)\n</code></pre>"},{"location":"documentation/retrieval/#retrieval","title":"Retrieval","text":"<p>Once the index is built, you can perform searches. The process involves encoding queries and using the retriever to fetch the top-k most relevant documents.</p>"},{"location":"documentation/retrieval/#step-by-step-retrieval","title":"Step-by-Step Retrieval","text":"<p>This example continues from the previous section, using the index we already created.</p> <pre><code>from pylate import retrieve\n\n# Assume 'model' and 'index' are already loaded from the previous steps.\n\n# --- Step 1: Initialize the ColBERT Retriever ---\n# The retriever links the model's scoring logic with the PLAID index.\nretriever = retrieve.ColBERT(index=index)\n\n# A list of queries to search for\nqueries = [\"monuments in Paris\", \"neural retrieval models\"]\n\n# --- Step 2: Encode Queries into Embeddings ---\n# Encode the search queries. Note that 'is_query=True' is critical here\n# as queries are processed differently from documents.\nqueries_embeddings = model.encode(\n    queries,\n    batch_size=32,\n    is_query=True,\n    show_progress_bar=True,\n)\n\n# --- Step 3: Retrieve Top-k Documents ---\n# The 'retrieve' method searches the index and returns ranked results.\n# 'k' specifies the maximum number of documents to return for each query.\nsearch_results = retriever.retrieve(\n    queries_embeddings=queries_embeddings,\n    k=2,\n)\n\nprint(search_results)\n</code></pre> <p>Example Output:</p> <pre><code>[\n    [\n        {'id': 'doc_002', 'score': 28.87158203125},\n        {'id': 'doc_004', 'score': 28.73193359375}\n    ],\n    [\n        {'id': 'doc_003', 'score': 29.3173828125},\n        {'id': 'doc_005', 'score': 29.087890625}\n    ]\n]\n</code></pre>"},{"location":"documentation/retrieval/#advanced-features-optimizations","title":"Advanced Features &amp; Optimizations","text":""},{"location":"documentation/retrieval/#filtering-search-results","title":"Filtering Search Results","text":"<p>You can constrain a search to a specific subset of document IDs. This is useful for implementing metadata-based filtering (e.g., <code>date &gt; 2024</code>) or searching within a user-defined collection. Filtering can also significantly accelerate searches by reducing the search space.</p> Document ID Assignment <p>The document IDs used in the <code>subset</code> parameter must match the string identifiers provided in <code>documents_ids</code> during the indexing phase.</p> <p>Single filter for all queries:</p> <p>Apply the same list of allowed document IDs to every query in the batch.</p> <pre><code># Only documents \"doc_001\" and \"doc_003\" are considered for search.\nscores = retriever.retrieve(\n    queries_embeddings=queries_embeddings,\n    k=10,\n    subset=[\"doc_001\", \"doc_003\"]\n)\n</code></pre> <p>Different filters for each query:</p> <p>Provide a list of lists, where each inner list is the specific subset of document IDs for the corresponding query.</p> <pre><code>scores = retriever.retrieve(\n    queries_embeddings=queries_embeddings,\n    k=10,\n    subset=[\n        [\"doc_001\", \"doc_002\"],  # Filter for the first query\n        [\"doc_003\"]             # Filter for the second query\n    ]\n)\n</code></pre>"},{"location":"documentation/retrieval/#compressing-document-embeddings-pooling","title":"Compressing Document Embeddings (Pooling)","text":"<p>To reduce the memory and storage footprint of the index, you can \"pool\" similar token embeddings within a document. This technique averages token embeddings that are close to each other in the embedding space, effectively compressing the document representation.</p> <p>You can enable this by setting the <code>pool_factor</code> during document encoding. A <code>pool_factor</code> of 2 will attempt to reduce the number of token embeddings by half.</p> Performance vs. Compression <p>As detailed in this blog post, a <code>pool_factor</code> of 2 can halve the index size with virtually zero drop in retrieval performance. Higher factors offer more compression at the cost of some accuracy.</p> <pre><code># Example of encoding with pooling\ndocuments_embeddings = model.encode(\n    documents,\n    batch_size=32,\n    is_query=False,\n    pool_factor=2,  # Keep 1/2 of the original tokens\n    show_progress_bar=True,\n)\n</code></pre>"},{"location":"documentation/retrieval/#incremental-index-updates","title":"Incremental Index Updates","text":"<p>The PLAID index computes its k-means centroids based on the initial set of documents provided. You can add new documents to an existing index at any time using <code>add_documents</code>, and <code>override=False</code> but these new documents will be assigned to the original centroids.</p> Recommendation for Large-Scale Updates <p>While incremental additions are supported, adding a very large volume of new documents that significantly shifts the data distribution may lead to suboptimal clustering. For best performance after massive updates, it is recommended to rebuild the index from scratch.</p>"},{"location":"documentation/retrieval/#performance-tuning","title":"Performance Tuning","text":"<p>FastPLAID offers several parameters to fine-tune the trade-off between indexing speed, search speed, and retrieval accuracy.</p>"},{"location":"documentation/retrieval/#indexing-time-parameters","title":"Indexing-Time Parameters","text":"<p>These are configured during the <code>indexes.PLAID</code> initialization.</p> Parameter Default Speed Impact Accuracy Impact Description <code>n_samples_kmeans</code> <code>None</code> Lower = faster Lower = less precise Number of embeddings sampled to train k-means centroids. <code>None</code> uses all embeddings. <code>nbits</code> 4 Lower = faster Lower = less precise Bits per sub-vector in Product Quantization. Controls compression level. (Range: 2-8) <code>kmeans_niters</code> 4 Higher = slower Higher = better clusters Number of iterations for the k-means clustering algorithm. <p>Guidelines:</p> <ul> <li>Fastest Indexing: Use lower <code>n_samples_kmeans</code>, <code>nbits</code> (e.g., 2), and <code>kmeans_niters</code> (e.g., 2).</li> <li>Highest Quality Index: Use higher <code>nbits</code> (e.g., 8) and <code>kmeans_niters</code> (e.g., 10+).</li> </ul>"},{"location":"documentation/retrieval/#search-time-parameters","title":"Search-Time Parameters","text":"<p>These are configured in the <code>retriever.retrieve</code> method.</p> Parameter Default Speed Impact Accuracy Impact Description <code>n_ivf_probe</code> 8 Higher = slower Higher = better recall Number of IVF clusters to visit for each query. A higher value increases the chance of finding relevant documents. <code>n_full_scores</code> 8192 Higher = slower Higher = better ranking Number of candidate documents to re-rank with the full <code>MaxSim</code> operation. <p>Guidelines:</p> <ul> <li>Fastest Search: Use a lower <code>n_ivf_probe</code> (e.g., 4) and <code>n_full_scores</code> (e.g., 1024).</li> <li>Highest Recall: Use a higher <code>n_ivf_probe</code> (e.g., 16-32) and <code>n_full_scores</code> (e.g., 8192+).</li> </ul>"},{"location":"documentation/retrieval/#stanford-plaid","title":"Stanford PLAID","text":"<p>Instead of using the default FastPLAID backend, you can opt for the original Stanford PLAID implementation. This is primarily for research or comparison purposes, as it is significantly slower.</p> <pre><code>index = indexes.PLAID(\n    index_folder=\"pylate-colbert-index\",\n    index_name=\"my_documents\",\n    override=False,\n    use_fast=False,  # Use the original Stanford PLAID implementation\n)\n</code></pre>"},{"location":"documentation/retrieval/#reranking","title":"Reranking","text":"<p>To perform reranking on top of your first-stage retrieval pipeline without building an index, you can simply use <code>rank.rerank</code> function which takes the queries and documents embeddings along with the documents ids to rerank them:</p> <pre><code>from pylate import rank\n\nqueries = [\n    \"query A\",\n    \"query B\",\n]\n\ndocuments = [\n    [\"document A\", \"document B\"],\n    [\"document 1\", \"document C\", \"document B\"],\n]\n\ndocuments_ids = [\n    [1, 2],\n    [1, 3, 2],\n]\n\nqueries_embeddings = model.encode(\n    queries,\n    is_query=True,\n)\n\ndocuments_embeddings = model.encode(\n    documents,\n    is_query=False,\n)\n\nreranked_documents = rank.rerank(\n    documents_ids=documents_ids,\n    queries_embeddings=queries_embeddings,\n    documents_embeddings=documents_embeddings,\n)\n</code></pre>"},{"location":"documentation/training/","title":"ColBERT Training","text":"<p>PyLate training is based on Sentence Transformer (and thus transformers) trainer, enabling a lot of functionality such multi-GPU and FP16/BF16 training as well as logging to Weights &amp; Biases out-of-the-box. This allows efficient, and scalable training.</p> Info <p>There are two primary ways to train ColBERT models using PyLate:</p> <ol> <li> <p>Contrastive Loss: Simplest method, it only requires a dataset containing triplets, each consisting of a query, a positive document (relevant to the query), and a negative document (irrelevant to the query). This method trains the model to maximize the similarity between the query and the positive document, while minimizing it with the negative document.</p> </li> <li> <p>Knowledge Distillation: To train a ColBERT model using knowledge distillation, you need to provide a dataset with three components: queries, documents, and the relevance scores between them. This method compresses the knowledge of a larger model / more accurate model (cross-encoder) into a smaller one, using the relevance scores to guide the training process.</p> </li> </ol>"},{"location":"documentation/training/#contrastive-training","title":"Contrastive Training","text":"<p>ColBERT was originally trained using contrastive learning. This approach involves teaching the model to distinguish between relevant (positive) and irrelevant (negative) documents for a given query. The model is trained to maximize the similarity between a query and its corresponding positive document while minimizing the similarity with irrelevant documents.</p> <p>PyLate uses contrastive learning with a triplet dataset, where each query is paired with one positive and one negative example. This makes it fully compatible with any triplet datasets from the sentence-transformers library.</p> <p>During training, the model is optimized to maximize the similarity between the query and its positive example while minimizing the similarity with all negative examples and the positives from other queries in the batch. This approach leverages in-batch negatives for more effective learning.</p> <p>Here is an example of code to run contrastive training with PyLate:</p> <pre><code>import torch\nfrom datasets import load_dataset\nfrom sentence_transformers import (\n    SentenceTransformerTrainer,\n    SentenceTransformerTrainingArguments,\n)\n\nfrom pylate import evaluation, losses, models, utils\n\n# Define model parameters for contrastive training\nmodel_name = \"bert-base-uncased\"  # Choose the pre-trained model you want to use as base\nbatch_size = 32  # Larger batch size often improves results, but requires more memory\n\nnum_train_epochs = 1  # Adjust based on your requirements\n# Set the run name for logging and output directory\nrun_name = \"contrastive-bert-base-uncased\"\noutput_dir = f\"output/{run_name}\"\n\n# 1. Here we define our ColBERT model. If not a ColBERT model, will add a linear layer to the base encoder.\nmodel = models.ColBERT(model_name_or_path=model_name)\n\n# Compiling the model makes the training faster\nmodel = torch.compile(model)\n\n# Load dataset\ndataset = load_dataset(\"sentence-transformers/msmarco-bm25\", \"triplet\", split=\"train\")\n# Split the dataset (this dataset does not have a validation set, so we split the training set)\nsplits = dataset.train_test_split(test_size=0.01)\ntrain_dataset = splits[\"train\"]\neval_dataset = splits[\"test\"]\n\n# Define the loss function\ntrain_loss = losses.Contrastive(model=model)\n\n# Initialize the evaluator\ndev_evaluator = evaluation.ColBERTTripletEvaluator(\n    anchors=eval_dataset[\"query\"],\n    positives=eval_dataset[\"positive\"],\n    negatives=eval_dataset[\"negative\"],\n)\n\n# Configure the training arguments (e.g., batch size, evaluation strategy, logging steps)\nargs = SentenceTransformerTrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=num_train_epochs,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    fp16=True,  # Set to False if you get an error that your GPU can't run on FP16\n    bf16=False,  # Set to True if you have a GPU that supports BF16\n    run_name=run_name,  # Will be used in W&amp;B if `wandb` is installed\n    learning_rate=3e-6,\n)\n\n# Initialize the trainer for the contrastive training\ntrainer = SentenceTransformerTrainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    loss=train_loss,\n    evaluator=dev_evaluator,\n    data_collator=utils.ColBERTCollator(model.tokenize),\n)\n# Start the training process\ntrainer.train()\n</code></pre> Tip <p>Please note that temperature parameter has a very high importance in contrastive learning. A low temperature allows to focus more on the hardest elements in the batch, creating more discriminative representations but is more sensible to false negative. A temperature around 0.02 is often used in the literature: <pre><code>train_loss = losses.Contrastive(model=model, temperature=0.02)\n</code></pre></p> Tip <p>As contrastive learning is not compatible with gradient accumulation, you can leverage GradCache to emulate bigger batch sizes without requiring more memory by using the <code>CachedContrastiveLoss</code> to define a mini_batch_size while increasing the <code>per_device_train_batch_size</code>: <pre><code>train_loss = losses.CachedContrastive(\n        model=model, mini_batch_size=mini_batch_size\n)\n</code></pre></p> Tip <p>Finally, if you are in a multi-GPU setting, you can gather all the elements from the different GPUs to create even bigger batch sizes by setting <code>gather_across_devices</code> to <code>True</code> (for both <code>Contrastive</code> and <code>CachedContrastive</code> losses): <pre><code>train_loss = losses.Contrastive(model=model, gather_across_devices=True)\n</code></pre></p> Tip <p>Please note that for multi-GPU training, running <code>python training.py</code> will use Data Parallel (DP) by default. We strongly suggest using using Distributed Data Parallelism (DDP) using accelerate or torchrun: <code>accelerate launch --num_processes num_gpu training.py</code>.</p> <p>Refer to this documentation for more information.</p>"},{"location":"documentation/training/#knowledge-distillation-training","title":"Knowledge Distillation Training","text":"<p>Training late-interaction models, such as ColBERT, has been shown to benefit from knowledge distillation compared to simpler contrastive learning approaches. Knowledge distillation training focuses on teaching ColBERT models to replicate the outputs of a more capable teacher model (e.g., a cross-encoder). This is achieved using a dataset that includes queries, documents, and the scores assigned by the teacher model to each query/document pair.</p> <p>Below is an example of code to run knowledge distillation training using PyLate:</p> <pre><code>import torch\nfrom datasets import load_dataset\nfrom sentence_transformers import (\n    SentenceTransformerTrainer,\n    SentenceTransformerTrainingArguments,\n)\n\nfrom pylate import losses, models, utils\n\n# Load the datasets required for knowledge distillation (train, queries, documents)\ntrain = load_dataset(\n    path=\"lightonai/ms-marco-en-bge\",\n    name=\"train\",\n)\n\nqueries = load_dataset(\n    path=\"lightonai/ms-marco-en-bge\",\n    name=\"queries\",\n)\n\ndocuments = load_dataset(\n    path=\"lightonai/ms-marco-en-bge\",\n    name=\"documents\",\n)\n\n# Set the transformation to load the documents/queries texts using the corresponding ids on the fly\ntrain.set_transform(\n    utils.KDProcessing(queries=queries, documents=documents).transform,\n)\n\n# Define the base model, training parameters, and output directory\nmodel_name = \"bert-base-uncased\"  # Choose the pre-trained model you want to use as base\nbatch_size = 16\nnum_train_epochs = 1\n# Set the run name for logging and output directory\nrun_name = \"knowledge-distillation-bert-base\"\noutput_dir = f\"output/{run_name}\"\n\n# Initialize the ColBERT model from the base model\nmodel = models.ColBERT(model_name_or_path=model_name)\n\n# Compiling the model to make the training faster\nmodel = torch.compile(model)\n\n# Configure the training arguments (e.g., epochs, batch size, learning rate)\nargs = SentenceTransformerTrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=num_train_epochs,\n    per_device_train_batch_size=batch_size,\n    fp16=True,  # Set to False if you get an error that your GPU can't run on FP16\n    bf16=False,  # Set to True if you have a GPU that supports BF16\n    run_name=run_name,\n    learning_rate=1e-5,\n)\n\n# Use the Distillation loss function for training\ntrain_loss = losses.Distillation(model=model)\n\n# Initialize the trainer\ntrainer = SentenceTransformerTrainer(\n    model=model,\n    args=args,\n    train_dataset=train,\n    loss=train_loss,\n    data_collator=utils.ColBERTCollator(tokenize_fn=model.tokenize),\n)\n\n# Start the training process\ntrainer.train()\n</code></pre> Tip <p>Please note that for multi-GPU training, running <code>python training.py</code> will use Data Parallel (DP) by default. We strongly suggest using using Distributed Data Parallelism (DDP) using accelerate or torchrun: <code>accelerate launch --num_processes num_gpu training.py</code>.</p> <p>Refer to this documentation for more information.</p>"},{"location":"documentation/training/#nanobeir-evaluator","title":"NanoBEIR evaluator","text":"<p>If you are training an English retrieval model, PyLate now features NanoBEIREvaluator, an evaluator that allows to run small versions of the BEIR datasets to get an idea of the performance on BEIR without taking too long to run. <pre><code>evaluator=evaluation.NanoBEIREvaluator(),\n</code></pre> You can select a subset of all the sets to run by adding the dataset names: <code>evaluation.NanoBEIREvaluator(dataset_names=[\"SciFact\"])</code></p>"},{"location":"documentation/training/#colbert-parameters","title":"ColBERT parameters","text":"<p>All the parameters of the ColBERT modeling can be found here. Important parameters to consider are:</p> Info <ul> <li><code>model_name_or_path</code> the name of the base encoder model or PyLate model to init from.</li> <li><code>embedding_size</code> the output size of the projection layer. Large values give more capacity to the model but are heavier to store.</li> <li><code>query_prefix</code> and <code>document_prefix</code> represents the strings that will be prepended to query and document respectively.</li> <li><code>query_length</code> and <code>document_length</code> set the maximum size of queries and documents. Queries will be padded/truncated to the maximum length while documents are only truncated.</li> <li><code>attend_to_expansion_tokens</code> define whether the model will attend to the query expansion tokens (padding of queries) or if only the expansion tokens will attend to the other tokens. In the original ColBERT, the tokens do not attend to expansion tokens.</li> <li><code>skiplist_words</code> is list of words to skip from the documents scoring (note that these tokens are used for encoding and are only skipped during the scoring), the default is the list of string.punctuation as in the original ColBERT.</li> </ul>"},{"location":"documentation/training/#sentence-transformers-training-arguments","title":"Sentence Transformers Training Arguments","text":"<p>PyLate is built on top of SentenceTransformer, so you can use the same arguments you are already familiar with to control the training process. The table below lists the arguments available in the SentenceTransformerTrainingArguments class. For more details, please refer to the SentenceTransformers documentation.</p> Table Parameter Name Definition Training Performance Observing Performance <code>output_dir</code> <code>str</code> The output directory where the model predictions and checkpoints will be written. <code>overwrite_output_dir</code> <code>bool</code>, optional, defaults to <code>False</code> If <code>True</code>, overwrite the content of the output directory. Use this to continue training if <code>output_dir</code> points to a checkpoint directory. <code>do_train</code> <code>bool</code>, optional, defaults to <code>False</code> Whether to run training or not. Intended to be used by your training/evaluation scripts. <code>do_eval</code> <code>bool</code>, optional Whether to run evaluation on the validation set. Will be <code>True</code> if <code>eval_strategy</code> is not <code>\"no\"</code>. Intended to be used by your training/evaluation scripts. <code>do_predict</code> <code>bool</code>, optional, defaults to <code>False</code> Whether to run predictions on the test set or not. Intended to be used by your training/evaluation scripts. <code>eval_strategy</code> <code>str</code> or <code>~trainer_utils.IntervalStrategy</code>, optional, defaults to <code>\"no\"</code> The evaluation strategy to adopt during training. Possible values are <code>\"no\"</code>, <code>\"steps\"</code>, or <code>\"epoch\"</code>. \u2705 <code>prediction_loss_only</code> <code>bool</code>, optional, defaults to <code>False</code> When performing evaluation and generating predictions, only returns the loss. <code>per_device_train_batch_size</code> <code>int</code>, optional, defaults to 8 The batch size per GPU/XPU/TPU/MPS/NPU core/CPU for training. \u2705 <code>per_device_eval_batch_size</code> <code>int</code>, optional, defaults to 8 The batch size per GPU/XPU/TPU/MPS/NPU core/CPU for evaluation. \u2705 <code>gradient_accumulation_steps</code> <code>int</code>, optional, defaults to 1 Number of updates steps to accumulate gradients before performing a backward/update pass. \u2705 <code>eval_accumulation_steps</code> <code>int</code>, optional Number of predictions steps to accumulate the output tensors before moving the results to CPU. \u2705 <code>eval_delay</code> <code>float</code>, optional Number of epochs or steps to wait before the first evaluation depending on <code>eval_strategy</code>. <code>torch_empty_cache_steps</code> <code>int</code>, optional Number of steps to wait before calling <code>torch.&lt;device&gt;.empty_cache()</code> to avoid CUDA out-of-memory errors. <code>learning_rate</code> <code>float</code>, optional, defaults to 5e-5 The initial learning rate for <code>AdamW</code> optimizer. \u2705 <code>num_train_epochs</code> <code>float</code>, optional, defaults to 3.0 Total number of training epochs to perform. \u2705 <code>max_steps</code> <code>int</code>, optional, defaults to -1 If set to a positive number, the total number of training steps to perform. Overrides <code>num_train_epochs</code>. \u2705 <code>lr_scheduler_type</code> <code>str</code> or <code>SchedulerType</code>, optional, defaults to <code>\"linear\"</code> The scheduler type to use. \u2705 <code>lr_scheduler_kwargs</code> <code>dict</code>, optional, defaults to {} Extra arguments for the learning rate scheduler. <code>warmup_ratio</code> <code>float</code>, optional, defaults to 0.0 Ratio of total training steps used for linear warmup from 0 to <code>learning_rate</code>. \u2705 <code>warmup_steps</code> <code>int</code>, optional, defaults to 0 Number of steps used for linear warmup from 0 to <code>learning_rate</code>. Overrides any effect of <code>warmup_ratio</code>. <code>log_level</code> <code>str</code>, optional, defaults to <code>passive</code> Logger log level to use on the main process. \u2705 <code>log_level_replica</code> <code>str</code>, optional, defaults to <code>\"warning\"</code> Logger log level to use on replicas. Same choices as <code>log_level</code>. <code>log_on_each_node</code> <code>bool</code>, optional, defaults to <code>True</code> Whether to log using <code>log_level</code> once per node or only on the main node. <code>logging_dir</code> <code>str</code>, optional TensorBoard log directory. <code>logging_strategy</code> <code>str</code> or <code>~trainer_utils.IntervalStrategy</code>, optional, defaults to <code>\"steps\"</code> The logging strategy to adopt during training. Possible values are <code>\"no\"</code>, <code>\"epoch\"</code>, or <code>\"steps\"</code>. \u2705 <code>logging_first_step</code> <code>bool</code>, optional, defaults to <code>False</code> Whether to log the first <code>global_step</code> or not. <code>logging_steps</code> <code>int</code> or <code>float</code>, optional, defaults to 500 Number of update steps between two logs if <code>logging_strategy=\"steps\"</code>. \u2705 <code>logging_nan_inf_filter</code> <code>bool</code>, optional, defaults to <code>True</code> Whether to filter <code>nan</code> and <code>inf</code> losses for logging. <code>save_strategy</code> <code>str</code> or <code>~trainer_utils.IntervalStrategy</code>, optional, defaults to <code>\"steps\"</code> The checkpoint save strategy to adopt during training. \u2705 <code>save_steps</code> <code>int</code> or <code>float</code>, optional, defaults to 500 Number of update steps before two checkpoint saves if <code>save_strategy=\"steps\"</code>. \u2705 <code>save_total_limit</code> <code>int</code>, optional Limit for total number of checkpoints. \u2705 <code>save_safetensors</code> <code>bool</code>, optional, defaults to <code>True</code> Use safetensors saving and loading for state dicts instead of default <code>torch.load</code> and <code>torch.save</code>. <code>save_on_each_node</code> <code>bool</code>, optional, defaults to <code>False</code> Whether to save models and checkpoints on each node or only on the main one during multi-node distributed training. <code>seed</code> <code>int</code>, optional, defaults to 42 Random seed set at the beginning of training for reproducibility. <code>auto_find_batch_size</code> <code>bool</code>, optional, defaults to <code>False</code> Whether to find a batch size that will fit into memory automatically. \u2705 <code>fp16</code> <code>bool</code>, optional, defaults to <code>False</code> Whether to use fp16 16-bit (mixed) precision training instead of 32-bit training. \u2705 <code>bf16</code> <code>bool</code>, optional, defaults to <code>False</code> Whether to use bf16 16-bit (mixed) precision training instead of 32-bit training. \u2705 <code>push_to_hub</code> <code>bool</code>, optional, defaults to <code>False</code> Whether to push the model to the Hub every time the model is saved. \u2705 <code>hub_model_id</code> <code>str</code>, optional The name of the repository to keep in sync with the local <code>output_dir</code>. \u2705 <code>hub_strategy</code> <code>str</code> or <code>~trainer_utils.HubStrategy</code>, optional, defaults to <code>\"every_save\"</code> Defines the scope of what is pushed to the Hub and when. \u2705 <code>hub_private_repo</code> <code>bool</code>, optional, defaults to <code>False</code> If <code>True</code>, the Hub repo will be set to private. \u2705 <code>load_best_model_at_end</code> <code>bool</code>, optional, defaults to <code>False</code> Whether or not to load the best model found during training at the end of training. \u2705 <code>report_to</code> <code>str</code> or <code>List[str]</code>, optional, defaults to <code>\"all\"</code> The list of integrations to report the results and logs to. \u2705"},{"location":"documentation/training/#sentence-transformer-trainer-arguments","title":"Sentence Transformer Trainer arguments","text":"Table Parameter Name Definition model <code>~sentence_transformers.SentenceTransformer</code>, optional The model to train, evaluate, or use for predictions. If not provided, a <code>model_init</code> must be passed. args <code>~sentence_transformers.training_args.SentenceTransformerTrainingArguments</code>, optional The arguments to tweak for training. Defaults to a basic instance of <code>SentenceTransformerTrainingArguments</code> with the <code>output_dir</code> set to a directory named tmp_trainer in the current directory if not provided. train_dataset <code>datasets.Dataset</code>, <code>datasets.DatasetDict</code>, or <code>Dict[str, datasets.Dataset]</code>, optional The dataset to use for training. Must have a format accepted by your loss function. Refer to <code>Training Overview &gt; Dataset Format</code>. eval_dataset <code>datasets.Dataset</code>, <code>datasets.DatasetDict</code>, or <code>Dict[str, datasets.Dataset]</code>, optional The dataset to use for evaluation. Must have a format accepted by your loss function. Refer to <code>Training Overview &gt; Dataset Format</code>. loss <code>torch.nn.Module</code>, <code>Dict[str, torch.nn.Module]</code>, Callable, or Dict[str, Callable], optional The loss function to use for training. It can be a loss class instance, a dictionary mapping dataset names to loss instances, a function returning a loss instance given a model, or a dictionary mapping dataset names to such functions. Defaults to <code>CoSENTLoss</code> if not provided. evaluator <code>~sentence_transformers.evaluation.SentenceEvaluator</code> or <code>List[~sentence_transformers.evaluation.SentenceEvaluator]</code>, optional The evaluator instance for useful metrics during training. Can be used with or without an <code>eval_dataset</code>. A list of evaluators will be wrapped in a <code>SequentialEvaluator</code> to run sequentially. Generally, evaluator metrics are more useful than loss values from <code>eval_dataset</code>. callbacks <code>List[transformers.TrainerCallback]</code>, optional A list of callbacks to customize the training loop. Adds to the list of default callbacks. To remove a default callback, use the <code>Trainer.remove_callback</code> method. optimizers <code>Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]</code>, optional, defaults to <code>(None, None)</code> A tuple containing the optimizer and scheduler to use. Defaults to an instance of <code>torch.optim.AdamW</code> for the model and a scheduler given by <code>transformers.get_linear_schedule_with_warmup</code>, controlled by <code>args</code>."},{"location":"models/models/","title":"Available models","text":"<p>Tip</p> <p>Following an update, all the models trained using the stanford-nlp ColBERT library or RAGatouille should be compatible with PyLate natively (including their configurations). You can simply load the model in PyLate:</p> <p><pre><code>from pylate import models\n\nmodel = models.ColBERT(\n    model_name_or_path=\"colbert-ir/colbertv2.0\",\n)\n</code></pre> or <pre><code>model = models.ColBERT(\n    model_name_or_path=\"jinaai/jina-colbert-v2\",\n    trust_remote_code=True,\n)\n</code></pre></p> <p>Here is a list of some of the pre-trained ColBERT models available in PyLate along with their results on BEIR:</p> Table Model BEIR AVG NFCorpus SciFact SCIDOCS FiQA2018 TRECCOVID HotpotQA Touche2020 ArguAna ClimateFEVER FEVER QuoraRetrieval NQ DBPedia lightonai/colbertv2.0 50.02 33.8 69.3 15.4 35.6 73.3 66.7 26.3 46.3 17.6 78.5 85.2 56.2 44.6 answerdotai/answerai-colbert-small-v1 53.79 37.3 74.77 18.42 41.15 84.59 76.11 25.69 50.09 33.07 90.96 87.72 59.1 45.58 jinaai/jina-colbert-v2 53.1 34.6 67.8 18.6 40.8 83.4 76.6 27.4 36.6 23.9 80.05 88.7 64.0 47.1 GTE-ModernColBERT-v1 54.89 37.93 76.34 19.06 48.51 83.59 77.32 31.23 48.51 30.62 87.44 86.61 61.8 48.3 Note <p><code>lightonai/colbertv2.0</code> is the original ColBERTv2 model made compatible with PyLate before we supported loading directly model from Stanford-NLP. We thank Omar Khattab for allowing us to share the model on PyLate.</p>"},{"location":"models/models/#defining-dense-layers","title":"Defining dense layers","text":"<p>By default, if you use a base model to create a PyLate model, it'll add a dense layer projecting the output dimension of the model to <code>embedding_size</code>. If you did not specify any <code>embedding_size</code>, it'll default to 128.</p> <pre><code>model = models.ColBERT(\"bert-base-uncased\")\n</code></pre> <p>If you create a PyLate model from a sentence-transformers model, it'll load the dense layer of this model and only add another one if you specified an embedding_size and it is not matching the size of the last dense layer of the ST model.</p> <p>If you do not want to use the dense layers of the ST model (but still want to use its base weights), you should use the modular syntax: <pre><code>import torch\nfrom sentence_transformers.models import Transformer\nfrom pylate import models\n\nbase_model = Transformer(\"answerdotai/ModernBERT-base\")\n\ndense_1 = models.Dense(\n    in_features=768,\n    out_features=512,\n    bias=False,\n    activation_function=torch.nn.GELU(),\n)\ndense_2 = models.Dense(\n    in_features=512,\n    out_features=128,\n    bias=False,\n    activation_function=torch.nn.Identity(),\n)\n\nmodel = models.ColBERT(\n    modules=[base_model, dense_1, dense_2],\n    document_length=300,\n    query_length=32,\n)\n\nColBERT(\n  (0): Transformer({'max_seq_length': 8192, 'do_lower_case': False, 'architecture': 'ModernBertModel'})\n  (1): Dense({'in_features': 768, 'out_features': 512, 'bias': False, 'activation_function': 'torch.nn.modules.activation.GELU', 'use_residual': False})\n  (2): Dense({'in_features': 512, 'out_features': 128, 'bias': False, 'activation_function': 'torch.nn.modules.linear.Identity', 'use_residual': False})\n)\n</code></pre></p> <p>It also allows you to define the activation function and use multiple dense layers. Please note that you can also append layers to existing models as well as remove them, so you can really create the modules you want <pre><code>import torch\nfrom pylate import models\nmodel = models.ColBERT(\"google/embeddinggemma-300m\")\nColBERT(\n  (0): Transformer({'max_seq_length': 2048, 'do_lower_case': False, 'architecture': 'Gemma3TextModel'})\n  (1): Dense({'in_features': 768, 'out_features': 3072, 'bias': False, 'activation_function': 'torch.nn.modules.linear.Identity', 'use_residual': False})\n  (2): Dense({'in_features': 3072, 'out_features': 768, 'bias': False, 'activation_function': 'torch.nn.modules.linear.Identity', 'use_residual': False})\n)\n\ndense_1 = models.Dense(\n    in_features=768,\n    out_features=128,\n    bias=False,\n    activation_function=torch.nn.Identity(),\n    use_residual=False,\n)\n\nmodel.append(dense_1)\nColBERT(\n  (0): Transformer({'max_seq_length': 2048, 'do_lower_case': False, 'architecture': 'Gemma3TextModel'})\n  (1): Dense({'in_features': 768, 'out_features': 3072, 'bias': False, 'activation_function': 'torch.nn.modules.linear.Identity', 'use_residual': False})\n  (2): Dense({'in_features': 3072, 'out_features': 768, 'bias': False, 'activation_function': 'torch.nn.modules.linear.Identity', 'use_residual': False})\n  (3): Dense({'in_features': 768, 'out_features': 128, 'bias': False, 'activation_function': 'torch.nn.modules.linear.Identity', 'use_residual': False})\n)\n\ndel model[3]\nColBERT(\n  (0): Transformer({'max_seq_length': 2048, 'do_lower_case': False, 'architecture': 'Gemma3TextModel'})\n  (1): Dense({'in_features': 768, 'out_features': 3072, 'bias': False, 'activation_function': 'torch.nn.modules.linear.Identity', 'use_residual': False})\n  (2): Dense({'in_features': 3072, 'out_features': 768, 'bias': False, 'activation_function': 'torch.nn.modules.linear.Identity', 'use_residual': False})\n)\n</code></pre></p> <p>Tip</p> <p>MixedBread study showed that it is beneficial to use MLPs to do the projection rather than a simple dense layer. The study explores different depths, activation functions and the use of residual layers. Please check the paper for a more thorough analysis. <pre><code>import torch\nfrom sentence_transformers.models import Transformer\nfrom pylate import models\n\nbase_model = Transformer(\"jhu-clsp/ettin-encoder-32m\")\n\ndense_1 = models.Dense(\n    in_features=384,\n    out_features=768,\n    bias=False,\n    activation_function=torch.nn.Identity(),\n    use_residual=True,\n)\ndense_2 = models.Dense(\n    in_features=768,\n    out_features=384,\n    bias=False,\n    activation_function=torch.nn.Identity(),\n    use_residual=False,\n)\n\nmodel = models.ColBERT(\n    modules=[base_model, dense_1, dense_2],\n)\n</code></pre></p>"},{"location":"parse/__main__/","title":"main","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"This script is responsible for building the API reference. The API reference is located in\ndocs/api. The script scans through all the modules, classes, and functions. It processes\nthe __doc__ of each object and formats it so that MkDocs can process it in turn.\n\"\"\"\n</pre> \"\"\"This script is responsible for building the API reference. The API reference is located in docs/api. The script scans through all the modules, classes, and functions. It processes the __doc__ of each object and formats it so that MkDocs can process it in turn. \"\"\" In\u00a0[\u00a0]: Copied! <pre>from __future__ import annotations\n</pre> from __future__ import annotations In\u00a0[\u00a0]: Copied! <pre>import functools\nimport importlib\nimport inspect\nimport os\nimport pathlib\nimport re\nimport shutil\n</pre> import functools import importlib import inspect import os import pathlib import re import shutil In\u00a0[\u00a0]: Copied! <pre>from numpydoc.docscrape import ClassDoc, FunctionDoc\n</pre> from numpydoc.docscrape import ClassDoc, FunctionDoc In\u00a0[\u00a0]: Copied! <pre>package = \"pylate\"\n</pre> package = \"pylate\" In\u00a0[\u00a0]: Copied! <pre>shutil.copy(\"README.md\", \"docs/index.md\")\n</pre> shutil.copy(\"README.md\", \"docs/index.md\") In\u00a0[\u00a0]: Copied! <pre>with open(\"docs/index.md\", mode=\"r\") as file:\n    content = file.read()\n</pre> with open(\"docs/index.md\", mode=\"r\") as file:     content = file.read() In\u00a0[\u00a0]: Copied! <pre>with open(\"docs/index.md\", mode=\"w\") as file:\n    file.write(content)\n</pre> with open(\"docs/index.md\", mode=\"w\") as file:     file.write(content) In\u00a0[\u00a0]: Copied! <pre>def paragraph(text):\n    return f\"{text}\\n\"\n</pre> def paragraph(text):     return f\"{text}\\n\" In\u00a0[\u00a0]: Copied! <pre>def h1(text):\n    return paragraph(f\"# {text}\")\n</pre> def h1(text):     return paragraph(f\"# {text}\") In\u00a0[\u00a0]: Copied! <pre>def h2(text):\n    return paragraph(f\"## {text}\")\n</pre> def h2(text):     return paragraph(f\"## {text}\") In\u00a0[\u00a0]: Copied! <pre>def h3(text):\n    return paragraph(f\"### {text}\")\n</pre> def h3(text):     return paragraph(f\"### {text}\") In\u00a0[\u00a0]: Copied! <pre>def h4(text):\n    return paragraph(f\"#### {text}\")\n</pre> def h4(text):     return paragraph(f\"#### {text}\") In\u00a0[\u00a0]: Copied! <pre>def link(caption, href):\n    return f\"[{caption}]({href})\"\n</pre> def link(caption, href):     return f\"[{caption}]({href})\" In\u00a0[\u00a0]: Copied! <pre>def code(text):\n    return f\"`{text}`\"\n</pre> def code(text):     return f\"`{text}`\" In\u00a0[\u00a0]: Copied! <pre>def li(text):\n    return f\"- {text}\\n\"\n</pre> def li(text):     return f\"- {text}\\n\" In\u00a0[\u00a0]: Copied! <pre>def snake_to_kebab(text):\n    return text.replace(\"_\", \"-\")\n</pre> def snake_to_kebab(text):     return text.replace(\"_\", \"-\") In\u00a0[\u00a0]: Copied! <pre>def inherit_docstring(c, meth):\n    \"\"\"Since Python 3.5, inspect.getdoc is supposed to return the docstring from a parent class\n    if a class has none. However this doesn't seem to work for Cython classes.\n    \"\"\"\n\n    doc = None\n\n    for ancestor in inspect.getmro(c):\n        try:\n            ancestor_meth = getattr(ancestor, meth)\n        except AttributeError:\n            break\n        doc = inspect.getdoc(ancestor_meth)\n        if doc:\n            break\n\n    return doc\n</pre> def inherit_docstring(c, meth):     \"\"\"Since Python 3.5, inspect.getdoc is supposed to return the docstring from a parent class     if a class has none. However this doesn't seem to work for Cython classes.     \"\"\"      doc = None      for ancestor in inspect.getmro(c):         try:             ancestor_meth = getattr(ancestor, meth)         except AttributeError:             break         doc = inspect.getdoc(ancestor_meth)         if doc:             break      return doc In\u00a0[\u00a0]: Copied! <pre>def inherit_signature(c, method_name):\n    m = getattr(c, method_name)\n    try:\n        sig = inspect.signature(m)\n    except ValueError:\n        return inspect.Signature()\n\n    params = []\n\n    for param in sig.parameters.values():\n        if param.name == \"self\" or param.annotation is not param.empty:\n            params.append(param)\n            continue\n\n        for ancestor in inspect.getmro(c):\n            try:\n                ancestor_meth = inspect.signature(getattr(ancestor, m.__name__))\n            except AttributeError:\n                break\n            try:\n                ancestor_param = ancestor_meth.parameters[param.name]\n            except KeyError:\n                break\n            if ancestor_param.annotation is not param.empty:\n                param = param.replace(annotation=ancestor_param.annotation)\n                break\n\n        params.append(param)\n\n    return_annotation = sig.return_annotation\n    if return_annotation is inspect._empty:\n        for ancestor in inspect.getmro(c):\n            try:\n                ancestor_meth = inspect.signature(getattr(ancestor, m.__name__))\n            except AttributeError:\n                break\n            if ancestor_meth.return_annotation is not inspect._empty:\n                return_annotation = ancestor_meth.return_annotation\n                break\n\n    return sig.replace(parameters=params, return_annotation=return_annotation)\n</pre> def inherit_signature(c, method_name):     m = getattr(c, method_name)     try:         sig = inspect.signature(m)     except ValueError:         return inspect.Signature()      params = []      for param in sig.parameters.values():         if param.name == \"self\" or param.annotation is not param.empty:             params.append(param)             continue          for ancestor in inspect.getmro(c):             try:                 ancestor_meth = inspect.signature(getattr(ancestor, m.__name__))             except AttributeError:                 break             try:                 ancestor_param = ancestor_meth.parameters[param.name]             except KeyError:                 break             if ancestor_param.annotation is not param.empty:                 param = param.replace(annotation=ancestor_param.annotation)                 break          params.append(param)      return_annotation = sig.return_annotation     if return_annotation is inspect._empty:         for ancestor in inspect.getmro(c):             try:                 ancestor_meth = inspect.signature(getattr(ancestor, m.__name__))             except AttributeError:                 break             if ancestor_meth.return_annotation is not inspect._empty:                 return_annotation = ancestor_meth.return_annotation                 break      return sig.replace(parameters=params, return_annotation=return_annotation) In\u00a0[\u00a0]: Copied! <pre>def pascal_to_kebab(string):\n    string = re.sub(\"(.)([A-Z][a-z]+)\", r\"\\1-\\2\", string)\n    string = re.sub(\"(.)([0-9]+)\", r\"\\1-\\2\", string)\n    return re.sub(\"([a-z0-9])([A-Z])\", r\"\\1-\\2\", string).lower()\n</pre> def pascal_to_kebab(string):     string = re.sub(\"(.)([A-Z][a-z]+)\", r\"\\1-\\2\", string)     string = re.sub(\"(.)([0-9]+)\", r\"\\1-\\2\", string)     return re.sub(\"([a-z0-9])([A-Z])\", r\"\\1-\\2\", string).lower() In\u00a0[\u00a0]: Copied! <pre>class Linkifier:\n    def __init__(self):\n        path_index = {}\n        name_index = {}\n\n        modules = {\n            module: importlib.import_module(f\"{package}.{module}\")\n            for module in importlib.import_module(f\"{package}\").__all__\n        }\n\n        def index_module(mod_name, mod, path):\n            path = os.path.join(path, mod_name)\n            dotted_path = path.replace(\"/\", \".\")\n\n            for func_name, func in inspect.getmembers(mod, inspect.isfunction):\n                for e in (\n                    f\"{mod_name}.{func_name}\",\n                    f\"{dotted_path}.{func_name}\",\n                    f\"{func.__module__}.{func_name}\",\n                ):\n                    path_index[e] = os.path.join(path, snake_to_kebab(func_name))\n                    name_index[e] = f\"{dotted_path}.{func_name}\"\n\n            for klass_name, klass in inspect.getmembers(mod, inspect.isclass):\n                for e in (\n                    f\"{mod_name}.{klass_name}\",\n                    f\"{dotted_path}.{klass_name}\",\n                    f\"{klass.__module__}.{klass_name}\",\n                ):\n                    path_index[e] = os.path.join(path, klass_name)\n                    name_index[e] = f\"{dotted_path}.{klass_name}\"\n\n            for submod_name, submod in inspect.getmembers(mod, inspect.ismodule):\n                if submod_name not in mod.__all__ or submod_name == \"typing\":\n                    continue\n                for e in (f\"{mod_name}.{submod_name}\", f\"{dotted_path}.{submod_name}\"):\n                    path_index[e] = os.path.join(path, snake_to_kebab(submod_name))\n\n                # Recurse\n                index_module(submod_name, submod, path=path)\n\n        for mod_name, mod in modules.items():\n            index_module(mod_name, mod, path=\"\")\n\n        # Prepend {package} to each index entry\n        for k in list(path_index.keys()):\n            path_index[f\"{package}.{k}\"] = path_index[k]\n        for k in list(name_index.keys()):\n            name_index[f\"{package}.{k}\"] = name_index[k]\n\n        self.path_index = path_index\n        self.name_index = name_index\n\n    def linkify(self, text, use_fences, depth):\n        path = self.path_index.get(text)\n        name = self.name_index.get(text)\n        if path and name:\n            backwards = \"../\" * (depth + 1)\n            if use_fences:\n                return f\"[`{name}`]({backwards}{path})\"\n            return f\"[{name}]({backwards}{path})\"\n        return None\n\n    def linkify_fences(self, text, depth):\n        between_fences = re.compile(\"`[\\w\\.]+\\.\\w+`\")\n        return between_fences.sub(\n            lambda x: self.linkify(x.group().strip(\"`\"), True, depth) or x.group(), text\n        )\n\n    def linkify_dotted(self, text, depth):\n        dotted = re.compile(\"\\w+\\.[\\.\\w]+\")\n        return dotted.sub(\n            lambda x: self.linkify(x.group(), False, depth) or x.group(), text\n        )\n</pre> class Linkifier:     def __init__(self):         path_index = {}         name_index = {}          modules = {             module: importlib.import_module(f\"{package}.{module}\")             for module in importlib.import_module(f\"{package}\").__all__         }          def index_module(mod_name, mod, path):             path = os.path.join(path, mod_name)             dotted_path = path.replace(\"/\", \".\")              for func_name, func in inspect.getmembers(mod, inspect.isfunction):                 for e in (                     f\"{mod_name}.{func_name}\",                     f\"{dotted_path}.{func_name}\",                     f\"{func.__module__}.{func_name}\",                 ):                     path_index[e] = os.path.join(path, snake_to_kebab(func_name))                     name_index[e] = f\"{dotted_path}.{func_name}\"              for klass_name, klass in inspect.getmembers(mod, inspect.isclass):                 for e in (                     f\"{mod_name}.{klass_name}\",                     f\"{dotted_path}.{klass_name}\",                     f\"{klass.__module__}.{klass_name}\",                 ):                     path_index[e] = os.path.join(path, klass_name)                     name_index[e] = f\"{dotted_path}.{klass_name}\"              for submod_name, submod in inspect.getmembers(mod, inspect.ismodule):                 if submod_name not in mod.__all__ or submod_name == \"typing\":                     continue                 for e in (f\"{mod_name}.{submod_name}\", f\"{dotted_path}.{submod_name}\"):                     path_index[e] = os.path.join(path, snake_to_kebab(submod_name))                  # Recurse                 index_module(submod_name, submod, path=path)          for mod_name, mod in modules.items():             index_module(mod_name, mod, path=\"\")          # Prepend {package} to each index entry         for k in list(path_index.keys()):             path_index[f\"{package}.{k}\"] = path_index[k]         for k in list(name_index.keys()):             name_index[f\"{package}.{k}\"] = name_index[k]          self.path_index = path_index         self.name_index = name_index      def linkify(self, text, use_fences, depth):         path = self.path_index.get(text)         name = self.name_index.get(text)         if path and name:             backwards = \"../\" * (depth + 1)             if use_fences:                 return f\"[`{name}`]({backwards}{path})\"             return f\"[{name}]({backwards}{path})\"         return None      def linkify_fences(self, text, depth):         between_fences = re.compile(\"`[\\w\\.]+\\.\\w+`\")         return between_fences.sub(             lambda x: self.linkify(x.group().strip(\"`\"), True, depth) or x.group(), text         )      def linkify_dotted(self, text, depth):         dotted = re.compile(\"\\w+\\.[\\.\\w]+\")         return dotted.sub(             lambda x: self.linkify(x.group(), False, depth) or x.group(), text         ) In\u00a0[\u00a0]: Copied! <pre>def concat_lines(lines):\n    return inspect.cleandoc(\" \".join(\"\\n\\n\" if line == \"\" else line for line in lines))\n</pre> def concat_lines(lines):     return inspect.cleandoc(\" \".join(\"\\n\\n\" if line == \"\" else line for line in lines)) In\u00a0[\u00a0]: Copied! <pre>def print_docstring(obj, file, depth):\n    \"\"\"Prints a classes's docstring to a file.\"\"\"\n\n    doc = ClassDoc(obj) if inspect.isclass(obj) else FunctionDoc(obj)\n\n    printf = functools.partial(print, file=file)\n\n    printf(h1(obj.__name__))\n    printf(linkifier.linkify_fences(paragraph(concat_lines(doc[\"Summary\"])), depth))\n    printf(\n        linkifier.linkify_fences(\n            paragraph(concat_lines(doc[\"Extended Summary\"])), depth\n        )\n    )\n\n    # We infer the type annotations from the signatures, and therefore rely on the signature\n    # instead of the docstring for documenting parameters\n    try:\n        signature = inspect.signature(obj)\n    except ValueError:\n        signature = (\n            inspect.Signature()\n        )  # TODO: this is necessary for Cython classes, but it's not correct\n    params_desc = {param.name: \" \".join(param.desc) for param in doc[\"Parameters\"]}\n\n    # Parameters\n    if signature.parameters:\n        printf(h2(\"Parameters\"))\n    for param in signature.parameters.values():\n        # Name\n        printf(f\"- **{param.name}**\", end=\"\")\n        # Type annotation\n        if param.annotation is not param.empty:\n            anno = inspect.formatannotation(param.annotation)\n            anno = linkifier.linkify_dotted(anno, depth)\n            printf(f\" (*{anno}*)\", end=\"\")\n        # Default value\n        if param.default is not param.empty:\n            printf(f\" \u2013 defaults to `{param.default}`\", end=\"\")\n        printf(\"\\n\", file=file)\n        # Description\n        if param.name in params_desc:\n            desc = params_desc[param.name]\n            if desc:\n                printf(f\"    {desc}\\n\")\n    printf(\"\")\n\n    # Attributes\n    if doc[\"Attributes\"]:\n        printf(h2(\"Attributes\"))\n    for attr in doc[\"Attributes\"]:\n        # Name\n        printf(f\"- **{attr.name}**\", end=\"\")\n        # Type annotation\n        if attr.type:\n            printf(f\" (*{attr.type}*)\", end=\"\")\n        printf(\"\\n\", file=file)\n        # Description\n        desc = \" \".join(attr.desc)\n        if desc:\n            printf(f\"    {desc}\\n\")\n    printf(\"\")\n\n    # Examples\n    if doc[\"Examples\"]:\n        printf(h2(\"Examples\"))\n\n        in_code = False\n        after_space = False\n\n        for line in inspect.cleandoc(\"\\n\".join(doc[\"Examples\"])).splitlines():\n            if (\n                in_code\n                and after_space\n                and line\n                and not line.startswith(\"&gt;&gt;&gt;\")\n                and not line.startswith(\"...\")\n            ):\n                printf(\"```\\n\")\n                in_code = False\n                after_space = False\n\n            if not in_code and line.startswith(\"&gt;&gt;&gt;\"):\n                printf(\"```python\")\n                in_code = True\n\n            after_space = False\n            if not line:\n                after_space = True\n\n            printf(line)\n\n        if in_code:\n            printf(\"```\")\n    printf(\"\")\n\n    # Methods\n    if inspect.isclass(obj) and doc[\"Methods\"]:\n        printf(h2(\"Methods\"))\n        printf_indent = lambda x, **kwargs: printf(f\"    {x}\", **kwargs)  # noqa: E731\n\n        for meth in doc[\"Methods\"]:\n            printf(paragraph(f'???- note \"{meth.name}\"'))\n\n            # Parse method docstring\n            docstring = inherit_docstring(c=obj, meth=meth.name)\n            if not docstring:\n                continue\n            meth_doc = FunctionDoc(func=None, doc=docstring)\n\n            printf_indent(paragraph(\" \".join(meth_doc[\"Summary\"])))\n            if meth_doc[\"Extended Summary\"]:\n                printf_indent(paragraph(\" \".join(meth_doc[\"Extended Summary\"])))\n\n            # We infer the type annotations from the signatures, and therefore rely on the signature\n            # instead of the docstring for documenting parameters\n            signature = inherit_signature(obj, meth.name)\n            params_desc = {\n                param.name: \" \".join(param.desc) for param in doc[\"Parameters\"]\n            }\n\n            # Parameters\n            if (\n                len(signature.parameters) &gt; 1\n            ):  # signature is never empty, but self doesn't count\n                printf_indent(\"**Parameters**\\n\")\n            for param in signature.parameters.values():\n                if param.name == \"self\":\n                    continue\n                # Name\n                printf_indent(f\"- **{param.name}**\", end=\"\")\n                # Type annotation\n                if param.annotation is not param.empty:\n                    printf_indent(\n                        f\" (*{inspect.formatannotation(param.annotation)}*)\", end=\"\"\n                    )\n                # Default value\n                if param.default is not param.empty:\n                    printf_indent(f\" \u2013 defaults to `{param.default}`\", end=\"\")\n                printf_indent(\"\", file=file)\n                # Description\n                desc = params_desc.get(param.name)\n                if desc:\n                    printf_indent(f\"    {desc}\")\n            printf_indent(\"\")\n\n            # Returns\n            if meth_doc[\"Returns\"]:\n                printf_indent(\"**Returns**\\n\")\n                return_val = meth_doc[\"Returns\"][0]\n                if signature.return_annotation is not inspect._empty:\n                    if inspect.isclass(signature.return_annotation):\n                        printf_indent(\n                            f\"*{signature.return_annotation.__name__}*: \", end=\"\"\n                        )\n                    else:\n                        printf_indent(f\"*{signature.return_annotation}*: \", end=\"\")\n                printf_indent(return_val.type)\n                printf_indent(\"\")\n\n    # Notes\n    if doc[\"Notes\"]:\n        printf(h2(\"Notes\"))\n        printf(paragraph(\"\\n\".join(doc[\"Notes\"])))\n\n    # References\n    if doc[\"References\"]:\n        printf(h2(\"References\"))\n        printf(paragraph(\"\\n\".join(doc[\"References\"])))\n</pre> def print_docstring(obj, file, depth):     \"\"\"Prints a classes's docstring to a file.\"\"\"      doc = ClassDoc(obj) if inspect.isclass(obj) else FunctionDoc(obj)      printf = functools.partial(print, file=file)      printf(h1(obj.__name__))     printf(linkifier.linkify_fences(paragraph(concat_lines(doc[\"Summary\"])), depth))     printf(         linkifier.linkify_fences(             paragraph(concat_lines(doc[\"Extended Summary\"])), depth         )     )      # We infer the type annotations from the signatures, and therefore rely on the signature     # instead of the docstring for documenting parameters     try:         signature = inspect.signature(obj)     except ValueError:         signature = (             inspect.Signature()         )  # TODO: this is necessary for Cython classes, but it's not correct     params_desc = {param.name: \" \".join(param.desc) for param in doc[\"Parameters\"]}      # Parameters     if signature.parameters:         printf(h2(\"Parameters\"))     for param in signature.parameters.values():         # Name         printf(f\"- **{param.name}**\", end=\"\")         # Type annotation         if param.annotation is not param.empty:             anno = inspect.formatannotation(param.annotation)             anno = linkifier.linkify_dotted(anno, depth)             printf(f\" (*{anno}*)\", end=\"\")         # Default value         if param.default is not param.empty:             printf(f\" \u2013 defaults to `{param.default}`\", end=\"\")         printf(\"\\n\", file=file)         # Description         if param.name in params_desc:             desc = params_desc[param.name]             if desc:                 printf(f\"    {desc}\\n\")     printf(\"\")      # Attributes     if doc[\"Attributes\"]:         printf(h2(\"Attributes\"))     for attr in doc[\"Attributes\"]:         # Name         printf(f\"- **{attr.name}**\", end=\"\")         # Type annotation         if attr.type:             printf(f\" (*{attr.type}*)\", end=\"\")         printf(\"\\n\", file=file)         # Description         desc = \" \".join(attr.desc)         if desc:             printf(f\"    {desc}\\n\")     printf(\"\")      # Examples     if doc[\"Examples\"]:         printf(h2(\"Examples\"))          in_code = False         after_space = False          for line in inspect.cleandoc(\"\\n\".join(doc[\"Examples\"])).splitlines():             if (                 in_code                 and after_space                 and line                 and not line.startswith(\"&gt;&gt;&gt;\")                 and not line.startswith(\"...\")             ):                 printf(\"```\\n\")                 in_code = False                 after_space = False              if not in_code and line.startswith(\"&gt;&gt;&gt;\"):                 printf(\"```python\")                 in_code = True              after_space = False             if not line:                 after_space = True              printf(line)          if in_code:             printf(\"```\")     printf(\"\")      # Methods     if inspect.isclass(obj) and doc[\"Methods\"]:         printf(h2(\"Methods\"))         printf_indent = lambda x, **kwargs: printf(f\"    {x}\", **kwargs)  # noqa: E731          for meth in doc[\"Methods\"]:             printf(paragraph(f'???- note \"{meth.name}\"'))              # Parse method docstring             docstring = inherit_docstring(c=obj, meth=meth.name)             if not docstring:                 continue             meth_doc = FunctionDoc(func=None, doc=docstring)              printf_indent(paragraph(\" \".join(meth_doc[\"Summary\"])))             if meth_doc[\"Extended Summary\"]:                 printf_indent(paragraph(\" \".join(meth_doc[\"Extended Summary\"])))              # We infer the type annotations from the signatures, and therefore rely on the signature             # instead of the docstring for documenting parameters             signature = inherit_signature(obj, meth.name)             params_desc = {                 param.name: \" \".join(param.desc) for param in doc[\"Parameters\"]             }              # Parameters             if (                 len(signature.parameters) &gt; 1             ):  # signature is never empty, but self doesn't count                 printf_indent(\"**Parameters**\\n\")             for param in signature.parameters.values():                 if param.name == \"self\":                     continue                 # Name                 printf_indent(f\"- **{param.name}**\", end=\"\")                 # Type annotation                 if param.annotation is not param.empty:                     printf_indent(                         f\" (*{inspect.formatannotation(param.annotation)}*)\", end=\"\"                     )                 # Default value                 if param.default is not param.empty:                     printf_indent(f\" \u2013 defaults to `{param.default}`\", end=\"\")                 printf_indent(\"\", file=file)                 # Description                 desc = params_desc.get(param.name)                 if desc:                     printf_indent(f\"    {desc}\")             printf_indent(\"\")              # Returns             if meth_doc[\"Returns\"]:                 printf_indent(\"**Returns**\\n\")                 return_val = meth_doc[\"Returns\"][0]                 if signature.return_annotation is not inspect._empty:                     if inspect.isclass(signature.return_annotation):                         printf_indent(                             f\"*{signature.return_annotation.__name__}*: \", end=\"\"                         )                     else:                         printf_indent(f\"*{signature.return_annotation}*: \", end=\"\")                 printf_indent(return_val.type)                 printf_indent(\"\")      # Notes     if doc[\"Notes\"]:         printf(h2(\"Notes\"))         printf(paragraph(\"\\n\".join(doc[\"Notes\"])))      # References     if doc[\"References\"]:         printf(h2(\"References\"))         printf(paragraph(\"\\n\".join(doc[\"References\"]))) In\u00a0[\u00a0]: Copied! <pre>def print_module(mod, path, overview, is_submodule=False):\n    mod_name = mod.__name__.split(\".\")[-1]\n\n    # Create a directory for the module\n    mod_slug = snake_to_kebab(mod_name)\n    mod_path = path.joinpath(mod_slug)\n    mod_short_path = str(mod_path).replace(\"docs/api/\", \"\")\n    os.makedirs(mod_path, exist_ok=True)\n    with open(mod_path.joinpath(\".pages\"), \"w\") as f:\n        f.write(f\"title: {mod_name}\")\n\n    # Add the module to the overview\n    if is_submodule:\n        print(h3(mod_name), file=overview)\n    else:\n        print(h2(mod_name), file=overview)\n    if mod.__doc__:\n        print(paragraph(mod.__doc__), file=overview)\n\n    # Extract all public classes and functions\n    ispublic = lambda x: x.__name__ in mod.__all__ and not x.__name__.startswith(\"_\")  # noqa: E731\n    classes = inspect.getmembers(mod, lambda x: inspect.isclass(x) and ispublic(x))\n    funcs = inspect.getmembers(mod, lambda x: inspect.isfunction(x) and ispublic(x))\n\n    # Classes\n\n    if classes and funcs:\n        print(\"\\n**Classes**\\n\", file=overview)\n\n    for _, c in classes:\n        print(f\"{mod_name}.{c.__name__}\")\n\n        # Add the class to the overview\n        slug = snake_to_kebab(c.__name__)\n        print(\n            li(link(c.__name__, f\"../{mod_short_path}/{slug}\")), end=\"\", file=overview\n        )\n\n        # Write down the class' docstring\n        with open(mod_path.joinpath(slug).with_suffix(\".md\"), \"w\") as file:\n            print_docstring(obj=c, file=file, depth=mod_short_path.count(\"/\") + 1)\n\n    # Functions\n\n    if classes and funcs:\n        print(\"\\n**Functions**\\n\", file=overview)\n\n    for _, f in funcs:\n        print(f\"{mod_name}.{f.__name__}\")\n\n        # Add the function to the overview\n        slug = snake_to_kebab(f.__name__)\n        print(\n            li(link(f.__name__, f\"../{mod_short_path}/{slug}\")), end=\"\", file=overview\n        )\n\n        # Write down the function' docstring\n        with open(mod_path.joinpath(slug).with_suffix(\".md\"), \"w\") as file:\n            print_docstring(obj=f, file=file, depth=mod_short_path.count(\".\") + 1)\n\n    # Sub-modules\n    for name, submod in inspect.getmembers(mod, inspect.ismodule):\n        # We only want to go through the public submodules, such as optim.schedulers\n        if (\n            name in (\"tags\", \"typing\", \"inspect\", \"skmultiflow_utils\")\n            or name not in mod.__all__\n            or name.startswith(\"_\")\n        ):\n            continue\n        print_module(mod=submod, path=mod_path, overview=overview, is_submodule=True)\n\n    print(\"\", file=overview)\n</pre> def print_module(mod, path, overview, is_submodule=False):     mod_name = mod.__name__.split(\".\")[-1]      # Create a directory for the module     mod_slug = snake_to_kebab(mod_name)     mod_path = path.joinpath(mod_slug)     mod_short_path = str(mod_path).replace(\"docs/api/\", \"\")     os.makedirs(mod_path, exist_ok=True)     with open(mod_path.joinpath(\".pages\"), \"w\") as f:         f.write(f\"title: {mod_name}\")      # Add the module to the overview     if is_submodule:         print(h3(mod_name), file=overview)     else:         print(h2(mod_name), file=overview)     if mod.__doc__:         print(paragraph(mod.__doc__), file=overview)      # Extract all public classes and functions     ispublic = lambda x: x.__name__ in mod.__all__ and not x.__name__.startswith(\"_\")  # noqa: E731     classes = inspect.getmembers(mod, lambda x: inspect.isclass(x) and ispublic(x))     funcs = inspect.getmembers(mod, lambda x: inspect.isfunction(x) and ispublic(x))      # Classes      if classes and funcs:         print(\"\\n**Classes**\\n\", file=overview)      for _, c in classes:         print(f\"{mod_name}.{c.__name__}\")          # Add the class to the overview         slug = snake_to_kebab(c.__name__)         print(             li(link(c.__name__, f\"../{mod_short_path}/{slug}\")), end=\"\", file=overview         )          # Write down the class' docstring         with open(mod_path.joinpath(slug).with_suffix(\".md\"), \"w\") as file:             print_docstring(obj=c, file=file, depth=mod_short_path.count(\"/\") + 1)      # Functions      if classes and funcs:         print(\"\\n**Functions**\\n\", file=overview)      for _, f in funcs:         print(f\"{mod_name}.{f.__name__}\")          # Add the function to the overview         slug = snake_to_kebab(f.__name__)         print(             li(link(f.__name__, f\"../{mod_short_path}/{slug}\")), end=\"\", file=overview         )          # Write down the function' docstring         with open(mod_path.joinpath(slug).with_suffix(\".md\"), \"w\") as file:             print_docstring(obj=f, file=file, depth=mod_short_path.count(\".\") + 1)      # Sub-modules     for name, submod in inspect.getmembers(mod, inspect.ismodule):         # We only want to go through the public submodules, such as optim.schedulers         if (             name in (\"tags\", \"typing\", \"inspect\", \"skmultiflow_utils\")             or name not in mod.__all__             or name.startswith(\"_\")         ):             continue         print_module(mod=submod, path=mod_path, overview=overview, is_submodule=True)      print(\"\", file=overview) In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    api_path = pathlib.Path(\"docs/api\")\n\n    # Create a directory for the API reference\n    shutil.rmtree(api_path, ignore_errors=True)\n    os.makedirs(api_path, exist_ok=True)\n    with open(api_path.joinpath(\".pages\"), \"w\") as f:\n        f.write(\"title: API reference\\narrange:\\n  - overview.md\\n  - ...\\n\")\n\n    overview = open(api_path.joinpath(\"overview.md\"), \"w\")\n    print(h1(\"Overview\"), file=overview)\n\n    linkifier = Linkifier()\n\n    for mod_name, mod in inspect.getmembers(\n        importlib.import_module(f\"{package}\"), inspect.ismodule\n    ):\n        if mod_name.startswith(\"_\"):\n            continue\n        print(mod_name)\n        print_module(mod, path=api_path, overview=overview)\n</pre> if __name__ == \"__main__\":     api_path = pathlib.Path(\"docs/api\")      # Create a directory for the API reference     shutil.rmtree(api_path, ignore_errors=True)     os.makedirs(api_path, exist_ok=True)     with open(api_path.joinpath(\".pages\"), \"w\") as f:         f.write(\"title: API reference\\narrange:\\n  - overview.md\\n  - ...\\n\")      overview = open(api_path.joinpath(\"overview.md\"), \"w\")     print(h1(\"Overview\"), file=overview)      linkifier = Linkifier()      for mod_name, mod in inspect.getmembers(         importlib.import_module(f\"{package}\"), inspect.ismodule     ):         if mod_name.startswith(\"_\"):             continue         print(mod_name)         print_module(mod, path=api_path, overview=overview)"}]}