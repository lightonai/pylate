{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"PyLate <p>Flexible Training and Retrieval for Late Interaction Models</p> <p></p> <p> PyLate is a library built on top of Sentence Transformers, designed to simplify and optimize fine-tuning, inference, and retrieval with state-of-the-art ColBERT models. It enables easy fine-tuning on both single and multiple GPUs, providing flexibility for various hardware setups. PyLate also streamlines document retrieval and allows you to load a wide range of models, enabling you to construct ColBERT models from most pre-trained language models. </p>"},{"location":"#installation","title":"Installation","text":"<p>You can install PyLate using pip:</p> <pre><code>pip install pylate\n</code></pre> <p>For evaluation dependencies, use:</p> <pre><code>pip install \"pylate[eval]\"\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<p>The complete documentation is available here, which includes in-depth guides, examples, and API references.</p>"},{"location":"#training","title":"Training","text":""},{"location":"#contrastive-training","title":"Contrastive training","text":"<p>Here\u2019s a simple example of training a ColBERT model on the MS MARCO dataset triplet dataset using PyLate. This script demonstrates training with contrastive loss and evaluating the model on a held-out eval set:</p> <pre><code>import torch\nfrom datasets import load_dataset\nfrom sentence_transformers import (\n    SentenceTransformerTrainer,\n    SentenceTransformerTrainingArguments,\n)\n\nfrom pylate import evaluation, losses, models, utils\n\n# Define model parameters for contrastive training\nmodel_name = \"bert-base-uncased\"  # Choose the pre-trained model you want to use as base\nbatch_size = 32  # Larger batch size often improves results, but requires more memory\n\nnum_train_epochs = 1  # Adjust based on your requirements\n# Set the run name for logging and output directory\nrun_name = \"contrastive-bert-base-uncased\"\noutput_dir = f\"output/{run_name}\"\n\n# 1. Here we define our ColBERT model. If not a ColBERT model, will add a linear layer to the base encoder.\nmodel = models.ColBERT(model_name_or_path=model_name)\n\n# Compiling the model makes the training faster\nmodel = torch.compile(model)\n\n# Load dataset\ndataset = load_dataset(\"sentence-transformers/msmarco-bm25\", \"triplet\", split=\"train\")\n# Split the dataset (this dataset does not have a validation set, so we split the training set)\nsplits = dataset.train_test_split(test_size=0.01)\ntrain_dataset = splits[\"train\"]\neval_dataset = splits[\"test\"]\n\n# Define the loss function\ntrain_loss = losses.Contrastive(model=model)\n\n# Initialize the evaluator\ndev_evaluator = evaluation.ColBERTTripletEvaluator(\n    anchors=eval_dataset[\"query\"],\n    positives=eval_dataset[\"positive\"],\n    negatives=eval_dataset[\"negative\"],\n)\n\n# Configure the training arguments (e.g., batch size, evaluation strategy, logging steps)\nargs = SentenceTransformerTrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=num_train_epochs,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    fp16=True,  # Set to False if you get an error that your GPU can't run on FP16\n    bf16=False,  # Set to True if you have a GPU that supports BF16\n    run_name=run_name,  # Will be used in W&amp;B if `wandb` is installed\n    learning_rate=3e-6,\n)\n\n# Initialize the trainer for the contrastive training\ntrainer = SentenceTransformerTrainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    loss=train_loss,\n    evaluator=dev_evaluator,\n    data_collator=utils.ColBERTCollator(model.tokenize),\n)\n# Start the training process\ntrainer.train()\n</code></pre> <p>After training, the model can be loaded using the output directory path:</p> <pre><code>from pylate import models\n\nmodel = models.ColBERT(model_name_or_path=\"contrastive-bert-base-uncased\")\n</code></pre> <p>Please note that temperature parameter has a very high importance in contrastive learning, and a temperature around 0.02 is often used in the literature:</p> <pre><code>train_loss = losses.Contrastive(model=model, temperature=0.02)\n</code></pre> <p>As contrastive learning is not compatible with gradient accumulation, you can leverage GradCache to emulate bigger batch sizes without requiring more memory by using the <code>CachedContrastiveLoss</code> to define a mini_batch_size while increasing the <code>per_device_train_batch_size</code>:</p> <pre><code>train_loss = losses.CachedContrastive(\n        model=model, mini_batch_size=mini_batch_size\n)\n</code></pre> <p>Finally, if you are in a multi-GPU setting, you can gather all the elements from the different GPUs to create even bigger batch sizes by setting <code>gather_across_devices</code> to <code>True</code> (for both <code>Contrastive</code> and <code>CachedContrastive</code> losses):</p> <pre><code>train_loss = losses.Contrastive(model=model, gather_across_devices=True)\n</code></pre>"},{"location":"#knowledge-distillation","title":"Knowledge distillation","text":"<p>To get the best performance when training a ColBERT model, you should use knowledge distillation to train the model using the scores of a strong teacher model. Here's a simple example of how to train a model using knowledge distillation in PyLate on MS MARCO:</p> <pre><code>import torch\nfrom datasets import load_dataset\nfrom sentence_transformers import (\n    SentenceTransformerTrainer,\n    SentenceTransformerTrainingArguments,\n)\n\nfrom pylate import losses, models, utils\n\n# Load the datasets required for knowledge distillation (train, queries, documents)\ntrain = load_dataset(\n    path=\"lightonai/ms-marco-en-bge\",\n    name=\"train\",\n)\n\nqueries = load_dataset(\n    path=\"lightonai/ms-marco-en-bge\",\n    name=\"queries\",\n)\n\ndocuments = load_dataset(\n    path=\"lightonai/ms-marco-en-bge\",\n    name=\"documents\",\n)\n\n# Set the transformation to load the documents/queries texts using the corresponding ids on the fly\ntrain.set_transform(\n    utils.KDProcessing(queries=queries, documents=documents).transform,\n)\n\n# Define the base model, training parameters, and output directory\nmodel_name = \"bert-base-uncased\"  # Choose the pre-trained model you want to use as base\nbatch_size = 16\nnum_train_epochs = 1\n# Set the run name for logging and output directory\nrun_name = \"knowledge-distillation-bert-base\"\noutput_dir = f\"output/{run_name}\"\n\n# Initialize the ColBERT model from the base model\nmodel = models.ColBERT(model_name_or_path=model_name)\n\n# Compiling the model to make the training faster\nmodel = torch.compile(model)\n\n# Configure the training arguments (e.g., epochs, batch size, learning rate)\nargs = SentenceTransformerTrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=num_train_epochs,\n    per_device_train_batch_size=batch_size,\n    fp16=True,  # Set to False if you get an error that your GPU can't run on FP16\n    bf16=False,  # Set to True if you have a GPU that supports BF16\n    run_name=run_name,\n    learning_rate=1e-5,\n)\n\n# Use the Distillation loss function for training\ntrain_loss = losses.Distillation(model=model)\n\n# Initialize the trainer\ntrainer = SentenceTransformerTrainer(\n    model=model,\n    args=args,\n    train_dataset=train,\n    loss=train_loss,\n    data_collator=utils.ColBERTCollator(tokenize_fn=model.tokenize),\n)\n\n# Start the training process\ntrainer.train()\n</code></pre>"},{"location":"#nanobeir-evaluator","title":"NanoBEIR evaluator","text":"<p>If you are training an English retrieval model, you can use NanoBEIR evaluator, which allows to run small version of BEIR to get quick validation results.</p> <pre><code>evaluator=evaluation.NanoBEIREvaluator(),\n</code></pre>"},{"location":"#datasets","title":"Datasets","text":"<p>PyLate supports Hugging Face Datasets, enabling seamless triplet / knowledge distillation based training. For contrastive training, you can use any of the existing sentence transformers triplet datasets. Below is an example of creating a custom triplet dataset for training:</p> <pre><code>from datasets import Dataset\n\ndataset = [\n    {\n        \"query\": \"example query 1\",\n        \"positive\": \"example positive document 1\",\n        \"negative\": \"example negative document 1\",\n    },\n    {\n        \"query\": \"example query 2\",\n        \"positive\": \"example positive document 2\",\n        \"negative\": \"example negative document 2\",\n    },\n    {\n        \"query\": \"example query 3\",\n        \"positive\": \"example positive document 3\",\n        \"negative\": \"example negative document 3\",\n    },\n]\n\ndataset = Dataset.from_list(mapping=dataset)\n\ntrain_dataset, test_dataset = dataset.train_test_split(test_size=0.3)\n</code></pre> <p>Note that PyLate supports more than one negative per query, simply add the additional negatives after the first one in the row.</p> <pre><code>{\n        \"query\": \"example query 1\",\n        \"positive\": \"example positive document 1\",\n        \"negative_1\": \"example negative document 1\",\n        \"negative_2\": \"example negative document 2\",\n}\n</code></pre> <p>To create a knowledge distillation dataset, you can use the following snippet:</p> <pre><code>from datasets import Dataset\n\ndataset = [\n    {\n        \"query_id\": 54528,\n        \"document_ids\": [\n            6862419,\n            335116,\n            339186,\n        ],\n        \"scores\": [\n            0.4546215673141326,\n            0.6575686537173476,\n            0.26825184192900203,\n        ],\n    },\n    {\n        \"query_id\": 749480,\n        \"document_ids\": [\n            6862419,\n            335116,\n            339186,\n        ],\n        \"scores\": [\n            0.2546215673141326,\n            0.7575686537173476,\n            0.96825184192900203,\n        ],\n    },\n]\n\n\ndataset = Dataset.from_list(mapping=dataset)\n\ndocuments = [\n    {\"document_id\": 6862419, \"text\": \"example doc 1\"},\n    {\"document_id\": 335116, \"text\": \"example doc 2\"},\n    {\"document_id\": 339186, \"text\": \"example doc 3\"},\n]\n\nqueries = [\n    {\"query_id\": 749480, \"text\": \"example query\"},\n]\n\ndocuments = Dataset.from_list(mapping=documents)\n\nqueries = Dataset.from_list(mapping=queries)\n</code></pre>"},{"location":"#retrieve","title":"Retrieve","text":"<p>PyLate allows easy retrieval of top documents for a given query set using the trained ColBERT model and PLAID index, simply load the model and init the index:</p> <pre><code>from pylate import indexes, models, retrieve\n\nmodel = models.ColBERT(\n    model_name_or_path=\"lightonai/GTE-ModernColBERT-v1\",\n)\n\nindex = indexes.PLAID(\n    index_folder=\"pylate-index\",\n    index_name=\"index\",\n    override=True,\n)\n\nretriever = retrieve.ColBERT(index=index)\n</code></pre> <p>Once the model and index are set up, we can add documents to the index using their embeddings and corresponding ids:</p> <pre><code>documents_ids = [\"1\", \"2\", \"3\"]\n\ndocuments = [\n    \"ColBERT\u2019s late-interaction keeps token-level embeddings to deliver cross-encoder-quality ranking at near-bi-encoder speed, enabling fine-grained relevance, robustness across domains, and hardware-friendly scalable search.\",\n\n    \"PLAID compresses ColBERT token vectors via product quantization to shrink storage by 10\u00d7, uses two-stage centroid scoring for sub-200 ms latency, and plugs directly into existing ColBERT pipelines.\",\n\n    \"PyLate is a library built on top of Sentence Transformers, designed to simplify and optimize fine-tuning, inference, and retrieval with state-of-the-art ColBERT models. It enables easy fine-tuning on both single and multiple GPUs, providing flexibility for various hardware setups. PyLate also streamlines document retrieval and allows you to load a wide range of models, enabling you to construct ColBERT models from most pre-trained language models.\",\n]\n\n# Encode the documents\ndocuments_embeddings = model.encode(\n    documents,\n    batch_size=32,\n    is_query=False, # Encoding documents\n    show_progress_bar=True,\n)\n\n# Add the documents ids and embeddings to the PLAID index\nindex.add_documents(\n    documents_ids=documents_ids,\n    documents_embeddings=documents_embeddings,\n)\n</code></pre> <p>Then we can retrieve the top-k documents for a given set of queries:</p> <pre><code>queries_embeddings = model.encode(\n    [\"query for document 3\", \"query for document 1\"],\n    batch_size=32,\n    is_query=True, # Encoding queries\n    show_progress_bar=True,\n)\n\nscores = retriever.retrieve(\n    queries_embeddings=queries_embeddings,\n    k=10,\n)\n\nprint(scores)\n</code></pre> <p>Sample Output:</p> <pre><code>[\n    [\n        {\"id\": \"3\", \"score\": 11.266985893249512},\n        {\"id\": \"1\", \"score\": 10.303335189819336},\n        {\"id\": \"2\", \"score\": 9.502392768859863},\n    ],\n    [\n        {\"id\": \"1\", \"score\": 10.88800048828125},\n        {\"id\": \"3\", \"score\": 9.950843811035156},\n        {\"id\": \"2\", \"score\": 9.602447509765625},\n    ],\n]\n</code></pre>"},{"location":"#rerank","title":"Rerank","text":"<p>If you only want to use the ColBERT model to perform reranking on top of your first-stage retrieval pipeline without building an index, you can simply use rank function and pass the queries and documents to rerank:</p> <pre><code>from pylate import rank\n\nqueries = [\n    \"query A\",\n    \"query B\",\n]\ndocuments = [\n    [\"document A\", \"document B\"],\n    [\"document 1\", \"document C\", \"document B\"],\n]\ndocuments_ids = [\n    [1, 2],\n    [1, 3, 2],\n]\n\nqueries_embeddings = model.encode(\n    queries,\n    is_query=True,\n)\ndocuments_embeddings = model.encode(\n    documents,\n    is_query=False,\n)\n\nreranked_documents = rank.rerank(\n    documents_ids=documents_ids,\n    queries_embeddings=queries_embeddings,\n    documents_embeddings=documents_embeddings,\n)\n</code></pre>"},{"location":"#contributing","title":"Contributing","text":"<p>We welcome contributions! To get started:</p> <ol> <li>Install the development dependencies:</li> </ol> <pre><code>pip install \"pylate[dev]\"\n</code></pre> <ol> <li>Run tests:</li> </ol> <pre><code>make test\n</code></pre> <ol> <li>Format code with Ruff:</li> </ol> <pre><code>make lint\n</code></pre>"},{"location":"#citation","title":"Citation","text":"<p>You can refer to the library with this BibTeX:</p> <pre><code>@misc{PyLate,\n  title={PyLate: Flexible Training and Retrieval for Late Interaction Models},\n  author={Chaffin, Antoine and Sourty, Rapha\u00ebl},\n  url={https://github.com/lightonai/pylate},\n  year={2024}\n}\n</code></pre>"},{"location":"#deepwiki","title":"DeepWiki","text":"<p>PyLate is indexed on DeepWiki so you can ask questions to LLMs using Deep Research to explore the codebase and get help to add new features.</p>"},{"location":"api/overview/","title":"Overview","text":""},{"location":"api/overview/#evaluation","title":"evaluation","text":"<p>Classes</p> <ul> <li>ColBERTDistillationEvaluator</li> <li>ColBERTTripletEvaluator</li> <li>NanoBEIREvaluator</li> <li>PyLateInformationRetrievalEvaluator</li> </ul> <p>Functions</p> <ul> <li>evaluate</li> <li>get_beir_triples</li> <li>load_beir</li> <li>load_custom_dataset</li> </ul>"},{"location":"api/overview/#hf_hub","title":"hf_hub","text":"<ul> <li>PylateModelCardData</li> </ul>"},{"location":"api/overview/#indexes","title":"indexes","text":"<ul> <li>PLAID</li> <li>Voyager</li> </ul>"},{"location":"api/overview/#losses","title":"losses","text":"<ul> <li>CachedContrastive</li> <li>Contrastive</li> <li>Distillation</li> </ul>"},{"location":"api/overview/#models","title":"models","text":"<ul> <li>ColBERT</li> <li>Dense</li> </ul>"},{"location":"api/overview/#rank","title":"rank","text":"<ul> <li>rerank</li> </ul>"},{"location":"api/overview/#retrieve","title":"retrieve","text":"<ul> <li>ColBERT</li> </ul>"},{"location":"api/overview/#scores","title":"scores","text":"<p>Classes</p> <ul> <li>SimilarityFunction</li> </ul> <p>Functions</p> <ul> <li>colbert_kd_scores</li> <li>colbert_scores</li> <li>colbert_scores_pairwise</li> </ul>"},{"location":"api/overview/#utils","title":"utils","text":"<p>Classes</p> <ul> <li>ColBERTCollator</li> <li>KDProcessing</li> </ul> <p>Functions</p> <ul> <li>all_gather</li> <li>all_gather_with_gradients</li> <li>convert_to_tensor</li> <li>get_rank</li> <li>get_world_size</li> <li>iter_batch</li> </ul>"},{"location":"api/evaluation/ColBERTDistillationEvaluator/","title":"ColBERTDistillationEvaluator","text":"<p>ColBERT Distillation Evaluator. This class is used to monitor the distillation process of a ColBERT model.</p>"},{"location":"api/evaluation/ColBERTDistillationEvaluator/#parameters","title":"Parameters","text":"<ul> <li> <p>queries ('list[str]')</p> <p>Set of queries.</p> </li> <li> <p>documents ('list[list[str]]')</p> <p>Set of documents. Each query has a list of documents. Each document is a list of strings. Number of documents should be the same for each query.</p> </li> <li> <p>scores ('list[list[float]]')</p> <p>The scores associated with the documents. Each query / documents pairs has a list of scores.</p> </li> <li> <p>name ('str') \u2013 defaults to ``</p> <p>The name of the evaluator.</p> </li> <li> <p>batch_size ('int') \u2013 defaults to <code>16</code></p> <p>The batch size.</p> </li> <li> <p>show_progress_bar ('bool') \u2013 defaults to <code>False</code></p> <p>Whether to show the progress bar.</p> </li> <li> <p>write_csv ('bool') \u2013 defaults to <code>True</code></p> <p>Whether to write the results to a CSV file.</p> </li> <li> <p>truncate_dim ('int | None') \u2013 defaults to <code>None</code></p> <p>The dimension to truncate the embeddings.</p> </li> <li> <p>normalize_scores ('bool') \u2013 defaults to <code>True</code></p> </li> </ul>"},{"location":"api/evaluation/ColBERTDistillationEvaluator/#attributes","title":"Attributes","text":"<ul> <li> <p>description</p> <p>Returns a human-readable description of the evaluator: BinaryClassificationEvaluator -&gt; Binary Classification  1. Replace \"CE\" prefix with \"CrossEncoder\" 2. Remove \"Evaluator\" from the class name 3. Add a space before every capital letter</p> </li> </ul>"},{"location":"api/evaluation/ColBERTDistillationEvaluator/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from pylate import models, evaluation\n\n&gt;&gt;&gt; model = models.ColBERT(\n...     model_name_or_path=\"sentence-transformers/all-MiniLM-L6-v2\", device=\"cpu\"\n... )\n\n&gt;&gt;&gt; queries = [\n...     \"query A\",\n...     \"query B\",\n... ]\n\n&gt;&gt;&gt; documents = [\n...     [\"document A\", \"document B\", \"document C\"],\n...     [\"document C C\", \"document B B\", \"document A A\"],\n... ]\n\n&gt;&gt;&gt; scores = [\n...     [0.9, 0.1, 0.05],\n...     [0.05, 0.9, 0.1],\n... ]\n\n&gt;&gt;&gt; distillation_evaluator = evaluation.ColBERTDistillationEvaluator(\n...     queries=queries,\n...     documents=documents,\n...     scores=scores,\n...     write_csv=True,\n... )\n\n&gt;&gt;&gt; results = distillation_evaluator(model=model, output_path=\".\")\n\n&gt;&gt;&gt; assert \"kl_divergence\" in results\n&gt;&gt;&gt; assert isinstance(results[\"kl_divergence\"], float)\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; df = pd.read_csv(distillation_evaluator.csv_file)\n&gt;&gt;&gt; assert df.columns.tolist() == distillation_evaluator.csv_headers\n</code></pre>"},{"location":"api/evaluation/ColBERTDistillationEvaluator/#methods","title":"Methods","text":"call <p>This is called during training to evaluate the model. It returns a score for the evaluation with a higher score indicating a better result.</p> <p>Args:     model: the model to evaluate     output_path: path where predictions and metrics are written         to     epoch: the epoch where the evaluation takes place. This is         used for the file prefixes. If this is -1, then we         assume evaluation on test data.     steps: the steps in the current epoch at time of the         evaluation. This is used for the file prefixes. If this         is -1, then we assume evaluation at the end of the         epoch.  Returns:     Either a score for the evaluation with a higher score     indicating a better result, or a dictionary with scores. If     the latter is chosen, then <code>evaluator.primary_metric</code> must     be defined</p> <p>Parameters</p> <ul> <li>model     (\"'SentenceTransformer'\")    </li> <li>output_path     ('str')     \u2013 defaults to <code>None</code> </li> <li>epoch     ('int')     \u2013 defaults to <code>-1</code> </li> <li>steps     ('int')     \u2013 defaults to <code>-1</code> </li> </ul> get_config_dict <p>Return a dictionary with all meaningful configuration values of the evaluator to store in the model card.</p> prefix_name_to_metrics store_metrics_in_model_card_data"},{"location":"api/evaluation/ColBERTTripletEvaluator/","title":"ColBERTTripletEvaluator","text":"<p>Evaluate a model based on a set of triples. The evaluation will compare the score between the anchor and the positive sample with the score between the anchor and the negative sample. The accuracy is computed as the number of times the score between the anchor and the positive sample is higher than the score between the anchor and the negative sample.</p>"},{"location":"api/evaluation/ColBERTTripletEvaluator/#parameters","title":"Parameters","text":"<ul> <li> <p>anchors ('list[str]')</p> <p>Sentences to check similarity to. (e.g. a query)</p> </li> <li> <p>positives ('list[str]')</p> <p>List of positive sentences</p> </li> <li> <p>negatives ('list[str]')</p> <p>List of negative sentences</p> </li> <li> <p>name ('str') \u2013 defaults to ``</p> <p>Name for the output.</p> </li> <li> <p>batch_size ('int') \u2013 defaults to <code>32</code></p> <p>Batch size used to compute embeddings.</p> </li> <li> <p>show_progress_bar ('bool') \u2013 defaults to <code>False</code></p> <p>If true, prints a progress bar.</p> </li> <li> <p>write_csv ('bool') \u2013 defaults to <code>True</code></p> <p>Wether or not to write results to a CSV file.</p> </li> <li> <p>truncate_dim ('int | None') \u2013 defaults to <code>None</code></p> <p>The dimension to truncate sentence embeddings to. If None, do not truncate.</p> </li> </ul>"},{"location":"api/evaluation/ColBERTTripletEvaluator/#attributes","title":"Attributes","text":"<ul> <li> <p>description</p> <p>Returns a human-readable description of the evaluator: BinaryClassificationEvaluator -&gt; Binary Classification  1. Replace \"CE\" prefix with \"CrossEncoder\" 2. Remove \"Evaluator\" from the class name 3. Add a space before every capital letter</p> </li> </ul>"},{"location":"api/evaluation/ColBERTTripletEvaluator/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from pylate import evaluation, models\n\n&gt;&gt;&gt; model = models.ColBERT(\n...     model_name_or_path=\"sentence-transformers/all-MiniLM-L6-v2\",\n...     device=\"cpu\",\n... )\n\n&gt;&gt;&gt; anchors = [\n...     \"fruits are healthy.\",\n...     \"fruits are healthy.\",\n... ]\n\n&gt;&gt;&gt; positives = [\n...     \"fruits are good for health.\",\n...     \"Fruits are growing in the trees.\",\n... ]\n\n&gt;&gt;&gt; negatives = [\n...     \"Fruits are growing in the trees.\",\n...     \"fruits are good for health.\",\n... ]\n\n&gt;&gt;&gt; triplet_evaluation = evaluation.ColBERTTripletEvaluator(\n...     anchors=anchors,\n...     positives=positives,\n...     negatives=negatives,\n...     write_csv=True,\n... )\n\n&gt;&gt;&gt; results = triplet_evaluation(model=model, output_path=\".\")\n\n&gt;&gt;&gt; results\n{'accuracy': 0.5}\n\n&gt;&gt;&gt; triplet_evaluation.csv_headers\n['epoch', 'steps', 'accuracy']\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; df = pd.read_csv(triplet_evaluation.csv_file)\n&gt;&gt;&gt; assert df.columns.tolist() == triplet_evaluation.csv_headers\n</code></pre>"},{"location":"api/evaluation/ColBERTTripletEvaluator/#methods","title":"Methods","text":"call <p>Evaluate the model on the triplet dataset. Measure the scoring between the anchor and the positive with every other positive and negative samples using HITS@K.</p> <p>Parameters</p> <ul> <li>model     ('ColBERT')    </li> <li>output_path     ('str')     \u2013 defaults to <code>None</code> </li> <li>epoch     ('int')     \u2013 defaults to <code>-1</code> </li> <li>steps     ('int')     \u2013 defaults to <code>-1</code> </li> </ul> from_input_examples get_config_dict <p>Return a dictionary with all meaningful configuration values of the evaluator to store in the model card.</p> prefix_name_to_metrics store_metrics_in_model_card_data"},{"location":"api/evaluation/NanoBEIREvaluator/","title":"NanoBEIREvaluator","text":"<p>Evaluate the performance of a PyLate Model on the NanoBEIR collection.</p> <p>This is a direct extension of the NanoBEIREvaluator from the sentence-transformers library, leveraging the PyLateInformationRetrievalEvaluator class. The collection is a set of datasets based on the BEIR collection, but with a significantly smaller size, so it can be used for quickly evaluating the retrieval performance of a model before commiting to a full evaluation. The Evaluator will return the same metrics as the InformationRetrievalEvaluator (i.e., MRR, nDCG, Recall@k), for each dataset and on average.</p>"},{"location":"api/evaluation/NanoBEIREvaluator/#parameters","title":"Parameters","text":"<ul> <li> <p>dataset_names ('list[DatasetNameType] | None') \u2013 defaults to <code>None</code></p> </li> <li> <p>mrr_at_k ('list[int]') \u2013 defaults to <code>[10]</code></p> </li> <li> <p>ndcg_at_k ('list[int]') \u2013 defaults to <code>[10]</code></p> </li> <li> <p>accuracy_at_k ('list[int]') \u2013 defaults to <code>[1, 3, 5, 10]</code></p> </li> <li> <p>precision_recall_at_k ('list[int]') \u2013 defaults to <code>[1, 3, 5, 10]</code></p> </li> <li> <p>map_at_k ('list[int]') \u2013 defaults to <code>[100]</code></p> </li> <li> <p>show_progress_bar ('bool') \u2013 defaults to <code>False</code></p> </li> <li> <p>batch_size ('int') \u2013 defaults to <code>32</code></p> </li> <li> <p>write_csv ('bool') \u2013 defaults to <code>True</code></p> </li> <li> <p>truncate_dim ('int | None') \u2013 defaults to <code>None</code></p> </li> <li> <p>score_functions ('dict[str, Callable[[Tensor, Tensor], Tensor]]') \u2013 defaults to <code>None</code></p> </li> <li> <p>main_score_function ('str | SimilarityFunction | None') \u2013 defaults to <code>None</code></p> </li> <li> <p>aggregate_fn ('Callable[[list[float]], float]') \u2013 defaults to <code>&lt;function mean at 0x1026fdbc0&gt;</code></p> </li> <li> <p>aggregate_key ('str') \u2013 defaults to <code>mean</code></p> </li> <li> <p>query_prompts ('str | dict[str, str] | None') \u2013 defaults to <code>None</code></p> </li> <li> <p>corpus_prompts ('str | dict[str, str] | None') \u2013 defaults to <code>None</code></p> </li> </ul>"},{"location":"api/evaluation/NanoBEIREvaluator/#attributes","title":"Attributes","text":"<ul> <li> <p>description</p> <p>Returns a human-readable description of the evaluator: BinaryClassificationEvaluator -&gt; Binary Classification  1. Replace \"CE\" prefix with \"CrossEncoder\" 2. Remove \"Evaluator\" from the class name 3. Add a space before every capital letter</p> </li> </ul>"},{"location":"api/evaluation/NanoBEIREvaluator/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from pylate import models, evaluation\n\n&gt;&gt;&gt; model = models.ColBERT(\n...     model_name_or_path=\"lightonai/colbertv2.0\"\n... )\n\n&gt;&gt;&gt; datasets = [\"SciFact\"]\n\n&gt;&gt;&gt; try:\n...     evaluator = evaluation.NanoBEIREvaluator(\n...         dataset_names=datasets\n...     )\n...     results = evaluator(model)\n... except Exception:\n...     pass\n</code></pre> <p>{'NanoSciFact_MaxSim_accuracy@1': 0.62, 'NanoSciFact_MaxSim_accuracy@3': 0.74, 'NanoSciFact_MaxSim_accuracy@5': 0.8, 'NanoSciFact_MaxSim_accuracy@10': 0.86, 'NanoSciFact_MaxSim_precision@1': 0.62, 'NanoSciFact_MaxSim_precision@3': 0.26666666666666666, 'NanoSciFact_MaxSim_precision@5': 0.18, 'NanoSciFact_MaxSim_precision@10': 0.096, 'NanoSciFact_MaxSim_recall@1': 0.595, 'NanoSciFact_MaxSim_recall@3': 0.715, 'NanoSciFact_MaxSim_recall@5': 0.79, 'NanoSciFact_MaxSim_recall@10': 0.85, 'NanoSciFact_MaxSim_ndcg@10': 0.7279903941189909, 'NanoSciFact_MaxSim_mrr@10': 0.6912222222222222, 'NanoSciFact_MaxSim_map@100': 0.6903374780806633, 'NanoBEIR_mean_MaxSim_accuracy@1': 0.62, 'NanoBEIR_mean_MaxSim_accuracy@3': 0.74, 'NanoBEIR_mean_MaxSim_accuracy@5': 0.8, 'NanoBEIR_mean_MaxSim_accuracy@10': 0.86, 'NanoBEIR_mean_MaxSim_precision@1': 0.62, 'NanoBEIR_mean_MaxSim_precision@3': 0.26666666666666666, 'NanoBEIR_mean_MaxSim_precision@5': 0.18, 'NanoBEIR_mean_MaxSim_precision@10': 0.096, 'NanoBEIR_mean_MaxSim_recall@1': 0.595, 'NanoBEIR_mean_MaxSim_recall@3': 0.715, 'NanoBEIR_mean_MaxSim_recall@5': 0.79, 'NanoBEIR_mean_MaxSim_recall@10': 0.85, 'NanoBEIR_mean_MaxSim_ndcg@10': 0.7279903941189909, 'NanoBEIR_mean_MaxSim_mrr@10': 0.6912222222222222, 'NanoBEIR_mean_MaxSim_map@100': 0.6903374780806633}</p>"},{"location":"api/evaluation/NanoBEIREvaluator/#methods","title":"Methods","text":"call <p>This is called during training to evaluate the model. It returns a score for the evaluation with a higher score indicating a better result.</p> <p>Args:     model: the model to evaluate     output_path: path where predictions and metrics are written         to     epoch: the epoch where the evaluation takes place. This is         used for the file prefixes. If this is -1, then we         assume evaluation on test data.     steps: the steps in the current epoch at time of the         evaluation. This is used for the file prefixes. If this         is -1, then we assume evaluation at the end of the         epoch.  Returns:     Either a score for the evaluation with a higher score     indicating a better result, or a dictionary with scores. If     the latter is chosen, then <code>evaluator.primary_metric</code> must     be defined</p> <p>Parameters</p> <ul> <li>model     ('SentenceTransformer')    </li> <li>output_path     ('str')     \u2013 defaults to <code>None</code> </li> <li>epoch     ('int')     \u2013 defaults to <code>-1</code> </li> <li>steps     ('int')     \u2013 defaults to <code>-1</code> </li> <li>args </li> <li>kwargs </li> </ul> get_config_dict <p>Return a dictionary with all meaningful configuration values of the evaluator to store in the model card.</p> prefix_name_to_metrics store_metrics_in_model_card_data"},{"location":"api/evaluation/NanoBEIREvaluator/#references","title":"References","text":"<ul> <li>NanoBEIR</li> </ul>"},{"location":"api/evaluation/PyLateInformationRetrievalEvaluator/","title":"PyLateInformationRetrievalEvaluator","text":"<p>This class evaluates an Information Retrieval (IR) setting. This is a direct extension of the InformationRetrievalEvaluator from the sentence-transformers library, only override the compute_metrices method to be compilatible with PyLate models (define assymetric encoding using is_query params and add padding).</p>"},{"location":"api/evaluation/PyLateInformationRetrievalEvaluator/#parameters","title":"Parameters","text":"<ul> <li> <p>queries ('dict[str, str]')</p> </li> <li> <p>corpus ('dict[str, str]')</p> </li> <li> <p>relevant_docs ('dict[str, set[str]]')</p> </li> <li> <p>corpus_chunk_size ('int') \u2013 defaults to <code>50000</code></p> </li> <li> <p>mrr_at_k ('list[int]') \u2013 defaults to <code>[10]</code></p> </li> <li> <p>ndcg_at_k ('list[int]') \u2013 defaults to <code>[10]</code></p> </li> <li> <p>accuracy_at_k ('list[int]') \u2013 defaults to <code>[1, 3, 5, 10]</code></p> </li> <li> <p>precision_recall_at_k ('list[int]') \u2013 defaults to <code>[1, 3, 5, 10]</code></p> </li> <li> <p>map_at_k ('list[int]') \u2013 defaults to <code>[100]</code></p> </li> <li> <p>show_progress_bar ('bool') \u2013 defaults to <code>False</code></p> </li> <li> <p>batch_size ('int') \u2013 defaults to <code>32</code></p> </li> <li> <p>name ('str') \u2013 defaults to ``</p> </li> <li> <p>write_csv ('bool') \u2013 defaults to <code>True</code></p> </li> <li> <p>truncate_dim ('int | None') \u2013 defaults to <code>None</code></p> </li> <li> <p>score_functions ('dict[str, Callable[[Tensor, Tensor], Tensor]] | None') \u2013 defaults to <code>None</code></p> </li> <li> <p>main_score_function ('str | SimilarityFunction | None') \u2013 defaults to <code>None</code></p> </li> <li> <p>query_prompt ('str | None') \u2013 defaults to <code>None</code></p> </li> <li> <p>query_prompt_name ('str | None') \u2013 defaults to <code>None</code></p> </li> <li> <p>corpus_prompt ('str | None') \u2013 defaults to <code>None</code></p> </li> <li> <p>corpus_prompt_name ('str | None') \u2013 defaults to <code>None</code></p> </li> </ul>"},{"location":"api/evaluation/PyLateInformationRetrievalEvaluator/#attributes","title":"Attributes","text":"<ul> <li> <p>description</p> <p>Returns a human-readable description of the evaluator: BinaryClassificationEvaluator -&gt; Binary Classification  1. Replace \"CE\" prefix with \"CrossEncoder\" 2. Remove \"Evaluator\" from the class name 3. Add a space before every capital letter</p> </li> </ul>"},{"location":"api/evaluation/PyLateInformationRetrievalEvaluator/#methods","title":"Methods","text":"call <p>This is called during training to evaluate the model. It returns a score for the evaluation with a higher score indicating a better result.</p> <p>Args:     model: the model to evaluate     output_path: path where predictions and metrics are written         to     epoch: the epoch where the evaluation takes place. This is         used for the file prefixes. If this is -1, then we         assume evaluation on test data.     steps: the steps in the current epoch at time of the         evaluation. This is used for the file prefixes. If this         is -1, then we assume evaluation at the end of the         epoch.  Returns:     Either a score for the evaluation with a higher score     indicating a better result, or a dictionary with scores. If     the latter is chosen, then <code>evaluator.primary_metric</code> must     be defined</p> <p>Parameters</p> <ul> <li>model     ('SentenceTransformer')    </li> <li>output_path     ('str')     \u2013 defaults to <code>None</code> </li> <li>epoch     ('int')     \u2013 defaults to <code>-1</code> </li> <li>steps     ('int')     \u2013 defaults to <code>-1</code> </li> <li>args </li> <li>kwargs </li> </ul> compute_dcg_at_k compute_metrices compute_metrics get_config_dict <p>Return a dictionary with all meaningful configuration values of the evaluator to store in the model card.</p> output_scores prefix_name_to_metrics store_metrics_in_model_card_data"},{"location":"api/evaluation/evaluate/","title":"evaluate","text":"<p>Evaluate candidates matchs.</p>"},{"location":"api/evaluation/evaluate/#parameters","title":"Parameters","text":"<ul> <li> <p>scores ('list[list[dict]]')</p> </li> <li> <p>qrels ('dict')</p> <p>Qrels.</p> </li> <li> <p>queries ('list[str]')</p> <p>index of queries of qrels.</p> </li> <li> <p>metrics ('list | None') \u2013 defaults to <code>None</code></p> <p>Metrics to compute.</p> </li> </ul>"},{"location":"api/evaluation/evaluate/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from pylate import evaluation\n\n&gt;&gt;&gt; scores = [\n...     [{\"id\": \"1\", \"score\": 0.9}, {\"id\": \"2\", \"score\": 0.8}],\n...     [{\"id\": \"3\", \"score\": 0.7}, {\"id\": \"4\", \"score\": 0.6}],\n... ]\n\n&gt;&gt;&gt; qrels = {\n...     \"query1\": {\"1\": True, \"2\": True},\n...     \"query2\": {\"3\": True, \"4\": True},\n... }\n\n&gt;&gt;&gt; queries = [\"query1\", \"query2\"]\n\n&gt;&gt;&gt; results = evaluation.evaluate(\n...     scores=scores,\n...     qrels=qrels,\n...     queries=queries,\n...     metrics=[\"ndcg@10\", \"hits@1\"],\n... )\n</code></pre>"},{"location":"api/evaluation/get-beir-triples/","title":"get_beir_triples","text":"<p>Build BEIR triples.</p>"},{"location":"api/evaluation/get-beir-triples/#parameters","title":"Parameters","text":"<ul> <li> <p>documents ('list')</p> <p>Documents.</p> </li> <li> <p>queries ('list[str]')</p> <p>Queries.</p> </li> <li> <p>qrels ('dict')</p> </li> </ul>"},{"location":"api/evaluation/get-beir-triples/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from pylate import evaluation\n\n&gt;&gt;&gt; documents, queries, qrels = evaluation.load_beir(\n...     \"scifact\",\n...     split=\"test\",\n... )\n\n&gt;&gt;&gt; triples = evaluation.get_beir_triples(\n...     documents=documents,\n...     queries=queries,\n...     qrels=qrels\n... )\n\n&gt;&gt;&gt; len(triples)\n339\n</code></pre>"},{"location":"api/evaluation/load-beir/","title":"load_beir","text":"<p>Load BEIR dataset.</p>"},{"location":"api/evaluation/load-beir/#parameters","title":"Parameters","text":"<ul> <li> <p>dataset_name ('str')</p> <p>Name of the beir dataset.</p> </li> <li> <p>split ('str') \u2013 defaults to <code>test</code></p> <p>Split to load.</p> </li> </ul>"},{"location":"api/evaluation/load-beir/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from pylate import evaluation\n\n&gt;&gt;&gt; documents, queries, qrels = evaluation.load_beir(\n...     \"scifact\",\n...     split=\"test\",\n... )\n\n&gt;&gt;&gt; len(documents)\n5183\n\n&gt;&gt;&gt; len(queries)\n300\n\n&gt;&gt;&gt; len(qrels)\n300\n</code></pre>"},{"location":"api/evaluation/load-custom-dataset/","title":"load_custom_dataset","text":"<p>Load a custom dataset.</p>"},{"location":"api/evaluation/load-custom-dataset/#parameters","title":"Parameters","text":"<ul> <li> <p>path ('str')</p> <p>Path of the dataset.</p> </li> <li> <p>split ('str') \u2013 defaults to <code>test</code></p> <p>Split to load.</p> </li> </ul>"},{"location":"api/hf-hub/PylateModelCardData/","title":"PylateModelCardData","text":"<p>A dataclass for storing data used in the model card.</p>"},{"location":"api/hf-hub/PylateModelCardData/#parameters","title":"Parameters","text":"<ul> <li> <p>language ('str | list[str] | None') \u2013 defaults to <code>&lt;factory&gt;</code></p> <p>The model language, either a string or a list of strings, e.g., \"en\" or [\"en\", \"de\", \"nl\"].</p> </li> <li> <p>license ('str | None') \u2013 defaults to <code>None</code></p> <p>The license of the model, e.g., \"apache-2.0\", \"mit\", or \"cc-by-nc-sa-4.0\".</p> </li> <li> <p>model_name ('str | None') \u2013 defaults to <code>None</code></p> <p>The pretty name of the model, e.g., \"SentenceTransformer based on microsoft/mpnet-base\".</p> </li> <li> <p>model_id ('str | None') \u2013 defaults to <code>None</code></p> <p>The model ID for pushing the model to the Hub, e.g., \"tomaarsen/sbert-mpnet-base-allnli\".</p> </li> <li> <p>train_datasets ('list[dict[str, str]]') \u2013 defaults to <code>&lt;factory&gt;</code></p> <p>A list of dictionaries containing names and/or Hugging Face dataset IDs for training datasets, e.g., [{\"name\": \"SNLI\", \"id\": \"stanfordnlp/snli\"}, {\"name\": \"MultiNLI\", \"id\": \"nyu-mll/multi_nli\"}, {\"name\": \"STSB\"}].</p> </li> <li> <p>eval_datasets ('list[dict[str, str]]') \u2013 defaults to <code>&lt;factory&gt;</code></p> <p>A list of dictionaries containing names and/or Hugging Face dataset IDs for evaluation datasets, e.g., [{\"name\": \"SNLI\", \"id\": \"stanfordnlp/snli\"}, {\"id\": \"mteb/stsbenchmark-sts\"}].</p> </li> <li> <p>task_name ('str') \u2013 defaults to <code>semantic textual similarity, semantic search, paraphrase mining, text classification, clustering, and more</code></p> <p>The human-readable task the model is trained on, e.g., \"semantic textual similarity, semantic search, paraphrase mining, text classification, clustering, and more\".</p> </li> <li> <p>tags ('list[str] | None') \u2013 defaults to <code>&lt;factory&gt;</code></p> <p>A list of tags for the model, e.g., [\"sentence-transformers\", \"sentence-similarity\", \"feature-extraction\"].</p> </li> <li> <p>generate_widget_examples (\"Literal['deprecated']\") \u2013 defaults to <code>deprecated</code></p> </li> </ul>"},{"location":"api/hf-hub/PylateModelCardData/#attributes","title":"Attributes","text":"<ul> <li> <p>base_model</p> </li> <li> <p>base_model_revision</p> </li> <li> <p>best_model_step</p> </li> <li> <p>code_carbon_callback</p> </li> <li> <p>license</p> </li> <li> <p>model</p> </li> <li> <p>model_id</p> </li> <li> <p>model_name</p> </li> <li> <p>predict_example</p> </li> <li> <p>trainer</p> </li> </ul>"},{"location":"api/hf-hub/PylateModelCardData/#methods","title":"Methods","text":"add_tags compute_dataset_metrics <p>Given a dataset, compute the following: * Dataset Size * Dataset Columns * Dataset Stats     - Strings: min, mean, max word count/token length     - Integers: Counter() instance     - Floats: min, mean, max range     - List: number of elements or min, mean, max number of elements * 3 Example samples * Loss function name     - Loss function config</p> <p>Parameters</p> <ul> <li>dataset     ('Dataset | IterableDataset | None')    </li> <li>dataset_info     ('dict[str, Any]')    </li> <li>loss     ('dict[str, nn.Module] | nn.Module | None')    </li> </ul> extract_dataset_metadata format_eval_metrics <p>Format the evaluation metrics for the model card.</p> <p>The following keys will be returned: - eval_metrics: A list of dictionaries containing the class name, description, dataset name, and a markdown table   This is used to display the evaluation metrics in the model card. - metrics: A list of all metric keys. This is used in the model card metadata. - model-index: A list of dictionaries containing the task name, task type, dataset type, dataset name, metric name,   metric type, and metric value. This is used to display the evaluation metrics in the model card metadata.</p> format_training_logs get <p>Get value for a given metadata key.</p> <p>Parameters</p> <ul> <li>key     (str)    </li> <li>default     (Any)     \u2013 defaults to <code>None</code> </li> </ul> get_codecarbon_data get_model_specific_metadata infer_datasets pop <p>Pop value for a given metadata key.</p> <p>Parameters</p> <ul> <li>key     (str)    </li> <li>default     (Any)     \u2013 defaults to <code>None</code> </li> </ul> register_model set_base_model set_best_model_step set_evaluation_metrics set_label_examples set_language set_license set_losses set_model_id set_widget_examples <p>A function to create widget examples from a dataset. For now, set_widget_examples is not compatible with our transform/map operations, so we make it a no-op until it is fixed</p> <p>Parameters</p> <ul> <li>dataset     ('Dataset | DatasetDict')    </li> </ul> to_dict <p>Converts CardData to a dict.</p> <p>Returns:     <code>dict</code>: CardData represented as a dictionary ready to be dumped to a YAML     block for inclusion in a README.md file.</p> to_yaml <p>Dumps CardData to a YAML block for inclusion in a README.md file.</p> <p>Args:     line_break (str, optional):         The line break to use when dumping to yaml.  Returns:     <code>str</code>: CardData represented as a YAML block.</p> <p>Parameters</p> <ul> <li>line_break     \u2013 defaults to <code>None</code> </li> </ul> tokenize try_to_set_base_model validate_datasets"},{"location":"api/indexes/PLAID/","title":"PLAID","text":"<p>PLAID index. The PLAID index is the most scalable type of index for multi-vector search and leverage PQ-IVF as well as custom kernel for decompression.</p>"},{"location":"api/indexes/PLAID/#parameters","title":"Parameters","text":"<ul> <li> <p>index_folder ('str') \u2013 defaults to <code>indexes</code></p> <p>The folder where the index will be stored.</p> </li> <li> <p>index_name ('str') \u2013 defaults to <code>colbert</code></p> <p>The name of the index.</p> </li> <li> <p>override ('bool') \u2013 defaults to <code>False</code></p> <p>Whether to override the collection if it already exists.</p> </li> <li> <p>embedding_size ('int') \u2013 defaults to <code>128</code></p> <p>The number of dimensions of the embeddings.</p> </li> <li> <p>nbits ('int') \u2013 defaults to <code>2</code></p> <p>The number of bits to use for the quantization.</p> </li> <li> <p>nranks ('int') \u2013 defaults to <code>1</code></p> </li> <li> <p>kmeans_niters ('int') \u2013 defaults to <code>4</code></p> <p>The number of iterations to use for the k-means clustering.</p> </li> <li> <p>index_bsize ('int') \u2013 defaults to <code>1</code></p> </li> <li> <p>ndocs ('int') \u2013 defaults to <code>8192</code></p> <p>The number of candidate documents</p> </li> <li> <p>centroid_score_threshold ('float') \u2013 defaults to <code>0.35</code></p> <p>The threshold scores for centroid pruning.</p> </li> <li> <p>ncells ('int') \u2013 defaults to <code>8</code></p> <p>The number of cells to consider for search.</p> </li> <li> <p>search_batch_size ('int') \u2013 defaults to <code>262144</code></p> <p>The batch size to use when searching.</p> </li> </ul>"},{"location":"api/indexes/PLAID/#methods","title":"Methods","text":"call <p>Query the index for the nearest neighbors of the queries embeddings.</p> <p>Parameters</p> <ul> <li>queries_embeddings     ('np.ndarray | torch.Tensor')    </li> <li>k     ('int')     \u2013 defaults to <code>10</code> </li> </ul> add_documents <p>Add documents to the index.</p> <p>Parameters</p> <ul> <li>documents_ids     ('str | list[str]')    </li> <li>documents_embeddings     ('list[np.ndarray | torch.Tensor]')    </li> <li>batch_size     ('int')     \u2013 defaults to <code>2000</code> </li> </ul> get_documents_embeddings remove_documents <p>Remove documents from the index.</p> <p>Parameters</p> <ul> <li>documents_ids     ('list[str]')    </li> </ul>"},{"location":"api/indexes/Voyager/","title":"Voyager","text":"<p>Voyager index. The Voyager index is a fast and efficient index for approximate nearest neighbor search.</p>"},{"location":"api/indexes/Voyager/#parameters","title":"Parameters","text":"<ul> <li> <p>index_folder ('str') \u2013 defaults to <code>indexes</code></p> </li> <li> <p>index_name ('str') \u2013 defaults to <code>colbert</code></p> </li> <li> <p>override ('bool') \u2013 defaults to <code>False</code></p> <p>Whether to override the collection if it already exists.</p> </li> <li> <p>embedding_size ('int') \u2013 defaults to <code>128</code></p> <p>The number of dimensions of the embeddings.</p> </li> <li> <p>M ('int') \u2013 defaults to <code>64</code></p> <p>The number of subquantizers.</p> </li> <li> <p>ef_construction ('int') \u2013 defaults to <code>200</code></p> <p>The number of candidates to evaluate during the construction of the index.</p> </li> <li> <p>ef_search ('int') \u2013 defaults to <code>200</code></p> <p>The number of candidates to evaluate during the search.</p> </li> </ul>"},{"location":"api/indexes/Voyager/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from pylate import indexes, models\n\n&gt;&gt;&gt; index = indexes.Voyager(\n...     index_folder=\"test_indexes\",\n...     index_name=\"colbert\",\n...     override=True,\n...     embedding_size=128,\n... )\n\n&gt;&gt;&gt; model = models.ColBERT(\n...     model_name_or_path=\"sentence-transformers/all-MiniLM-L6-v2\",\n... )\n\n&gt;&gt;&gt; documents_embeddings = model.encode(\n...     [\"fruits are healthy.\", \"fruits are good for health.\", \"fruits are bad for health.\"],\n...     is_query=False,\n... )\n\n&gt;&gt;&gt; index = index.add_documents(\n...     documents_ids=[\"1\", \"2\", \"3\"],\n...     documents_embeddings=documents_embeddings\n... )\n\n&gt;&gt;&gt; queries_embeddings = model.encode(\n...     [\"fruits are healthy.\", \"fruits are good for health and fun.\"],\n...     is_query=True,\n... )\n\n&gt;&gt;&gt; matchs = index(queries_embeddings, k=30)\n\n&gt;&gt;&gt; assert matchs[\"distances\"].shape[0] == 2\n&gt;&gt;&gt; assert isinstance(matchs, dict)\n&gt;&gt;&gt; assert \"documents_ids\" in matchs\n&gt;&gt;&gt; assert \"distances\" in matchs\n\n&gt;&gt;&gt; queries_embeddings = model.encode(\n...     \"fruits are healthy.\",\n...     is_query=True,\n... )\n\n&gt;&gt;&gt; matchs = index(queries_embeddings, k=30)\n\n&gt;&gt;&gt; assert matchs[\"distances\"].shape[0] == 1\n&gt;&gt;&gt; assert isinstance(matchs, dict)\n&gt;&gt;&gt; assert \"documents_ids\" in matchs\n&gt;&gt;&gt; assert \"distances\" in matchs\n</code></pre>"},{"location":"api/indexes/Voyager/#methods","title":"Methods","text":"call <p>Query the index for the nearest neighbors of the queries embeddings.</p> <p>Parameters</p> <ul> <li>queries_embeddings     ('np.ndarray | torch.Tensor')    </li> <li>k     ('int')     \u2013 defaults to <code>10</code> </li> </ul> add_documents <p>Add documents to the index.</p> <p>Parameters</p> <ul> <li>documents_ids     ('str | list[str]')    </li> <li>documents_embeddings     ('list[np.ndarray | torch.Tensor]')    </li> <li>batch_size     ('int')     \u2013 defaults to <code>2000</code> </li> </ul> get_documents_embeddings <p>Retrieve document embeddings for re-ranking from Voyager.</p> <p>Parameters</p> <ul> <li>document_ids     ('list[list[str]]')    </li> </ul> remove_documents <p>Remove documents from the index.</p> <p>Parameters</p> <ul> <li>documents_ids     ('list[str]')    </li> </ul>"},{"location":"api/losses/CachedContrastive/","title":"CachedContrastive","text":"<p>A cached, in-batch negatives contrastive loss for PyLate, analogous to SentenceTransformers' CachedMultipleNegativesRankingLoss. This allows large effective batch sizes by chunking the embeddings pass and caching gradients w.r.t. those embeddings.</p>"},{"location":"api/losses/CachedContrastive/#parameters","title":"Parameters","text":"<ul> <li> <p>model ('ColBERT')</p> <p>A PyLate ColBERT model</p> </li> <li> <p>score_metric ('Callable') \u2013 defaults to <code>&lt;function colbert_scores at 0x13facb560&gt;</code></p> <p>ColBERT scoring function. Defaults to colbert_scores.</p> </li> <li> <p>mini_batch_size ('int') \u2013 defaults to <code>32</code></p> <p>Chunk size for the forward pass. You can keep this small to avoid OOM on large batch sizes.</p> </li> <li> <p>size_average ('bool') \u2013 defaults to <code>True</code></p> <p>Whether to average or sum the cross-entropy loss across the mini-batch.</p> </li> <li> <p>gather_across_devices ('bool') \u2013 defaults to <code>False</code></p> <p>Whether to gather the embeddings across devices to have more in batch negatives. We recommand making sure the sampling across GPUs use the same dataset in case of multi-dataset training to make sure the negatives are plausible.</p> </li> <li> <p>show_progress_bar ('bool') \u2013 defaults to <code>False</code></p> <p>Whether to show a TQDM progress bar for the embedding steps.</p> </li> <li> <p>temperature ('float') \u2013 defaults to <code>1.0</code></p> </li> </ul>"},{"location":"api/losses/CachedContrastive/#attributes","title":"Attributes","text":"<ul> <li>citation</li> </ul>"},{"location":"api/losses/CachedContrastive/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from pylate import models, losses\n\n&gt;&gt;&gt; model = models.ColBERT(\n...     model_name_or_path=\"sentence-transformers/all-MiniLM-L6-v2\", device=\"cpu\"\n... )\n\n&gt;&gt;&gt; loss = losses.CachedContrastive(model=model, mini_batch_size=1)\n\n&gt;&gt;&gt; anchors = model.tokenize([\n...     \"fruits are healthy.\", \"chips are not healthy.\"\n... ], is_query=True)\n\n&gt;&gt;&gt; positives = model.tokenize([\n...     \"fruits are good for health.\", \"chips are not good for health.\"\n... ], is_query=False)\n\n&gt;&gt;&gt; negatives = model.tokenize([\n...     \"fruits are bad for health.\", \"chips are good for health.\"\n... ], is_query=False)\n\n&gt;&gt;&gt; sentence_features = [anchors, positives, negatives]\n\n&gt;&gt;&gt; loss = loss(sentence_features=sentence_features)\n&gt;&gt;&gt; assert isinstance(loss.item(), float)\n</code></pre>"},{"location":"api/losses/CachedContrastive/#methods","title":"Methods","text":"call <p>Call self as a function.</p> <p>Parameters</p> <ul> <li>args </li> <li>kwargs </li> </ul> add_module <p>Add a child module to the current module.</p> <p>The module can be accessed as an attribute using the given name.  Args:     name (str): name of the child module. The child module can be         accessed from this module using the given name     module (Module): child module to be added to the module.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>module     (Optional[ForwardRef('Module')])    </li> </ul> apply <p>Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self.</p> <p>Typical use includes initializing the parameters of a model (see also :ref:<code>nn-init-doc</code>).  Args:     fn (:class:<code>Module</code> -&gt; None): function to be applied to each submodule  Returns:     Module: self  Example::      &gt;&gt;&gt; @torch.no_grad()     &gt;&gt;&gt; def init_weights(m):     &gt;&gt;&gt;     print(m)     &gt;&gt;&gt;     if type(m) == nn.Linear:     &gt;&gt;&gt;         m.weight.fill_(1.0)     &gt;&gt;&gt;         print(m.weight)     &gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))     &gt;&gt;&gt; net.apply(init_weights)     Linear(in_features=2, out_features=2, bias=True)     Parameter containing:     tensor([[1., 1.],             [1., 1.]], requires_grad=True)     Linear(in_features=2, out_features=2, bias=True)     Parameter containing:     tensor([[1., 1.],             [1., 1.]], requires_grad=True)     Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     )</p> <p>Parameters</p> <ul> <li>fn     (Callable[[ForwardRef('Module')], NoneType])    </li> </ul> bfloat16 <p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> buffers <p>Return an iterator over module buffers.</p> <p>Args:     recurse (bool): if True, then yields buffers of this module         and all submodules. Otherwise, yields only buffers that         are direct members of this module.  Yields:     torch.Tensor: module buffer  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for buf in model.buffers():     &gt;&gt;&gt;     print(type(buf), buf.size())      (20L,)      (20L, 1L, 5L, 5L) <p>Parameters</p> <ul> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> </ul> calculate_loss <p>Calculate the cross-entropy loss. No need to cache the gradients. Each sub-list in reps is a list of mini-batch chunk embeddings</p> <p>Parameters</p> <ul> <li>reps </li> <li>masks </li> <li>with_backward     ('bool')     \u2013 defaults to <code>False</code> </li> </ul> calculate_loss_and_cache_gradients <p>Calculate the cross-entropy loss and cache the gradients wrt. the embeddings.</p> <p>Parameters</p> <ul> <li>reps </li> <li>masks </li> </ul> children <p>Return an iterator over immediate children modules.</p> <p>Yields:     Module: a child module</p> compile <p>Compile this Module's forward using :func:<code>torch.compile</code>.</p> <p>This Module's <code>__call__</code> method is compiled and all arguments are passed as-is to :func:<code>torch.compile</code>.  See :func:<code>torch.compile</code> for details on the arguments for this function.</p> <p>Parameters</p> <ul> <li>args </li> <li>kwargs </li> </ul> cpu <p>Move all model parameters and buffers to the CPU.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> cuda <p>Move all model parameters and buffers to the GPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing the optimizer if the module will live on GPU while being optimized.  .. note::     This method modifies the module in-place.  Args:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, torch.device, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> double <p>Casts all floating point parameters and buffers to <code>double</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> embed_minibatch <p>Forward pass on a slice [begin:end] of sentence_feature. If 'with_grad' is False, we run under torch.no_grad. If 'copy_random_state' is True, we create and return a RandContext so that we can exactly reproduce this forward pass later.</p> <p>Parameters</p> <ul> <li>sentence_feature     ('dict[str, Tensor]')    </li> <li>begin     ('int')    </li> <li>end     ('int')    </li> <li>with_grad     ('bool')    </li> <li>copy_random_state     ('bool')    </li> <li>random_state     ('RandContext | None')     \u2013 defaults to <code>None</code> </li> </ul> embed_minibatch_iter <p>Yields chunks of embeddings (and corresponding RandContext) for the given sentence_feature, respecting the mini_batch_size limit.</p> <p>Parameters</p> <ul> <li>sentence_feature     ('dict[str, Tensor]')    </li> <li>with_grad     ('bool')    </li> <li>copy_random_state     ('bool')    </li> <li>random_states     ('list[RandContext] | None')     \u2013 defaults to <code>None</code> </li> </ul> eval <p>Set the module in evaluation mode.</p> <p>This has an effect only on certain modules. See the documentation of particular modules for details of their behaviors in training/evaluation mode, i.e. whether they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>, etc.  This is equivalent with :meth:<code>self.train(False) &lt;torch.nn.Module.train&gt;</code>.  See :ref:<code>locally-disable-grad-doc</code> for a comparison between <code>.eval()</code> and several similar mechanisms that may be confused with it.  Returns:     Module: self</p> extra_repr <p>Return the extra representation of the module.</p> <p>To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.</p> float <p>Casts all floating point parameters and buffers to <code>float</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> forward <p>Compute the CachedConstrastive loss.</p> <p>Parameters</p> <ul> <li>sentence_features     ('Iterable[dict[str, Tensor]]')    </li> <li>labels     ('Optional[Tensor]')     \u2013 defaults to <code>None</code> </li> </ul> get_buffer <p>Return the buffer given by <code>target</code> if it exists, otherwise throw an error.</p> <p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method's functionality as well as how to correctly specify <code>target</code>.  Args:     target: The fully-qualified string name of the buffer         to look for. (See <code>get_submodule</code> for how to specify a         fully-qualified string.)  Returns:     torch.Tensor: The buffer referenced by <code>target</code>  Raises:     AttributeError: If the target string references an invalid         path or resolves to something that is not a         buffer</p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> get_extra_state <p>Return any extra state to include in the module's state_dict.</p> <p>Implement this and a corresponding :func:<code>set_extra_state</code> for your module if you need to store extra state. This function is called when building the module's <code>state_dict()</code>.  Note that extra state should be picklable to ensure working serialization of the state_dict. We only provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes.  Returns:     object: Any extra state to store in the module's state_dict</p> get_parameter <p>Return the parameter given by <code>target</code> if it exists, otherwise throw an error.</p> <p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method's functionality as well as how to correctly specify <code>target</code>.  Args:     target: The fully-qualified string name of the Parameter         to look for. (See <code>get_submodule</code> for how to specify a         fully-qualified string.)  Returns:     torch.nn.Parameter: The Parameter referenced by <code>target</code>  Raises:     AttributeError: If the target string references an invalid         path or resolves to something that is not an         <code>nn.Parameter</code></p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> get_submodule <p>Return the submodule given by <code>target</code> if it exists, otherwise throw an error.</p> <p>For example, let's say you have an <code>nn.Module</code> <code>A</code> that looks like this:  .. code-block:: text      A(         (net_b): Module(             (net_c): Module(                 (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))             )             (linear): Linear(in_features=100, out_features=200, bias=True)         )     )  (The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> which has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)  To check whether or not we have the <code>linear</code> submodule, we would call <code>get_submodule(\"net_b.linear\")</code>. To check whether we have the <code>conv</code> submodule, we would call <code>get_submodule(\"net_b.net_c.conv\")</code>.  The runtime of <code>get_submodule</code> is bounded by the degree of module nesting in <code>target</code>. A query against <code>named_modules</code> achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, <code>get_submodule</code> should always be used.  Args:     target: The fully-qualified string name of the submodule         to look for. (See above example for how to specify a         fully-qualified string.)  Returns:     torch.nn.Module: The submodule referenced by <code>target</code>  Raises:     AttributeError: If the target string references an invalid         path or resolves to something that is not an         <code>nn.Module</code></p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> half <p>Casts all floating point parameters and buffers to <code>half</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> ipu <p>Move all model parameters and buffers to the IPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing the optimizer if the module will live on IPU while being optimized.  .. note::     This method modifies the module in-place.  Arguments:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, torch.device, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> load_state_dict <p>Copy parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants.</p> <p>If :attr:<code>strict</code> is <code>True</code>, then the keys of :attr:<code>state_dict</code> must exactly match the keys returned by this module's :meth:<code>~torch.nn.Module.state_dict</code> function.  .. warning::     If :attr:<code>assign</code> is <code>True</code> the optimizer must be created after     the call to :attr:<code>load_state_dict</code> unless     :func:<code>~torch.__future__.get_swap_module_params_on_conversion</code> is <code>True</code>.  Args:     state_dict (dict): a dict containing parameters and         persistent buffers.     strict (bool, optional): whether to strictly enforce that the keys         in :attr:<code>state_dict</code> match the keys returned by this module's         :meth:<code>~torch.nn.Module.state_dict</code> function. Default: <code>True</code>     assign (bool, optional): When set to <code>False</code>, the properties of the tensors         in the current module are preserved whereas setting it to <code>True</code> preserves         properties of the Tensors in the state dict. The only         exception is the <code>requires_grad</code> field of :class:<code>~torch.nn.Parameter</code>s         for which the value from the module is preserved.         Default: <code>False</code>  Returns:     <code>NamedTuple</code> with <code>missing_keys</code> and <code>unexpected_keys</code> fields:         * missing_keys is a list of str containing any keys that are expected             by this module but missing from the provided <code>state_dict</code>.         * unexpected_keys is a list of str containing the keys that are not             expected by this module but present in the provided <code>state_dict</code>.  Note:     If a parameter or buffer is registered as <code>None</code> and its corresponding key     exists in :attr:<code>state_dict</code>, :meth:<code>load_state_dict</code> will raise a     <code>RuntimeError</code>.</p> <p>Parameters</p> <ul> <li>state_dict     (Mapping[str, Any])    </li> <li>strict     (bool)     \u2013 defaults to <code>True</code> </li> <li>assign     (bool)     \u2013 defaults to <code>False</code> </li> </ul> modules <p>Return an iterator over all modules in the network.</p> <p>Yields:     Module: a module in the network  Note:     Duplicate modules are returned only once. In the following     example, <code>l</code> will be returned only once.  Example::      &gt;&gt;&gt; l = nn.Linear(2, 2)     &gt;&gt;&gt; net = nn.Sequential(l, l)     &gt;&gt;&gt; for idx, m in enumerate(net.modules()):     ...     print(idx, '-&gt;', m)      0 -&gt; Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     )     1 -&gt; Linear(in_features=2, out_features=2, bias=True)</p> mtia <p>Move all model parameters and buffers to the MTIA.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing the optimizer if the module will live on MTIA while being optimized.  .. note::     This method modifies the module in-place.  Arguments:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, torch.device, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> named_buffers <p>Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.</p> <p>Args:     prefix (str): prefix to prepend to all buffer names.     recurse (bool, optional): if True, then yields buffers of this module         and all submodules. Otherwise, yields only buffers that         are direct members of this module. Defaults to True.     remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.  Yields:     (str, torch.Tensor): Tuple containing the name and buffer  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, buf in self.named_buffers():     &gt;&gt;&gt;     if name in ['running_var']:     &gt;&gt;&gt;         print(buf.size())</p> <p>Parameters</p> <ul> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> named_children <p>Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.</p> <p>Yields:     (str, Module): Tuple containing a name and child module  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, module in model.named_children():     &gt;&gt;&gt;     if name in ['conv4', 'conv5']:     &gt;&gt;&gt;         print(module)</p> named_modules <p>Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.</p> <p>Args:     memo: a memo to store the set of modules already added to the result     prefix: a prefix that will be added to the name of the module     remove_duplicate: whether to remove the duplicated module instances in the result         or not  Yields:     (str, Module): Tuple of name and module  Note:     Duplicate modules are returned only once. In the following     example, <code>l</code> will be returned only once.  Example::      &gt;&gt;&gt; l = nn.Linear(2, 2)     &gt;&gt;&gt; net = nn.Sequential(l, l)     &gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):     ...     print(idx, '-&gt;', m)      0 -&gt; ('', Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     ))     1 -&gt; ('0', Linear(in_features=2, out_features=2, bias=True))</p> <p>Parameters</p> <ul> <li>memo     (Optional[Set[ForwardRef('Module')]])     \u2013 defaults to <code>None</code> </li> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> named_parameters <p>Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.</p> <p>Args:     prefix (str): prefix to prepend to all parameter names.     recurse (bool): if True, then yields parameters of this module         and all submodules. Otherwise, yields only parameters that         are direct members of this module.     remove_duplicate (bool, optional): whether to remove the duplicated         parameters in the result. Defaults to True.  Yields:     (str, Parameter): Tuple containing the name and parameter  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, param in self.named_parameters():     &gt;&gt;&gt;     if name in ['bias']:     &gt;&gt;&gt;         print(param.size())</p> <p>Parameters</p> <ul> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> parameters <p>Return an iterator over module parameters.</p> <p>This is typically passed to an optimizer.  Args:     recurse (bool): if True, then yields parameters of this module         and all submodules. Otherwise, yields only parameters that         are direct members of this module.  Yields:     Parameter: module parameter  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for param in model.parameters():     &gt;&gt;&gt;     print(type(param), param.size())      (20L,)      (20L, 1L, 5L, 5L) <p>Parameters</p> <ul> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> </ul> register_backward_hook <p>Register a backward hook on the module.</p> <p>This function is deprecated in favor of :meth:<code>~torch.nn.Module.register_full_backward_hook</code> and the behavior of this function will change in future versions.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]])    </li> </ul> register_buffer <p>Add a buffer to the module.</p> <p>This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's <code>running_mean</code> is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:<code>persistent</code> to <code>False</code>. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:<code>state_dict</code>.  Buffers can be accessed as attributes using given names.  Args:     name (str): name of the buffer. The buffer can be accessed         from this module using the given name     tensor (Tensor or None): buffer to be registered. If <code>None</code>, then operations         that run on buffers, such as :attr:<code>cuda</code>, are ignored. If <code>None</code>,         the buffer is not included in the module's :attr:<code>state_dict</code>.     persistent (bool): whether the buffer is part of this module's         :attr:<code>state_dict</code>.  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; self.register_buffer('running_mean', torch.zeros(num_features))</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>tensor     (Optional[torch.Tensor])    </li> <li>persistent     (bool)     \u2013 defaults to <code>True</code> </li> </ul> register_forward_hook <p>Register a forward hook on the module.</p> <p>The hook will be called every time after :func:<code>forward</code> has computed an output.  If <code>with_kwargs</code> is <code>False</code> or not specified, the input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the <code>forward</code>. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func:<code>forward</code> is called. The hook should have the following signature::      hook(module, args, output) -&gt; None or modified output  If <code>with_kwargs</code> is <code>True</code>, the forward hook will be passed the <code>kwargs</code> given to the forward function and be expected to return the output possibly modified. The hook should have the following signature::      hook(module, args, kwargs, output) -&gt; None or modified output  Args:     hook (Callable): The user defined hook to be registered.     prepend (bool): If <code>True</code>, the provided <code>hook</code> will be fired         before all existing <code>forward</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>forward</code> hooks on         this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>forward</code> hooks registered with         :func:<code>register_module_forward_hook</code> will fire before all hooks         registered by this method.         Default: <code>False</code>     with_kwargs (bool): If <code>True</code>, the <code>hook</code> will be passed the         kwargs given to the forward function.         Default: <code>False</code>     always_call (bool): If <code>True</code> the <code>hook</code> will be run regardless of         whether an exception is raised while calling the Module.         Default: <code>False</code>  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> <li>with_kwargs     (bool)     \u2013 defaults to <code>False</code> </li> <li>always_call     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_forward_pre_hook <p>Register a forward pre-hook on the module.</p> <p>The hook will be called every time before :func:<code>forward</code> is invoked.  If <code>with_kwargs</code> is false or not specified, the input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the <code>forward</code>. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned (unless that value is already a tuple). The hook should have the following signature::      hook(module, args) -&gt; None or modified input  If <code>with_kwargs</code> is true, the forward pre-hook will be passed the kwargs given to the forward function. And if the hook modifies the input, both the args and kwargs should be returned. The hook should have the following signature::      hook(module, args, kwargs) -&gt; None or a tuple of modified input and kwargs  Args:     hook (Callable): The user defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>forward_pre</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>forward_pre</code> hooks         on this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>forward_pre</code> hooks registered with         :func:<code>register_module_forward_pre_hook</code> will fire before all         hooks registered by this method.         Default: <code>False</code>     with_kwargs (bool): If true, the <code>hook</code> will be passed the kwargs         given to the forward function.         Default: <code>False</code>  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> <li>with_kwargs     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_full_backward_hook <p>Register a backward hook on the module.</p> <p>The hook will be called every time the gradients with respect to a module are computed, i.e. the hook will execute if and only if the gradients with respect to module outputs are computed. The hook should have the following signature::      hook(module, grad_input, grad_output) -&gt; tuple(Tensor) or None  The :attr:<code>grad_input</code> and :attr:<code>grad_output</code> are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr:<code>grad_input</code> in subsequent computations. :attr:<code>grad_input</code> will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr:<code>grad_input</code> and :attr:<code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.  For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function.  .. warning ::     Modifying inputs or outputs inplace is not allowed when using backward hooks and     will raise an error.  Args:     hook (Callable): The user-defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>backward</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>backward</code> hooks on         this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>backward</code> hooks registered with         :func:<code>register_module_full_backward_hook</code> will fire before         all hooks registered by this method.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_full_backward_pre_hook <p>Register a backward pre-hook on the module.</p> <p>The hook will be called every time the gradients for the module are computed. The hook should have the following signature::      hook(module, grad_output) -&gt; tuple[Tensor] or None  The :attr:<code>grad_output</code> is a tuple. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the output that will be used in place of :attr:<code>grad_output</code> in subsequent computations. Entries in :attr:<code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.  For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function.  .. warning ::     Modifying inputs inplace is not allowed when using backward hooks and     will raise an error.  Args:     hook (Callable): The user-defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>backward_pre</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>backward_pre</code> hooks         on this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>backward_pre</code> hooks registered with         :func:<code>register_module_full_backward_pre_hook</code> will fire before         all hooks registered by this method.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_load_state_dict_post_hook <p>Register a post-hook to be run after module's :meth:<code>~nn.Module.load_state_dict</code> is called.</p> <p>It should have the following signature::     hook(module, incompatible_keys) -&gt; None  The <code>module</code> argument is the current module that this hook is registered on, and the <code>incompatible_keys</code> argument is a <code>NamedTuple</code> consisting of attributes <code>missing_keys</code> and <code>unexpected_keys</code>. <code>missing_keys</code> is a <code>list</code> of <code>str</code> containing the missing keys and <code>unexpected_keys</code> is a <code>list</code> of <code>str</code> containing the unexpected keys.  The given incompatible_keys can be modified inplace if needed.  Note that the checks performed when calling :func:<code>load_state_dict</code> with <code>strict=True</code> are affected by modifications the hook makes to <code>missing_keys</code> or <code>unexpected_keys</code>, as expected. Additions to either set of keys will result in an error being thrown when <code>strict=True</code>, and clearing out both missing and unexpected keys will avoid an error.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook </li> </ul> register_load_state_dict_pre_hook <p>Register a pre-hook to be run before module's :meth:<code>~nn.Module.load_state_dict</code> is called.</p> <p>It should have the following signature::     hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -&gt; None  # noqa: B950  Arguments:     hook (Callable): Callable hook that will be invoked before         loading the state dict.</p> <p>Parameters</p> <ul> <li>hook </li> </ul> register_module <p>Alias for :func:<code>add_module</code>.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>module     (Optional[ForwardRef('Module')])    </li> </ul> register_parameter <p>Add a parameter to the module.</p> <p>The parameter can be accessed as an attribute using given name.  Args:     name (str): name of the parameter. The parameter can be accessed         from this module using the given name     param (Parameter or None): parameter to be added to the module. If         <code>None</code>, then operations that run on parameters, such as :attr:<code>cuda</code>,         are ignored. If <code>None</code>, the parameter is not included in the         module's :attr:<code>state_dict</code>.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>param     (Optional[torch.nn.parameter.Parameter])    </li> </ul> register_state_dict_post_hook <p>Register a post-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method.</p> <p>It should have the following signature::     hook(module, state_dict, prefix, local_metadata) -&gt; None  The registered hooks can modify the <code>state_dict</code> inplace.</p> <p>Parameters</p> <ul> <li>hook </li> </ul> register_state_dict_pre_hook <p>Register a pre-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method.</p> <p>It should have the following signature::     hook(module, prefix, keep_vars) -&gt; None  The registered hooks can be used to perform pre-processing before the <code>state_dict</code> call is made.</p> <p>Parameters</p> <ul> <li>hook </li> </ul> requires_grad_ <p>Change if autograd should record operations on parameters in this module.</p> <p>This method sets the parameters' :attr:<code>requires_grad</code> attributes in-place.  This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).  See :ref:<code>locally-disable-grad-doc</code> for a comparison between <code>.requires_grad_()</code> and several similar mechanisms that may be confused with it.  Args:     requires_grad (bool): whether autograd should record operations on                           parameters in this module. Default: <code>True</code>.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>requires_grad     (bool)     \u2013 defaults to <code>True</code> </li> </ul> set_extra_state <p>Set extra state contained in the loaded <code>state_dict</code>.</p> <p>This function is called from :func:<code>load_state_dict</code> to handle any extra state found within the <code>state_dict</code>. Implement this function and a corresponding :func:<code>get_extra_state</code> for your module if you need to store extra state within its <code>state_dict</code>.  Args:     state (dict): Extra state from the <code>state_dict</code></p> <p>Parameters</p> <ul> <li>state     (Any)    </li> </ul> set_submodule <p>Set the submodule given by <code>target</code> if it exists, otherwise throw an error.</p> <p>For example, let's say you have an <code>nn.Module</code> <code>A</code> that looks like this:  .. code-block:: text      A(         (net_b): Module(             (net_c): Module(                 (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))             )             (linear): Linear(in_features=100, out_features=200, bias=True)         )     )  (The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)  To overide the <code>Conv2d</code> with a new submodule <code>Linear</code>, you would call <code>set_submodule(\"net_b.net_c.conv\", nn.Linear(33, 16))</code>.  Args:     target: The fully-qualified string name of the submodule         to look for. (See above example for how to specify a         fully-qualified string.)     module: The module to set the submodule to.  Raises:     ValueError: If the target string is empty     AttributeError: If the target string references an invalid         path or resolves to something that is not an         <code>nn.Module</code></p> <p>Parameters</p> <ul> <li>target     (str)    </li> <li>module     ('Module')    </li> </ul> share_memory <p>See :meth:<code>torch.Tensor.share_memory_</code>.</p> state_dict <p>Return a dictionary containing references to the whole state of the module.</p> <p>Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to <code>None</code> are not included.  .. note::     The returned object is a shallow copy. It contains references     to the module's parameters and buffers.  .. warning::     Currently <code>state_dict()</code> also accepts positional arguments for     <code>destination</code>, <code>prefix</code> and <code>keep_vars</code> in order. However,     this is being deprecated and keyword arguments will be enforced in     future releases.  .. warning::     Please avoid the use of argument <code>destination</code> as it is not     designed for end-users.  Args:     destination (dict, optional): If provided, the state of module will         be updated into the dict and the same object is returned.         Otherwise, an <code>OrderedDict</code> will be created and returned.         Default: <code>None</code>.     prefix (str, optional): a prefix added to parameter and buffer         names to compose the keys in state_dict. Default: <code>''</code>.     keep_vars (bool, optional): by default the :class:<code>~torch.Tensor</code> s         returned in the state dict are detached from autograd. If it's         set to <code>True</code>, detaching will not be performed.         Default: <code>False</code>.  Returns:     dict:         a dictionary containing a whole state of the module  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; module.state_dict().keys()     ['bias', 'weight']</p> <p>Parameters</p> <ul> <li>args </li> <li>destination     \u2013 defaults to <code>None</code> </li> <li>prefix     \u2013 defaults to ``    </li> <li>keep_vars     \u2013 defaults to <code>False</code> </li> </ul> to <p>Move and/or cast the parameters and buffers.</p> <p>This can be called as  .. function:: to(device=None, dtype=None, non_blocking=False)    :noindex:  .. function:: to(dtype, non_blocking=False)    :noindex:  .. function:: to(tensor, non_blocking=False)    :noindex:  .. function:: to(memory_format=torch.channels_last)    :noindex:  Its signature is similar to :meth:<code>torch.Tensor.to</code>, but only accepts floating point or complex :attr:<code>dtype</code>\\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr:<code>dtype</code> (if given). The integral parameters and buffers will be moved :attr:<code>device</code>, if that is given, but with dtypes unchanged. When :attr:<code>non_blocking</code> is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices.  See below for examples.  .. note::     This method modifies the module in-place.  Args:     device (:class:<code>torch.device</code>): the desired device of the parameters         and buffers in this module     dtype (:class:<code>torch.dtype</code>): the desired floating point or complex dtype of         the parameters and buffers in this module     tensor (torch.Tensor): Tensor whose dtype and device are the desired         dtype and device for all parameters and buffers in this module     memory_format (:class:<code>torch.memory_format</code>): the desired memory         format for 4D parameters and buffers in this module (keyword         only argument)  Returns:     Module: self  Examples::      &gt;&gt;&gt; # xdoctest: +IGNORE_WANT(\"non-deterministic\")     &gt;&gt;&gt; linear = nn.Linear(2, 2)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1913, -0.3420],             [-0.5113, -0.2325]])     &gt;&gt;&gt; linear.to(torch.double)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1913, -0.3420],             [-0.5113, -0.2325]], dtype=torch.float64)     &gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)     &gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")     &gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1914, -0.3420],             [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')     &gt;&gt;&gt; cpu = torch.device(\"cpu\")     &gt;&gt;&gt; linear.to(cpu)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1914, -0.3420],             [-0.5112, -0.2324]], dtype=torch.float16)      &gt;&gt;&gt; linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.3741+0.j,  0.2382+0.j],             [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)     &gt;&gt;&gt; linear(torch.ones(3, 2, dtype=torch.cdouble))     tensor([[0.6122+0.j, 0.1150+0.j],             [0.6122+0.j, 0.1150+0.j],             [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)</p> <p>Parameters</p> <ul> <li>args </li> <li>kwargs </li> </ul> to_empty <p>Move the parameters and buffers to the specified device without copying storage.</p> <p>Args:     device (:class:<code>torch.device</code>): The desired device of the parameters         and buffers in this module.     recurse (bool): Whether parameters and buffers of submodules should         be recursively moved to the specified device.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, str, torch.device, NoneType])    </li> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> </ul> train <p>Set the module in training mode.</p> <p>This has an effect only on certain modules. See the documentation of particular modules for details of their behaviors in training/evaluation mode, i.e., whether they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>, etc.  Args:     mode (bool): whether to set training mode (<code>True</code>) or evaluation                  mode (<code>False</code>). Default: <code>True</code>.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>mode     (bool)     \u2013 defaults to <code>True</code> </li> </ul> type <p>Casts all parameters and buffers to :attr:<code>dst_type</code>.</p> <p>.. note::     This method modifies the module in-place.  Args:     dst_type (type or string): the desired type  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>dst_type     (Union[torch.dtype, str])    </li> </ul> xpu <p>Move all model parameters and buffers to the XPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized.  .. note::     This method modifies the module in-place.  Arguments:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, torch.device, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> zero_grad <p>Reset gradients of all model parameters.</p> <p>See similar function under :class:<code>torch.optim.Optimizer</code> for more context.  Args:     set_to_none (bool): instead of setting to zero, set the grads to None.         See :meth:<code>torch.optim.Optimizer.zero_grad</code> for details.</p> <p>Parameters</p> <ul> <li>set_to_none     (bool)     \u2013 defaults to <code>True</code> </li> </ul>"},{"location":"api/losses/Contrastive/","title":"Contrastive","text":"<p>Contrastive loss. Expects as input two texts and a label of either 0 or 1. If the label == 1, then the distance between the two embeddings is reduced. If the label == 0, then the distance between the embeddings is increased.</p>"},{"location":"api/losses/Contrastive/#parameters","title":"Parameters","text":"<ul> <li> <p>model ('ColBERT')</p> <p>ColBERT model.</p> </li> <li> <p>score_metric \u2013 defaults to <code>&lt;function colbert_scores at 0x13facb560&gt;</code></p> <p>ColBERT scoring function. Defaults to colbert_scores.</p> </li> <li> <p>size_average ('bool') \u2013 defaults to <code>True</code></p> <p>Average by the size of the mini-batch.</p> </li> <li> <p>gather_across_devices ('bool') \u2013 defaults to <code>False</code></p> <p>Whether to gather the embeddings across devices to have more in batch negatives. We recommand making sure the sampling across GPUs use the same dataset in case of multi-dataset training to make sure the negatives are plausible.</p> </li> <li> <p>temperature ('float') \u2013 defaults to <code>1.0</code></p> </li> </ul>"},{"location":"api/losses/Contrastive/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from pylate import models, losses\n\n&gt;&gt;&gt; model = models.ColBERT(\n...     model_name_or_path=\"sentence-transformers/all-MiniLM-L6-v2\", device=\"cpu\"\n... )\n\n&gt;&gt;&gt; loss = losses.Contrastive(model=model)\n\n&gt;&gt;&gt; anchor = model.tokenize([\n...     \"fruits are healthy.\",\n... ], is_query=True)\n\n&gt;&gt;&gt; positive = model.tokenize([\n...     \"fruits are good for health.\",\n... ], is_query=False)\n\n&gt;&gt;&gt; negative = model.tokenize([\n...     \"fruits are bad for health.\",\n... ], is_query=False)\n\n&gt;&gt;&gt; sentence_features = [anchor, positive, negative]\n\n&gt;&gt;&gt; loss = loss(sentence_features=sentence_features)\n&gt;&gt;&gt; assert isinstance(loss.item(), float)\n</code></pre>"},{"location":"api/losses/Contrastive/#methods","title":"Methods","text":"call <p>Call self as a function.</p> <p>Parameters</p> <ul> <li>args </li> <li>kwargs </li> </ul> add_module <p>Add a child module to the current module.</p> <p>The module can be accessed as an attribute using the given name.  Args:     name (str): name of the child module. The child module can be         accessed from this module using the given name     module (Module): child module to be added to the module.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>module     (Optional[ForwardRef('Module')])    </li> </ul> apply <p>Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self.</p> <p>Typical use includes initializing the parameters of a model (see also :ref:<code>nn-init-doc</code>).  Args:     fn (:class:<code>Module</code> -&gt; None): function to be applied to each submodule  Returns:     Module: self  Example::      &gt;&gt;&gt; @torch.no_grad()     &gt;&gt;&gt; def init_weights(m):     &gt;&gt;&gt;     print(m)     &gt;&gt;&gt;     if type(m) == nn.Linear:     &gt;&gt;&gt;         m.weight.fill_(1.0)     &gt;&gt;&gt;         print(m.weight)     &gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))     &gt;&gt;&gt; net.apply(init_weights)     Linear(in_features=2, out_features=2, bias=True)     Parameter containing:     tensor([[1., 1.],             [1., 1.]], requires_grad=True)     Linear(in_features=2, out_features=2, bias=True)     Parameter containing:     tensor([[1., 1.],             [1., 1.]], requires_grad=True)     Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     )</p> <p>Parameters</p> <ul> <li>fn     (Callable[[ForwardRef('Module')], NoneType])    </li> </ul> bfloat16 <p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> buffers <p>Return an iterator over module buffers.</p> <p>Args:     recurse (bool): if True, then yields buffers of this module         and all submodules. Otherwise, yields only buffers that         are direct members of this module.  Yields:     torch.Tensor: module buffer  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for buf in model.buffers():     &gt;&gt;&gt;     print(type(buf), buf.size())      (20L,)      (20L, 1L, 5L, 5L) <p>Parameters</p> <ul> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> </ul> children <p>Return an iterator over immediate children modules.</p> <p>Yields:     Module: a child module</p> compile <p>Compile this Module's forward using :func:<code>torch.compile</code>.</p> <p>This Module's <code>__call__</code> method is compiled and all arguments are passed as-is to :func:<code>torch.compile</code>.  See :func:<code>torch.compile</code> for details on the arguments for this function.</p> <p>Parameters</p> <ul> <li>args </li> <li>kwargs </li> </ul> cpu <p>Move all model parameters and buffers to the CPU.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> cuda <p>Move all model parameters and buffers to the GPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing the optimizer if the module will live on GPU while being optimized.  .. note::     This method modifies the module in-place.  Args:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, torch.device, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> double <p>Casts all floating point parameters and buffers to <code>double</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> eval <p>Set the module in evaluation mode.</p> <p>This has an effect only on certain modules. See the documentation of particular modules for details of their behaviors in training/evaluation mode, i.e. whether they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>, etc.  This is equivalent with :meth:<code>self.train(False) &lt;torch.nn.Module.train&gt;</code>.  See :ref:<code>locally-disable-grad-doc</code> for a comparison between <code>.eval()</code> and several similar mechanisms that may be confused with it.  Returns:     Module: self</p> extra_repr <p>Return the extra representation of the module.</p> <p>To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.</p> float <p>Casts all floating point parameters and buffers to <code>float</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> forward <p>Compute the Constrastive loss.</p> <p>Parameters</p> <ul> <li>sentence_features     ('Iterable[dict[str, Tensor]]')    </li> <li>labels     ('torch.Tensor | None')     \u2013 defaults to <code>None</code> </li> </ul> get_buffer <p>Return the buffer given by <code>target</code> if it exists, otherwise throw an error.</p> <p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method's functionality as well as how to correctly specify <code>target</code>.  Args:     target: The fully-qualified string name of the buffer         to look for. (See <code>get_submodule</code> for how to specify a         fully-qualified string.)  Returns:     torch.Tensor: The buffer referenced by <code>target</code>  Raises:     AttributeError: If the target string references an invalid         path or resolves to something that is not a         buffer</p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> get_extra_state <p>Return any extra state to include in the module's state_dict.</p> <p>Implement this and a corresponding :func:<code>set_extra_state</code> for your module if you need to store extra state. This function is called when building the module's <code>state_dict()</code>.  Note that extra state should be picklable to ensure working serialization of the state_dict. We only provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes.  Returns:     object: Any extra state to store in the module's state_dict</p> get_parameter <p>Return the parameter given by <code>target</code> if it exists, otherwise throw an error.</p> <p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method's functionality as well as how to correctly specify <code>target</code>.  Args:     target: The fully-qualified string name of the Parameter         to look for. (See <code>get_submodule</code> for how to specify a         fully-qualified string.)  Returns:     torch.nn.Parameter: The Parameter referenced by <code>target</code>  Raises:     AttributeError: If the target string references an invalid         path or resolves to something that is not an         <code>nn.Parameter</code></p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> get_submodule <p>Return the submodule given by <code>target</code> if it exists, otherwise throw an error.</p> <p>For example, let's say you have an <code>nn.Module</code> <code>A</code> that looks like this:  .. code-block:: text      A(         (net_b): Module(             (net_c): Module(                 (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))             )             (linear): Linear(in_features=100, out_features=200, bias=True)         )     )  (The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> which has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)  To check whether or not we have the <code>linear</code> submodule, we would call <code>get_submodule(\"net_b.linear\")</code>. To check whether we have the <code>conv</code> submodule, we would call <code>get_submodule(\"net_b.net_c.conv\")</code>.  The runtime of <code>get_submodule</code> is bounded by the degree of module nesting in <code>target</code>. A query against <code>named_modules</code> achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, <code>get_submodule</code> should always be used.  Args:     target: The fully-qualified string name of the submodule         to look for. (See above example for how to specify a         fully-qualified string.)  Returns:     torch.nn.Module: The submodule referenced by <code>target</code>  Raises:     AttributeError: If the target string references an invalid         path or resolves to something that is not an         <code>nn.Module</code></p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> half <p>Casts all floating point parameters and buffers to <code>half</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> ipu <p>Move all model parameters and buffers to the IPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing the optimizer if the module will live on IPU while being optimized.  .. note::     This method modifies the module in-place.  Arguments:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, torch.device, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> load_state_dict <p>Copy parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants.</p> <p>If :attr:<code>strict</code> is <code>True</code>, then the keys of :attr:<code>state_dict</code> must exactly match the keys returned by this module's :meth:<code>~torch.nn.Module.state_dict</code> function.  .. warning::     If :attr:<code>assign</code> is <code>True</code> the optimizer must be created after     the call to :attr:<code>load_state_dict</code> unless     :func:<code>~torch.__future__.get_swap_module_params_on_conversion</code> is <code>True</code>.  Args:     state_dict (dict): a dict containing parameters and         persistent buffers.     strict (bool, optional): whether to strictly enforce that the keys         in :attr:<code>state_dict</code> match the keys returned by this module's         :meth:<code>~torch.nn.Module.state_dict</code> function. Default: <code>True</code>     assign (bool, optional): When set to <code>False</code>, the properties of the tensors         in the current module are preserved whereas setting it to <code>True</code> preserves         properties of the Tensors in the state dict. The only         exception is the <code>requires_grad</code> field of :class:<code>~torch.nn.Parameter</code>s         for which the value from the module is preserved.         Default: <code>False</code>  Returns:     <code>NamedTuple</code> with <code>missing_keys</code> and <code>unexpected_keys</code> fields:         * missing_keys is a list of str containing any keys that are expected             by this module but missing from the provided <code>state_dict</code>.         * unexpected_keys is a list of str containing the keys that are not             expected by this module but present in the provided <code>state_dict</code>.  Note:     If a parameter or buffer is registered as <code>None</code> and its corresponding key     exists in :attr:<code>state_dict</code>, :meth:<code>load_state_dict</code> will raise a     <code>RuntimeError</code>.</p> <p>Parameters</p> <ul> <li>state_dict     (Mapping[str, Any])    </li> <li>strict     (bool)     \u2013 defaults to <code>True</code> </li> <li>assign     (bool)     \u2013 defaults to <code>False</code> </li> </ul> modules <p>Return an iterator over all modules in the network.</p> <p>Yields:     Module: a module in the network  Note:     Duplicate modules are returned only once. In the following     example, <code>l</code> will be returned only once.  Example::      &gt;&gt;&gt; l = nn.Linear(2, 2)     &gt;&gt;&gt; net = nn.Sequential(l, l)     &gt;&gt;&gt; for idx, m in enumerate(net.modules()):     ...     print(idx, '-&gt;', m)      0 -&gt; Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     )     1 -&gt; Linear(in_features=2, out_features=2, bias=True)</p> mtia <p>Move all model parameters and buffers to the MTIA.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing the optimizer if the module will live on MTIA while being optimized.  .. note::     This method modifies the module in-place.  Arguments:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, torch.device, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> named_buffers <p>Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.</p> <p>Args:     prefix (str): prefix to prepend to all buffer names.     recurse (bool, optional): if True, then yields buffers of this module         and all submodules. Otherwise, yields only buffers that         are direct members of this module. Defaults to True.     remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.  Yields:     (str, torch.Tensor): Tuple containing the name and buffer  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, buf in self.named_buffers():     &gt;&gt;&gt;     if name in ['running_var']:     &gt;&gt;&gt;         print(buf.size())</p> <p>Parameters</p> <ul> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> named_children <p>Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.</p> <p>Yields:     (str, Module): Tuple containing a name and child module  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, module in model.named_children():     &gt;&gt;&gt;     if name in ['conv4', 'conv5']:     &gt;&gt;&gt;         print(module)</p> named_modules <p>Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.</p> <p>Args:     memo: a memo to store the set of modules already added to the result     prefix: a prefix that will be added to the name of the module     remove_duplicate: whether to remove the duplicated module instances in the result         or not  Yields:     (str, Module): Tuple of name and module  Note:     Duplicate modules are returned only once. In the following     example, <code>l</code> will be returned only once.  Example::      &gt;&gt;&gt; l = nn.Linear(2, 2)     &gt;&gt;&gt; net = nn.Sequential(l, l)     &gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):     ...     print(idx, '-&gt;', m)      0 -&gt; ('', Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     ))     1 -&gt; ('0', Linear(in_features=2, out_features=2, bias=True))</p> <p>Parameters</p> <ul> <li>memo     (Optional[Set[ForwardRef('Module')]])     \u2013 defaults to <code>None</code> </li> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> named_parameters <p>Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.</p> <p>Args:     prefix (str): prefix to prepend to all parameter names.     recurse (bool): if True, then yields parameters of this module         and all submodules. Otherwise, yields only parameters that         are direct members of this module.     remove_duplicate (bool, optional): whether to remove the duplicated         parameters in the result. Defaults to True.  Yields:     (str, Parameter): Tuple containing the name and parameter  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, param in self.named_parameters():     &gt;&gt;&gt;     if name in ['bias']:     &gt;&gt;&gt;         print(param.size())</p> <p>Parameters</p> <ul> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> parameters <p>Return an iterator over module parameters.</p> <p>This is typically passed to an optimizer.  Args:     recurse (bool): if True, then yields parameters of this module         and all submodules. Otherwise, yields only parameters that         are direct members of this module.  Yields:     Parameter: module parameter  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for param in model.parameters():     &gt;&gt;&gt;     print(type(param), param.size())      (20L,)      (20L, 1L, 5L, 5L) <p>Parameters</p> <ul> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> </ul> register_backward_hook <p>Register a backward hook on the module.</p> <p>This function is deprecated in favor of :meth:<code>~torch.nn.Module.register_full_backward_hook</code> and the behavior of this function will change in future versions.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]])    </li> </ul> register_buffer <p>Add a buffer to the module.</p> <p>This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's <code>running_mean</code> is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:<code>persistent</code> to <code>False</code>. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:<code>state_dict</code>.  Buffers can be accessed as attributes using given names.  Args:     name (str): name of the buffer. The buffer can be accessed         from this module using the given name     tensor (Tensor or None): buffer to be registered. If <code>None</code>, then operations         that run on buffers, such as :attr:<code>cuda</code>, are ignored. If <code>None</code>,         the buffer is not included in the module's :attr:<code>state_dict</code>.     persistent (bool): whether the buffer is part of this module's         :attr:<code>state_dict</code>.  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; self.register_buffer('running_mean', torch.zeros(num_features))</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>tensor     (Optional[torch.Tensor])    </li> <li>persistent     (bool)     \u2013 defaults to <code>True</code> </li> </ul> register_forward_hook <p>Register a forward hook on the module.</p> <p>The hook will be called every time after :func:<code>forward</code> has computed an output.  If <code>with_kwargs</code> is <code>False</code> or not specified, the input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the <code>forward</code>. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func:<code>forward</code> is called. The hook should have the following signature::      hook(module, args, output) -&gt; None or modified output  If <code>with_kwargs</code> is <code>True</code>, the forward hook will be passed the <code>kwargs</code> given to the forward function and be expected to return the output possibly modified. The hook should have the following signature::      hook(module, args, kwargs, output) -&gt; None or modified output  Args:     hook (Callable): The user defined hook to be registered.     prepend (bool): If <code>True</code>, the provided <code>hook</code> will be fired         before all existing <code>forward</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>forward</code> hooks on         this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>forward</code> hooks registered with         :func:<code>register_module_forward_hook</code> will fire before all hooks         registered by this method.         Default: <code>False</code>     with_kwargs (bool): If <code>True</code>, the <code>hook</code> will be passed the         kwargs given to the forward function.         Default: <code>False</code>     always_call (bool): If <code>True</code> the <code>hook</code> will be run regardless of         whether an exception is raised while calling the Module.         Default: <code>False</code>  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> <li>with_kwargs     (bool)     \u2013 defaults to <code>False</code> </li> <li>always_call     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_forward_pre_hook <p>Register a forward pre-hook on the module.</p> <p>The hook will be called every time before :func:<code>forward</code> is invoked.  If <code>with_kwargs</code> is false or not specified, the input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the <code>forward</code>. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned (unless that value is already a tuple). The hook should have the following signature::      hook(module, args) -&gt; None or modified input  If <code>with_kwargs</code> is true, the forward pre-hook will be passed the kwargs given to the forward function. And if the hook modifies the input, both the args and kwargs should be returned. The hook should have the following signature::      hook(module, args, kwargs) -&gt; None or a tuple of modified input and kwargs  Args:     hook (Callable): The user defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>forward_pre</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>forward_pre</code> hooks         on this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>forward_pre</code> hooks registered with         :func:<code>register_module_forward_pre_hook</code> will fire before all         hooks registered by this method.         Default: <code>False</code>     with_kwargs (bool): If true, the <code>hook</code> will be passed the kwargs         given to the forward function.         Default: <code>False</code>  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> <li>with_kwargs     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_full_backward_hook <p>Register a backward hook on the module.</p> <p>The hook will be called every time the gradients with respect to a module are computed, i.e. the hook will execute if and only if the gradients with respect to module outputs are computed. The hook should have the following signature::      hook(module, grad_input, grad_output) -&gt; tuple(Tensor) or None  The :attr:<code>grad_input</code> and :attr:<code>grad_output</code> are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr:<code>grad_input</code> in subsequent computations. :attr:<code>grad_input</code> will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr:<code>grad_input</code> and :attr:<code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.  For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function.  .. warning ::     Modifying inputs or outputs inplace is not allowed when using backward hooks and     will raise an error.  Args:     hook (Callable): The user-defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>backward</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>backward</code> hooks on         this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>backward</code> hooks registered with         :func:<code>register_module_full_backward_hook</code> will fire before         all hooks registered by this method.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_full_backward_pre_hook <p>Register a backward pre-hook on the module.</p> <p>The hook will be called every time the gradients for the module are computed. The hook should have the following signature::      hook(module, grad_output) -&gt; tuple[Tensor] or None  The :attr:<code>grad_output</code> is a tuple. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the output that will be used in place of :attr:<code>grad_output</code> in subsequent computations. Entries in :attr:<code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.  For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function.  .. warning ::     Modifying inputs inplace is not allowed when using backward hooks and     will raise an error.  Args:     hook (Callable): The user-defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>backward_pre</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>backward_pre</code> hooks         on this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>backward_pre</code> hooks registered with         :func:<code>register_module_full_backward_pre_hook</code> will fire before         all hooks registered by this method.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_load_state_dict_post_hook <p>Register a post-hook to be run after module's :meth:<code>~nn.Module.load_state_dict</code> is called.</p> <p>It should have the following signature::     hook(module, incompatible_keys) -&gt; None  The <code>module</code> argument is the current module that this hook is registered on, and the <code>incompatible_keys</code> argument is a <code>NamedTuple</code> consisting of attributes <code>missing_keys</code> and <code>unexpected_keys</code>. <code>missing_keys</code> is a <code>list</code> of <code>str</code> containing the missing keys and <code>unexpected_keys</code> is a <code>list</code> of <code>str</code> containing the unexpected keys.  The given incompatible_keys can be modified inplace if needed.  Note that the checks performed when calling :func:<code>load_state_dict</code> with <code>strict=True</code> are affected by modifications the hook makes to <code>missing_keys</code> or <code>unexpected_keys</code>, as expected. Additions to either set of keys will result in an error being thrown when <code>strict=True</code>, and clearing out both missing and unexpected keys will avoid an error.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook </li> </ul> register_load_state_dict_pre_hook <p>Register a pre-hook to be run before module's :meth:<code>~nn.Module.load_state_dict</code> is called.</p> <p>It should have the following signature::     hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -&gt; None  # noqa: B950  Arguments:     hook (Callable): Callable hook that will be invoked before         loading the state dict.</p> <p>Parameters</p> <ul> <li>hook </li> </ul> register_module <p>Alias for :func:<code>add_module</code>.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>module     (Optional[ForwardRef('Module')])    </li> </ul> register_parameter <p>Add a parameter to the module.</p> <p>The parameter can be accessed as an attribute using given name.  Args:     name (str): name of the parameter. The parameter can be accessed         from this module using the given name     param (Parameter or None): parameter to be added to the module. If         <code>None</code>, then operations that run on parameters, such as :attr:<code>cuda</code>,         are ignored. If <code>None</code>, the parameter is not included in the         module's :attr:<code>state_dict</code>.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>param     (Optional[torch.nn.parameter.Parameter])    </li> </ul> register_state_dict_post_hook <p>Register a post-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method.</p> <p>It should have the following signature::     hook(module, state_dict, prefix, local_metadata) -&gt; None  The registered hooks can modify the <code>state_dict</code> inplace.</p> <p>Parameters</p> <ul> <li>hook </li> </ul> register_state_dict_pre_hook <p>Register a pre-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method.</p> <p>It should have the following signature::     hook(module, prefix, keep_vars) -&gt; None  The registered hooks can be used to perform pre-processing before the <code>state_dict</code> call is made.</p> <p>Parameters</p> <ul> <li>hook </li> </ul> requires_grad_ <p>Change if autograd should record operations on parameters in this module.</p> <p>This method sets the parameters' :attr:<code>requires_grad</code> attributes in-place.  This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).  See :ref:<code>locally-disable-grad-doc</code> for a comparison between <code>.requires_grad_()</code> and several similar mechanisms that may be confused with it.  Args:     requires_grad (bool): whether autograd should record operations on                           parameters in this module. Default: <code>True</code>.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>requires_grad     (bool)     \u2013 defaults to <code>True</code> </li> </ul> set_extra_state <p>Set extra state contained in the loaded <code>state_dict</code>.</p> <p>This function is called from :func:<code>load_state_dict</code> to handle any extra state found within the <code>state_dict</code>. Implement this function and a corresponding :func:<code>get_extra_state</code> for your module if you need to store extra state within its <code>state_dict</code>.  Args:     state (dict): Extra state from the <code>state_dict</code></p> <p>Parameters</p> <ul> <li>state     (Any)    </li> </ul> set_submodule <p>Set the submodule given by <code>target</code> if it exists, otherwise throw an error.</p> <p>For example, let's say you have an <code>nn.Module</code> <code>A</code> that looks like this:  .. code-block:: text      A(         (net_b): Module(             (net_c): Module(                 (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))             )             (linear): Linear(in_features=100, out_features=200, bias=True)         )     )  (The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)  To overide the <code>Conv2d</code> with a new submodule <code>Linear</code>, you would call <code>set_submodule(\"net_b.net_c.conv\", nn.Linear(33, 16))</code>.  Args:     target: The fully-qualified string name of the submodule         to look for. (See above example for how to specify a         fully-qualified string.)     module: The module to set the submodule to.  Raises:     ValueError: If the target string is empty     AttributeError: If the target string references an invalid         path or resolves to something that is not an         <code>nn.Module</code></p> <p>Parameters</p> <ul> <li>target     (str)    </li> <li>module     ('Module')    </li> </ul> share_memory <p>See :meth:<code>torch.Tensor.share_memory_</code>.</p> state_dict <p>Return a dictionary containing references to the whole state of the module.</p> <p>Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to <code>None</code> are not included.  .. note::     The returned object is a shallow copy. It contains references     to the module's parameters and buffers.  .. warning::     Currently <code>state_dict()</code> also accepts positional arguments for     <code>destination</code>, <code>prefix</code> and <code>keep_vars</code> in order. However,     this is being deprecated and keyword arguments will be enforced in     future releases.  .. warning::     Please avoid the use of argument <code>destination</code> as it is not     designed for end-users.  Args:     destination (dict, optional): If provided, the state of module will         be updated into the dict and the same object is returned.         Otherwise, an <code>OrderedDict</code> will be created and returned.         Default: <code>None</code>.     prefix (str, optional): a prefix added to parameter and buffer         names to compose the keys in state_dict. Default: <code>''</code>.     keep_vars (bool, optional): by default the :class:<code>~torch.Tensor</code> s         returned in the state dict are detached from autograd. If it's         set to <code>True</code>, detaching will not be performed.         Default: <code>False</code>.  Returns:     dict:         a dictionary containing a whole state of the module  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; module.state_dict().keys()     ['bias', 'weight']</p> <p>Parameters</p> <ul> <li>args </li> <li>destination     \u2013 defaults to <code>None</code> </li> <li>prefix     \u2013 defaults to ``    </li> <li>keep_vars     \u2013 defaults to <code>False</code> </li> </ul> to <p>Move and/or cast the parameters and buffers.</p> <p>This can be called as  .. function:: to(device=None, dtype=None, non_blocking=False)    :noindex:  .. function:: to(dtype, non_blocking=False)    :noindex:  .. function:: to(tensor, non_blocking=False)    :noindex:  .. function:: to(memory_format=torch.channels_last)    :noindex:  Its signature is similar to :meth:<code>torch.Tensor.to</code>, but only accepts floating point or complex :attr:<code>dtype</code>\\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr:<code>dtype</code> (if given). The integral parameters and buffers will be moved :attr:<code>device</code>, if that is given, but with dtypes unchanged. When :attr:<code>non_blocking</code> is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices.  See below for examples.  .. note::     This method modifies the module in-place.  Args:     device (:class:<code>torch.device</code>): the desired device of the parameters         and buffers in this module     dtype (:class:<code>torch.dtype</code>): the desired floating point or complex dtype of         the parameters and buffers in this module     tensor (torch.Tensor): Tensor whose dtype and device are the desired         dtype and device for all parameters and buffers in this module     memory_format (:class:<code>torch.memory_format</code>): the desired memory         format for 4D parameters and buffers in this module (keyword         only argument)  Returns:     Module: self  Examples::      &gt;&gt;&gt; # xdoctest: +IGNORE_WANT(\"non-deterministic\")     &gt;&gt;&gt; linear = nn.Linear(2, 2)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1913, -0.3420],             [-0.5113, -0.2325]])     &gt;&gt;&gt; linear.to(torch.double)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1913, -0.3420],             [-0.5113, -0.2325]], dtype=torch.float64)     &gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)     &gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")     &gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1914, -0.3420],             [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')     &gt;&gt;&gt; cpu = torch.device(\"cpu\")     &gt;&gt;&gt; linear.to(cpu)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1914, -0.3420],             [-0.5112, -0.2324]], dtype=torch.float16)      &gt;&gt;&gt; linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.3741+0.j,  0.2382+0.j],             [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)     &gt;&gt;&gt; linear(torch.ones(3, 2, dtype=torch.cdouble))     tensor([[0.6122+0.j, 0.1150+0.j],             [0.6122+0.j, 0.1150+0.j],             [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)</p> <p>Parameters</p> <ul> <li>args </li> <li>kwargs </li> </ul> to_empty <p>Move the parameters and buffers to the specified device without copying storage.</p> <p>Args:     device (:class:<code>torch.device</code>): The desired device of the parameters         and buffers in this module.     recurse (bool): Whether parameters and buffers of submodules should         be recursively moved to the specified device.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, str, torch.device, NoneType])    </li> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> </ul> train <p>Set the module in training mode.</p> <p>This has an effect only on certain modules. See the documentation of particular modules for details of their behaviors in training/evaluation mode, i.e., whether they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>, etc.  Args:     mode (bool): whether to set training mode (<code>True</code>) or evaluation                  mode (<code>False</code>). Default: <code>True</code>.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>mode     (bool)     \u2013 defaults to <code>True</code> </li> </ul> type <p>Casts all parameters and buffers to :attr:<code>dst_type</code>.</p> <p>.. note::     This method modifies the module in-place.  Args:     dst_type (type or string): the desired type  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>dst_type     (Union[torch.dtype, str])    </li> </ul> xpu <p>Move all model parameters and buffers to the XPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized.  .. note::     This method modifies the module in-place.  Arguments:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, torch.device, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> zero_grad <p>Reset gradients of all model parameters.</p> <p>See similar function under :class:<code>torch.optim.Optimizer</code> for more context.  Args:     set_to_none (bool): instead of setting to zero, set the grads to None.         See :meth:<code>torch.optim.Optimizer.zero_grad</code> for details.</p> <p>Parameters</p> <ul> <li>set_to_none     (bool)     \u2013 defaults to <code>True</code> </li> </ul>"},{"location":"api/losses/Distillation/","title":"Distillation","text":"<p>Distillation loss for ColBERT model. The loss is computed with respect to the format of SentenceTransformer library.</p>"},{"location":"api/losses/Distillation/#parameters","title":"Parameters","text":"<ul> <li> <p>model ('ColBERT')</p> <p>SentenceTransformer model.</p> </li> <li> <p>score_metric ('Callable') \u2013 defaults to <code>&lt;function colbert_kd_scores at 0x13faed4e0&gt;</code></p> <p>Function that returns a score between two sequences of embeddings.</p> </li> <li> <p>size_average ('bool') \u2013 defaults to <code>True</code></p> <p>Average by the size of the mini-batch or perform sum.</p> </li> <li> <p>normalize_scores ('bool') \u2013 defaults to <code>True</code></p> </li> </ul>"},{"location":"api/losses/Distillation/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from pylate import models, losses\n\n&gt;&gt;&gt; model = models.ColBERT(\n...     model_name_or_path=\"sentence-transformers/all-MiniLM-L6-v2\", device=\"cpu\"\n... )\n\n&gt;&gt;&gt; distillation = losses.Distillation(model=model)\n\n&gt;&gt;&gt; query = model.tokenize([\n...     \"fruits are healthy.\",\n... ], is_query=True)\n\n&gt;&gt;&gt; documents = model.tokenize([\n...     \"fruits are good for health.\",\n...     \"fruits are bad for health.\"\n... ], is_query=False)\n\n&gt;&gt;&gt; sentence_features = [query, documents]\n\n&gt;&gt;&gt; labels = torch.tensor([\n...     [0.7, 0.3],\n... ], dtype=torch.float32)\n\n&gt;&gt;&gt; loss = distillation(sentence_features=sentence_features, labels=labels)\n\n&gt;&gt;&gt; assert isinstance(loss.item(), float)\n</code></pre>"},{"location":"api/losses/Distillation/#methods","title":"Methods","text":"call <p>Call self as a function.</p> <p>Parameters</p> <ul> <li>args </li> <li>kwargs </li> </ul> add_module <p>Add a child module to the current module.</p> <p>The module can be accessed as an attribute using the given name.  Args:     name (str): name of the child module. The child module can be         accessed from this module using the given name     module (Module): child module to be added to the module.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>module     (Optional[ForwardRef('Module')])    </li> </ul> apply <p>Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self.</p> <p>Typical use includes initializing the parameters of a model (see also :ref:<code>nn-init-doc</code>).  Args:     fn (:class:<code>Module</code> -&gt; None): function to be applied to each submodule  Returns:     Module: self  Example::      &gt;&gt;&gt; @torch.no_grad()     &gt;&gt;&gt; def init_weights(m):     &gt;&gt;&gt;     print(m)     &gt;&gt;&gt;     if type(m) == nn.Linear:     &gt;&gt;&gt;         m.weight.fill_(1.0)     &gt;&gt;&gt;         print(m.weight)     &gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))     &gt;&gt;&gt; net.apply(init_weights)     Linear(in_features=2, out_features=2, bias=True)     Parameter containing:     tensor([[1., 1.],             [1., 1.]], requires_grad=True)     Linear(in_features=2, out_features=2, bias=True)     Parameter containing:     tensor([[1., 1.],             [1., 1.]], requires_grad=True)     Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     )</p> <p>Parameters</p> <ul> <li>fn     (Callable[[ForwardRef('Module')], NoneType])    </li> </ul> bfloat16 <p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> buffers <p>Return an iterator over module buffers.</p> <p>Args:     recurse (bool): if True, then yields buffers of this module         and all submodules. Otherwise, yields only buffers that         are direct members of this module.  Yields:     torch.Tensor: module buffer  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for buf in model.buffers():     &gt;&gt;&gt;     print(type(buf), buf.size())      (20L,)      (20L, 1L, 5L, 5L) <p>Parameters</p> <ul> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> </ul> children <p>Return an iterator over immediate children modules.</p> <p>Yields:     Module: a child module</p> compile <p>Compile this Module's forward using :func:<code>torch.compile</code>.</p> <p>This Module's <code>__call__</code> method is compiled and all arguments are passed as-is to :func:<code>torch.compile</code>.  See :func:<code>torch.compile</code> for details on the arguments for this function.</p> <p>Parameters</p> <ul> <li>args </li> <li>kwargs </li> </ul> cpu <p>Move all model parameters and buffers to the CPU.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> cuda <p>Move all model parameters and buffers to the GPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing the optimizer if the module will live on GPU while being optimized.  .. note::     This method modifies the module in-place.  Args:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, torch.device, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> double <p>Casts all floating point parameters and buffers to <code>double</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> eval <p>Set the module in evaluation mode.</p> <p>This has an effect only on certain modules. See the documentation of particular modules for details of their behaviors in training/evaluation mode, i.e. whether they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>, etc.  This is equivalent with :meth:<code>self.train(False) &lt;torch.nn.Module.train&gt;</code>.  See :ref:<code>locally-disable-grad-doc</code> for a comparison between <code>.eval()</code> and several similar mechanisms that may be confused with it.  Returns:     Module: self</p> extra_repr <p>Return the extra representation of the module.</p> <p>To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.</p> float <p>Casts all floating point parameters and buffers to <code>float</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> forward <p>Computes the distillation loss with respect to SentenceTransformer.</p> <p>Parameters</p> <ul> <li>sentence_features     ('Iterable[dict[str, torch.Tensor]]')    </li> <li>labels     ('torch.Tensor')    </li> </ul> get_buffer <p>Return the buffer given by <code>target</code> if it exists, otherwise throw an error.</p> <p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method's functionality as well as how to correctly specify <code>target</code>.  Args:     target: The fully-qualified string name of the buffer         to look for. (See <code>get_submodule</code> for how to specify a         fully-qualified string.)  Returns:     torch.Tensor: The buffer referenced by <code>target</code>  Raises:     AttributeError: If the target string references an invalid         path or resolves to something that is not a         buffer</p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> get_extra_state <p>Return any extra state to include in the module's state_dict.</p> <p>Implement this and a corresponding :func:<code>set_extra_state</code> for your module if you need to store extra state. This function is called when building the module's <code>state_dict()</code>.  Note that extra state should be picklable to ensure working serialization of the state_dict. We only provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes.  Returns:     object: Any extra state to store in the module's state_dict</p> get_parameter <p>Return the parameter given by <code>target</code> if it exists, otherwise throw an error.</p> <p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method's functionality as well as how to correctly specify <code>target</code>.  Args:     target: The fully-qualified string name of the Parameter         to look for. (See <code>get_submodule</code> for how to specify a         fully-qualified string.)  Returns:     torch.nn.Parameter: The Parameter referenced by <code>target</code>  Raises:     AttributeError: If the target string references an invalid         path or resolves to something that is not an         <code>nn.Parameter</code></p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> get_submodule <p>Return the submodule given by <code>target</code> if it exists, otherwise throw an error.</p> <p>For example, let's say you have an <code>nn.Module</code> <code>A</code> that looks like this:  .. code-block:: text      A(         (net_b): Module(             (net_c): Module(                 (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))             )             (linear): Linear(in_features=100, out_features=200, bias=True)         )     )  (The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> which has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)  To check whether or not we have the <code>linear</code> submodule, we would call <code>get_submodule(\"net_b.linear\")</code>. To check whether we have the <code>conv</code> submodule, we would call <code>get_submodule(\"net_b.net_c.conv\")</code>.  The runtime of <code>get_submodule</code> is bounded by the degree of module nesting in <code>target</code>. A query against <code>named_modules</code> achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, <code>get_submodule</code> should always be used.  Args:     target: The fully-qualified string name of the submodule         to look for. (See above example for how to specify a         fully-qualified string.)  Returns:     torch.nn.Module: The submodule referenced by <code>target</code>  Raises:     AttributeError: If the target string references an invalid         path or resolves to something that is not an         <code>nn.Module</code></p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> half <p>Casts all floating point parameters and buffers to <code>half</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> ipu <p>Move all model parameters and buffers to the IPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing the optimizer if the module will live on IPU while being optimized.  .. note::     This method modifies the module in-place.  Arguments:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, torch.device, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> load_state_dict <p>Copy parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants.</p> <p>If :attr:<code>strict</code> is <code>True</code>, then the keys of :attr:<code>state_dict</code> must exactly match the keys returned by this module's :meth:<code>~torch.nn.Module.state_dict</code> function.  .. warning::     If :attr:<code>assign</code> is <code>True</code> the optimizer must be created after     the call to :attr:<code>load_state_dict</code> unless     :func:<code>~torch.__future__.get_swap_module_params_on_conversion</code> is <code>True</code>.  Args:     state_dict (dict): a dict containing parameters and         persistent buffers.     strict (bool, optional): whether to strictly enforce that the keys         in :attr:<code>state_dict</code> match the keys returned by this module's         :meth:<code>~torch.nn.Module.state_dict</code> function. Default: <code>True</code>     assign (bool, optional): When set to <code>False</code>, the properties of the tensors         in the current module are preserved whereas setting it to <code>True</code> preserves         properties of the Tensors in the state dict. The only         exception is the <code>requires_grad</code> field of :class:<code>~torch.nn.Parameter</code>s         for which the value from the module is preserved.         Default: <code>False</code>  Returns:     <code>NamedTuple</code> with <code>missing_keys</code> and <code>unexpected_keys</code> fields:         * missing_keys is a list of str containing any keys that are expected             by this module but missing from the provided <code>state_dict</code>.         * unexpected_keys is a list of str containing the keys that are not             expected by this module but present in the provided <code>state_dict</code>.  Note:     If a parameter or buffer is registered as <code>None</code> and its corresponding key     exists in :attr:<code>state_dict</code>, :meth:<code>load_state_dict</code> will raise a     <code>RuntimeError</code>.</p> <p>Parameters</p> <ul> <li>state_dict     (Mapping[str, Any])    </li> <li>strict     (bool)     \u2013 defaults to <code>True</code> </li> <li>assign     (bool)     \u2013 defaults to <code>False</code> </li> </ul> modules <p>Return an iterator over all modules in the network.</p> <p>Yields:     Module: a module in the network  Note:     Duplicate modules are returned only once. In the following     example, <code>l</code> will be returned only once.  Example::      &gt;&gt;&gt; l = nn.Linear(2, 2)     &gt;&gt;&gt; net = nn.Sequential(l, l)     &gt;&gt;&gt; for idx, m in enumerate(net.modules()):     ...     print(idx, '-&gt;', m)      0 -&gt; Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     )     1 -&gt; Linear(in_features=2, out_features=2, bias=True)</p> mtia <p>Move all model parameters and buffers to the MTIA.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing the optimizer if the module will live on MTIA while being optimized.  .. note::     This method modifies the module in-place.  Arguments:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, torch.device, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> named_buffers <p>Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.</p> <p>Args:     prefix (str): prefix to prepend to all buffer names.     recurse (bool, optional): if True, then yields buffers of this module         and all submodules. Otherwise, yields only buffers that         are direct members of this module. Defaults to True.     remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.  Yields:     (str, torch.Tensor): Tuple containing the name and buffer  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, buf in self.named_buffers():     &gt;&gt;&gt;     if name in ['running_var']:     &gt;&gt;&gt;         print(buf.size())</p> <p>Parameters</p> <ul> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> named_children <p>Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.</p> <p>Yields:     (str, Module): Tuple containing a name and child module  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, module in model.named_children():     &gt;&gt;&gt;     if name in ['conv4', 'conv5']:     &gt;&gt;&gt;         print(module)</p> named_modules <p>Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.</p> <p>Args:     memo: a memo to store the set of modules already added to the result     prefix: a prefix that will be added to the name of the module     remove_duplicate: whether to remove the duplicated module instances in the result         or not  Yields:     (str, Module): Tuple of name and module  Note:     Duplicate modules are returned only once. In the following     example, <code>l</code> will be returned only once.  Example::      &gt;&gt;&gt; l = nn.Linear(2, 2)     &gt;&gt;&gt; net = nn.Sequential(l, l)     &gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):     ...     print(idx, '-&gt;', m)      0 -&gt; ('', Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     ))     1 -&gt; ('0', Linear(in_features=2, out_features=2, bias=True))</p> <p>Parameters</p> <ul> <li>memo     (Optional[Set[ForwardRef('Module')]])     \u2013 defaults to <code>None</code> </li> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> named_parameters <p>Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.</p> <p>Args:     prefix (str): prefix to prepend to all parameter names.     recurse (bool): if True, then yields parameters of this module         and all submodules. Otherwise, yields only parameters that         are direct members of this module.     remove_duplicate (bool, optional): whether to remove the duplicated         parameters in the result. Defaults to True.  Yields:     (str, Parameter): Tuple containing the name and parameter  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, param in self.named_parameters():     &gt;&gt;&gt;     if name in ['bias']:     &gt;&gt;&gt;         print(param.size())</p> <p>Parameters</p> <ul> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> parameters <p>Return an iterator over module parameters.</p> <p>This is typically passed to an optimizer.  Args:     recurse (bool): if True, then yields parameters of this module         and all submodules. Otherwise, yields only parameters that         are direct members of this module.  Yields:     Parameter: module parameter  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for param in model.parameters():     &gt;&gt;&gt;     print(type(param), param.size())      (20L,)      (20L, 1L, 5L, 5L) <p>Parameters</p> <ul> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> </ul> register_backward_hook <p>Register a backward hook on the module.</p> <p>This function is deprecated in favor of :meth:<code>~torch.nn.Module.register_full_backward_hook</code> and the behavior of this function will change in future versions.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]])    </li> </ul> register_buffer <p>Add a buffer to the module.</p> <p>This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's <code>running_mean</code> is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:<code>persistent</code> to <code>False</code>. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:<code>state_dict</code>.  Buffers can be accessed as attributes using given names.  Args:     name (str): name of the buffer. The buffer can be accessed         from this module using the given name     tensor (Tensor or None): buffer to be registered. If <code>None</code>, then operations         that run on buffers, such as :attr:<code>cuda</code>, are ignored. If <code>None</code>,         the buffer is not included in the module's :attr:<code>state_dict</code>.     persistent (bool): whether the buffer is part of this module's         :attr:<code>state_dict</code>.  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; self.register_buffer('running_mean', torch.zeros(num_features))</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>tensor     (Optional[torch.Tensor])    </li> <li>persistent     (bool)     \u2013 defaults to <code>True</code> </li> </ul> register_forward_hook <p>Register a forward hook on the module.</p> <p>The hook will be called every time after :func:<code>forward</code> has computed an output.  If <code>with_kwargs</code> is <code>False</code> or not specified, the input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the <code>forward</code>. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func:<code>forward</code> is called. The hook should have the following signature::      hook(module, args, output) -&gt; None or modified output  If <code>with_kwargs</code> is <code>True</code>, the forward hook will be passed the <code>kwargs</code> given to the forward function and be expected to return the output possibly modified. The hook should have the following signature::      hook(module, args, kwargs, output) -&gt; None or modified output  Args:     hook (Callable): The user defined hook to be registered.     prepend (bool): If <code>True</code>, the provided <code>hook</code> will be fired         before all existing <code>forward</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>forward</code> hooks on         this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>forward</code> hooks registered with         :func:<code>register_module_forward_hook</code> will fire before all hooks         registered by this method.         Default: <code>False</code>     with_kwargs (bool): If <code>True</code>, the <code>hook</code> will be passed the         kwargs given to the forward function.         Default: <code>False</code>     always_call (bool): If <code>True</code> the <code>hook</code> will be run regardless of         whether an exception is raised while calling the Module.         Default: <code>False</code>  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> <li>with_kwargs     (bool)     \u2013 defaults to <code>False</code> </li> <li>always_call     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_forward_pre_hook <p>Register a forward pre-hook on the module.</p> <p>The hook will be called every time before :func:<code>forward</code> is invoked.  If <code>with_kwargs</code> is false or not specified, the input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the <code>forward</code>. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned (unless that value is already a tuple). The hook should have the following signature::      hook(module, args) -&gt; None or modified input  If <code>with_kwargs</code> is true, the forward pre-hook will be passed the kwargs given to the forward function. And if the hook modifies the input, both the args and kwargs should be returned. The hook should have the following signature::      hook(module, args, kwargs) -&gt; None or a tuple of modified input and kwargs  Args:     hook (Callable): The user defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>forward_pre</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>forward_pre</code> hooks         on this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>forward_pre</code> hooks registered with         :func:<code>register_module_forward_pre_hook</code> will fire before all         hooks registered by this method.         Default: <code>False</code>     with_kwargs (bool): If true, the <code>hook</code> will be passed the kwargs         given to the forward function.         Default: <code>False</code>  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> <li>with_kwargs     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_full_backward_hook <p>Register a backward hook on the module.</p> <p>The hook will be called every time the gradients with respect to a module are computed, i.e. the hook will execute if and only if the gradients with respect to module outputs are computed. The hook should have the following signature::      hook(module, grad_input, grad_output) -&gt; tuple(Tensor) or None  The :attr:<code>grad_input</code> and :attr:<code>grad_output</code> are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr:<code>grad_input</code> in subsequent computations. :attr:<code>grad_input</code> will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr:<code>grad_input</code> and :attr:<code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.  For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function.  .. warning ::     Modifying inputs or outputs inplace is not allowed when using backward hooks and     will raise an error.  Args:     hook (Callable): The user-defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>backward</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>backward</code> hooks on         this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>backward</code> hooks registered with         :func:<code>register_module_full_backward_hook</code> will fire before         all hooks registered by this method.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_full_backward_pre_hook <p>Register a backward pre-hook on the module.</p> <p>The hook will be called every time the gradients for the module are computed. The hook should have the following signature::      hook(module, grad_output) -&gt; tuple[Tensor] or None  The :attr:<code>grad_output</code> is a tuple. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the output that will be used in place of :attr:<code>grad_output</code> in subsequent computations. Entries in :attr:<code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.  For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function.  .. warning ::     Modifying inputs inplace is not allowed when using backward hooks and     will raise an error.  Args:     hook (Callable): The user-defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>backward_pre</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>backward_pre</code> hooks         on this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>backward_pre</code> hooks registered with         :func:<code>register_module_full_backward_pre_hook</code> will fire before         all hooks registered by this method.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_load_state_dict_post_hook <p>Register a post-hook to be run after module's :meth:<code>~nn.Module.load_state_dict</code> is called.</p> <p>It should have the following signature::     hook(module, incompatible_keys) -&gt; None  The <code>module</code> argument is the current module that this hook is registered on, and the <code>incompatible_keys</code> argument is a <code>NamedTuple</code> consisting of attributes <code>missing_keys</code> and <code>unexpected_keys</code>. <code>missing_keys</code> is a <code>list</code> of <code>str</code> containing the missing keys and <code>unexpected_keys</code> is a <code>list</code> of <code>str</code> containing the unexpected keys.  The given incompatible_keys can be modified inplace if needed.  Note that the checks performed when calling :func:<code>load_state_dict</code> with <code>strict=True</code> are affected by modifications the hook makes to <code>missing_keys</code> or <code>unexpected_keys</code>, as expected. Additions to either set of keys will result in an error being thrown when <code>strict=True</code>, and clearing out both missing and unexpected keys will avoid an error.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook </li> </ul> register_load_state_dict_pre_hook <p>Register a pre-hook to be run before module's :meth:<code>~nn.Module.load_state_dict</code> is called.</p> <p>It should have the following signature::     hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -&gt; None  # noqa: B950  Arguments:     hook (Callable): Callable hook that will be invoked before         loading the state dict.</p> <p>Parameters</p> <ul> <li>hook </li> </ul> register_module <p>Alias for :func:<code>add_module</code>.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>module     (Optional[ForwardRef('Module')])    </li> </ul> register_parameter <p>Add a parameter to the module.</p> <p>The parameter can be accessed as an attribute using given name.  Args:     name (str): name of the parameter. The parameter can be accessed         from this module using the given name     param (Parameter or None): parameter to be added to the module. If         <code>None</code>, then operations that run on parameters, such as :attr:<code>cuda</code>,         are ignored. If <code>None</code>, the parameter is not included in the         module's :attr:<code>state_dict</code>.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>param     (Optional[torch.nn.parameter.Parameter])    </li> </ul> register_state_dict_post_hook <p>Register a post-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method.</p> <p>It should have the following signature::     hook(module, state_dict, prefix, local_metadata) -&gt; None  The registered hooks can modify the <code>state_dict</code> inplace.</p> <p>Parameters</p> <ul> <li>hook </li> </ul> register_state_dict_pre_hook <p>Register a pre-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method.</p> <p>It should have the following signature::     hook(module, prefix, keep_vars) -&gt; None  The registered hooks can be used to perform pre-processing before the <code>state_dict</code> call is made.</p> <p>Parameters</p> <ul> <li>hook </li> </ul> requires_grad_ <p>Change if autograd should record operations on parameters in this module.</p> <p>This method sets the parameters' :attr:<code>requires_grad</code> attributes in-place.  This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).  See :ref:<code>locally-disable-grad-doc</code> for a comparison between <code>.requires_grad_()</code> and several similar mechanisms that may be confused with it.  Args:     requires_grad (bool): whether autograd should record operations on                           parameters in this module. Default: <code>True</code>.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>requires_grad     (bool)     \u2013 defaults to <code>True</code> </li> </ul> set_extra_state <p>Set extra state contained in the loaded <code>state_dict</code>.</p> <p>This function is called from :func:<code>load_state_dict</code> to handle any extra state found within the <code>state_dict</code>. Implement this function and a corresponding :func:<code>get_extra_state</code> for your module if you need to store extra state within its <code>state_dict</code>.  Args:     state (dict): Extra state from the <code>state_dict</code></p> <p>Parameters</p> <ul> <li>state     (Any)    </li> </ul> set_submodule <p>Set the submodule given by <code>target</code> if it exists, otherwise throw an error.</p> <p>For example, let's say you have an <code>nn.Module</code> <code>A</code> that looks like this:  .. code-block:: text      A(         (net_b): Module(             (net_c): Module(                 (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))             )             (linear): Linear(in_features=100, out_features=200, bias=True)         )     )  (The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)  To overide the <code>Conv2d</code> with a new submodule <code>Linear</code>, you would call <code>set_submodule(\"net_b.net_c.conv\", nn.Linear(33, 16))</code>.  Args:     target: The fully-qualified string name of the submodule         to look for. (See above example for how to specify a         fully-qualified string.)     module: The module to set the submodule to.  Raises:     ValueError: If the target string is empty     AttributeError: If the target string references an invalid         path or resolves to something that is not an         <code>nn.Module</code></p> <p>Parameters</p> <ul> <li>target     (str)    </li> <li>module     ('Module')    </li> </ul> share_memory <p>See :meth:<code>torch.Tensor.share_memory_</code>.</p> state_dict <p>Return a dictionary containing references to the whole state of the module.</p> <p>Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to <code>None</code> are not included.  .. note::     The returned object is a shallow copy. It contains references     to the module's parameters and buffers.  .. warning::     Currently <code>state_dict()</code> also accepts positional arguments for     <code>destination</code>, <code>prefix</code> and <code>keep_vars</code> in order. However,     this is being deprecated and keyword arguments will be enforced in     future releases.  .. warning::     Please avoid the use of argument <code>destination</code> as it is not     designed for end-users.  Args:     destination (dict, optional): If provided, the state of module will         be updated into the dict and the same object is returned.         Otherwise, an <code>OrderedDict</code> will be created and returned.         Default: <code>None</code>.     prefix (str, optional): a prefix added to parameter and buffer         names to compose the keys in state_dict. Default: <code>''</code>.     keep_vars (bool, optional): by default the :class:<code>~torch.Tensor</code> s         returned in the state dict are detached from autograd. If it's         set to <code>True</code>, detaching will not be performed.         Default: <code>False</code>.  Returns:     dict:         a dictionary containing a whole state of the module  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; module.state_dict().keys()     ['bias', 'weight']</p> <p>Parameters</p> <ul> <li>args </li> <li>destination     \u2013 defaults to <code>None</code> </li> <li>prefix     \u2013 defaults to ``    </li> <li>keep_vars     \u2013 defaults to <code>False</code> </li> </ul> to <p>Move and/or cast the parameters and buffers.</p> <p>This can be called as  .. function:: to(device=None, dtype=None, non_blocking=False)    :noindex:  .. function:: to(dtype, non_blocking=False)    :noindex:  .. function:: to(tensor, non_blocking=False)    :noindex:  .. function:: to(memory_format=torch.channels_last)    :noindex:  Its signature is similar to :meth:<code>torch.Tensor.to</code>, but only accepts floating point or complex :attr:<code>dtype</code>\\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr:<code>dtype</code> (if given). The integral parameters and buffers will be moved :attr:<code>device</code>, if that is given, but with dtypes unchanged. When :attr:<code>non_blocking</code> is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices.  See below for examples.  .. note::     This method modifies the module in-place.  Args:     device (:class:<code>torch.device</code>): the desired device of the parameters         and buffers in this module     dtype (:class:<code>torch.dtype</code>): the desired floating point or complex dtype of         the parameters and buffers in this module     tensor (torch.Tensor): Tensor whose dtype and device are the desired         dtype and device for all parameters and buffers in this module     memory_format (:class:<code>torch.memory_format</code>): the desired memory         format for 4D parameters and buffers in this module (keyword         only argument)  Returns:     Module: self  Examples::      &gt;&gt;&gt; # xdoctest: +IGNORE_WANT(\"non-deterministic\")     &gt;&gt;&gt; linear = nn.Linear(2, 2)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1913, -0.3420],             [-0.5113, -0.2325]])     &gt;&gt;&gt; linear.to(torch.double)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1913, -0.3420],             [-0.5113, -0.2325]], dtype=torch.float64)     &gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)     &gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")     &gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1914, -0.3420],             [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')     &gt;&gt;&gt; cpu = torch.device(\"cpu\")     &gt;&gt;&gt; linear.to(cpu)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1914, -0.3420],             [-0.5112, -0.2324]], dtype=torch.float16)      &gt;&gt;&gt; linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.3741+0.j,  0.2382+0.j],             [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)     &gt;&gt;&gt; linear(torch.ones(3, 2, dtype=torch.cdouble))     tensor([[0.6122+0.j, 0.1150+0.j],             [0.6122+0.j, 0.1150+0.j],             [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)</p> <p>Parameters</p> <ul> <li>args </li> <li>kwargs </li> </ul> to_empty <p>Move the parameters and buffers to the specified device without copying storage.</p> <p>Args:     device (:class:<code>torch.device</code>): The desired device of the parameters         and buffers in this module.     recurse (bool): Whether parameters and buffers of submodules should         be recursively moved to the specified device.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, str, torch.device, NoneType])    </li> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> </ul> train <p>Set the module in training mode.</p> <p>This has an effect only on certain modules. See the documentation of particular modules for details of their behaviors in training/evaluation mode, i.e., whether they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>, etc.  Args:     mode (bool): whether to set training mode (<code>True</code>) or evaluation                  mode (<code>False</code>). Default: <code>True</code>.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>mode     (bool)     \u2013 defaults to <code>True</code> </li> </ul> type <p>Casts all parameters and buffers to :attr:<code>dst_type</code>.</p> <p>.. note::     This method modifies the module in-place.  Args:     dst_type (type or string): the desired type  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>dst_type     (Union[torch.dtype, str])    </li> </ul> xpu <p>Move all model parameters and buffers to the XPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized.  .. note::     This method modifies the module in-place.  Arguments:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, torch.device, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> zero_grad <p>Reset gradients of all model parameters.</p> <p>See similar function under :class:<code>torch.optim.Optimizer</code> for more context.  Args:     set_to_none (bool): instead of setting to zero, set the grads to None.         See :meth:<code>torch.optim.Optimizer.zero_grad</code> for details.</p> <p>Parameters</p> <ul> <li>set_to_none     (bool)     \u2013 defaults to <code>True</code> </li> </ul>"},{"location":"api/models/ColBERT/","title":"ColBERT","text":"<p>Loads or creates a ColBERT model that can be used to map sentences / text to multi-vectors embeddings.</p>"},{"location":"api/models/ColBERT/#parameters","title":"Parameters","text":"<ul> <li> <p>model_name_or_path ('str | None') \u2013 defaults to <code>None</code></p> <p>If it is a filepath on disc, it loads the model from that path. If it is not a path, it first tries to download a pre-trained SentenceTransformer model. If that fails, tries to construct a model from the Hugging Face Hub with that name.</p> </li> <li> <p>modules ('Optional[Iterable[nn.Module]]') \u2013 defaults to <code>None</code></p> <p>A list of torch Modules that should be called sequentially, can be used to create custom SentenceTransformer models from scratch.</p> </li> <li> <p>device ('str | None') \u2013 defaults to <code>None</code></p> <p>Device (like \"cuda\", \"cpu\", \"mps\", \"npu\") that should be used for computation. If None, checks if a GPU can be used.</p> </li> <li> <p>prompts ('dict[str, str] | None') \u2013 defaults to <code>None</code></p> <p>A dictionary with prompts for the model. The key is the prompt name, the value is the prompt text. The prompt text will be prepended before any text to encode. For example: <code>{\"query\": \"query: \", \"passage\": \"passage: \"}</code> or <code>{\"clustering\": \"Identify the main category based on the titles in \"}</code>.</p> </li> <li> <p>default_prompt_name ('str | None') \u2013 defaults to <code>None</code></p> <p>The name of the prompt that should be used by default. If not set, no prompt will be applied.</p> </li> <li> <p>similarity_fn_name ('Optional[str | SimilarityFunction]') \u2013 defaults to <code>None</code></p> <p>The name of the similarity function to use. Valid options are \"cosine\", \"dot\", \"euclidean\", and \"manhattan\". If not set, it is automatically set to \"cosine\" if <code>similarity</code> or <code>similarity_pairwise</code> are called while <code>model.similarity_fn_name</code> is still <code>None</code>.</p> </li> <li> <p>cache_folder ('str | None') \u2013 defaults to <code>None</code></p> <p>Path to store models. Can also be set by the SENTENCE_TRANSFORMERS_HOME environment variable.</p> </li> <li> <p>trust_remote_code ('bool') \u2013 defaults to <code>False</code></p> <p>Whether or not to allow for custom models defined on the Hub in their own modeling files. This option should only be set to True for repositories you trust and in which you have read the code, as it will execute code present on the Hub on your local machine.</p> </li> <li> <p>revision ('str | None') \u2013 defaults to <code>None</code></p> <p>The specific model version to use. It can be a branch name, a tag name, or a commit id, for a stored model on Hugging Face.</p> </li> <li> <p>local_files_only ('bool') \u2013 defaults to <code>False</code></p> <p>Whether or not to only look at local files (i.e., do not try to download the model).</p> </li> <li> <p>token ('bool | str | None') \u2013 defaults to <code>None</code></p> <p>Hugging Face authentication token to download private models.</p> </li> <li> <p>use_auth_token ('bool | str | None') \u2013 defaults to <code>None</code></p> <p>Deprecated argument. Please use <code>token</code> instead.</p> </li> <li> <p>truncate_dim ('int | None') \u2013 defaults to <code>None</code></p> <p>The dimension to truncate sentence embeddings to. <code>None</code> does no truncation. Truncation is only applicable during inference when :meth:<code>SentenceTransformer.encode</code> is called.</p> </li> <li> <p>embedding_size ('int | None') \u2013 defaults to <code>None</code></p> <p>The output size of the projection layer. Default to 128.</p> </li> <li> <p>bias ('bool') \u2013 defaults to <code>False</code></p> </li> <li> <p>query_prefix ('str | None') \u2013 defaults to <code>None</code></p> <p>Prefix to add to the queries.</p> </li> <li> <p>document_prefix ('str | None') \u2013 defaults to <code>None</code></p> <p>Prefix to add to the documents.</p> </li> <li> <p>add_special_tokens ('bool') \u2013 defaults to <code>True</code></p> <p>Add the prefix to the inputs.</p> </li> <li> <p>truncation ('bool') \u2013 defaults to <code>True</code></p> <p>Truncate the inputs to the encoder max lengths or use sliding window encoding.</p> </li> <li> <p>query_length ('int | None') \u2013 defaults to <code>None</code></p> <p>The length of the query to truncate/pad to with mask tokens. If set, will override the config value. Default to 32.</p> </li> <li> <p>document_length ('int | None') \u2013 defaults to <code>None</code></p> <p>The max length of the document to truncate. If set, will override the config value. Default to 180.</p> </li> <li> <p>attend_to_expansion_tokens ('bool | None') \u2013 defaults to <code>None</code></p> <p>Whether to attend to the expansion tokens in the attention layers model. If False, the original tokens will not only attend to the expansion tokens, only the expansion tokens will attend to the original tokens. Default is False (as in the original ColBERT codebase).</p> </li> <li> <p>skiplist_words ('list[str] | None') \u2013 defaults to <code>None</code></p> <p>A list of words to skip from the documents scoring (note that these tokens are used for encoding and are only skipped during the scoring). Default is the list of string.punctuation.</p> </li> <li> <p>model_kwargs ('dict | None') \u2013 defaults to <code>None</code></p> <p>Additional model configuration parameters to be passed to the Huggingface Transformers model. Particularly useful options are:  - <code>torch_dtype</code>: Override the default <code>torch.dtype</code> and load the model under a specific <code>dtype</code>. The     different options are:          1. <code>torch.float16</code>, <code>torch.bfloat16</code> or <code>torch.float</code>: load in a specified <code>dtype</code>,         ignoring the model's <code>config.torch_dtype</code> if one exists. If not specified - the model will get         loaded in <code>torch.float</code> (fp32).          2. <code>\"auto\"</code> - A <code>torch_dtype</code> entry in the <code>config.json</code> file of the model will be attempted         to be used. If this entry isn't found then next check the <code>dtype</code> of the first weight in the         checkpoint that's of a floating point type and use that as <code>dtype</code>. This will load the model using         the <code>dtype</code> it was saved in at the end of the training. It can't be used as an indicator of how the         model was trained. Since it could be trained in one of half precision dtypes, but saved in fp32. - <code>attn_implementation</code>: The attention implementation to use in the model (if relevant). Can be any of     <code>\"eager\"</code> (manual implementation of the attention), <code>\"sdpa\"</code> (using <code>F.scaled_dot_product_attention     &lt;https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html&gt;</code>),     or <code>\"flash_attention_2\"</code> (using <code>Dao-AILab/flash-attention     &lt;https://github.com/Dao-AILab/flash-attention&gt;</code>). By default, if available, SDPA will be used for     torch&gt;=2.1.1. The default is otherwise the manual <code>\"eager\"</code> implementation.  See the <code>PreTrainedModel.from_pretrained &lt;https://huggingface.co/docs/transformers/en/main_classes/model#transformers.PreTrainedModel.from_pretrained&gt;</code>_ documentation for more details.</p> </li> <li> <p>tokenizer_kwargs ('dict | None') \u2013 defaults to <code>None</code></p> <p>Additional tokenizer configuration parameters to be passed to the Huggingface Transformers tokenizer. See the <code>AutoTokenizer.from_pretrained &lt;https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained&gt;</code>_ documentation for more details.</p> </li> <li> <p>config_kwargs ('dict | None') \u2013 defaults to <code>None</code></p> <p>Additional model configuration parameters to be passed to the Huggingface Transformers config. See the <code>AutoConfig.from_pretrained &lt;https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoConfig.from_pretrained&gt;</code>_ documentation for more details.</p> </li> <li> <p>model_card_data ('PylateModelCardData | None') \u2013 defaults to <code>None</code></p> <p>A model card data object that contains information about the model. This is used to generate a model card when saving the model. If not set, a default model card data object is created.</p> </li> </ul>"},{"location":"api/models/ColBERT/#attributes","title":"Attributes","text":"<ul> <li> <p>device</p> <p>Get torch.device from module, assuming that the whole module has one device. In case there are no PyTorch parameters, fall back to CPU.</p> </li> <li> <p>max_seq_length</p> <p>Returns the maximal input sequence length for the model. Longer inputs will be truncated.  Returns:     int: The maximal input sequence length.  Example:     ::          from sentence_transformers import SentenceTransformer          model = SentenceTransformer(\"all-mpnet-base-v2\")         print(model.max_seq_length)         # =&gt; 384</p> </li> <li> <p>similarity</p> <p>Compute the similarity between two collections of embeddings. The output will be a matrix with the similarity scores between all embeddings from the first parameter and all embeddings from the second parameter. This differs from <code>similarity_pairwise</code> which computes the similarity between each pair of embeddings. This method supports only embeddings with fp32 precision and does not accommodate quantized embeddings.  Args:     embeddings1 (Union[Tensor, ndarray]): [num_embeddings_1, embedding_dim] or [embedding_dim]-shaped numpy array or torch tensor.     embeddings2 (Union[Tensor, ndarray]): [num_embeddings_2, embedding_dim] or [embedding_dim]-shaped numpy array or torch tensor.  Returns:     Tensor: A [num_embeddings_1, num_embeddings_2]-shaped torch tensor with similarity scores.  Example:     ::          &gt;&gt;&gt; model = SentenceTransformer(\"all-mpnet-base-v2\")         &gt;&gt;&gt; sentences = [         ...     \"The weather is so nice!\",         ...     \"It's so sunny outside.\",         ...     \"He's driving to the movie theater.\",         ...     \"She's going to the cinema.\",         ... ]         &gt;&gt;&gt; embeddings = model.encode(sentences, normalize_embeddings=True)         &gt;&gt;&gt; model.similarity(embeddings, embeddings)         tensor([[1.0000, 0.7235, 0.0290, 0.1309],                 [0.7235, 1.0000, 0.0613, 0.1129],                 [0.0290, 0.0613, 1.0000, 0.5027],                 [0.1309, 0.1129, 0.5027, 1.0000]])         &gt;&gt;&gt; model.similarity_fn_name         \"cosine\"         &gt;&gt;&gt; model.similarity_fn_name = \"euclidean\"         &gt;&gt;&gt; model.similarity(embeddings, embeddings)         tensor([[-0.0000, -0.7437, -1.3935, -1.3184],                 [-0.7437, -0.0000, -1.3702, -1.3320],                 [-1.3935, -1.3702, -0.0000, -0.9973],                 [-1.3184, -1.3320, -0.9973, -0.0000]])</p> </li> <li> <p>similarity_fn_name</p> <p>Return the name of the similarity function used by :meth:<code>SentenceTransformer.similarity</code> and :meth:<code>SentenceTransformer.similarity_pairwise</code>.  Returns:     Optional[str]: The name of the similarity function. Can be None if not set, in which case it will         default to \"cosine\" when first called. Examples -------- &gt;&gt;&gt; model = ColBERT(\"bert-base-uncased\") &gt;&gt;&gt; model.similarity_fn_name     'MaxSim'</p> </li> <li> <p>similarity_pairwise</p> <p>Compute the similarity between two collections of embeddings. The output will be a vector with the similarity scores between each pair of embeddings. This method supports only embeddings with fp32 precision and does not accommodate quantized embeddings.  Args:     embeddings1 (Union[Tensor, ndarray]): [num_embeddings, embedding_dim] or [embedding_dim]-shaped numpy array or torch tensor.     embeddings2 (Union[Tensor, ndarray]): [num_embeddings, embedding_dim] or [embedding_dim]-shaped numpy array or torch tensor.  Returns:     Tensor: A [num_embeddings]-shaped torch tensor with pairwise similarity scores.  Example:     ::          &gt;&gt;&gt; model = SentenceTransformer(\"all-mpnet-base-v2\")         &gt;&gt;&gt; sentences = [         ...     \"The weather is so nice!\",         ...     \"It's so sunny outside.\",         ...     \"He's driving to the movie theater.\",         ...     \"She's going to the cinema.\",         ... ]         &gt;&gt;&gt; embeddings = model.encode(sentences, normalize_embeddings=True)         &gt;&gt;&gt; model.similarity_pairwise(embeddings[::2], embeddings[1::2])         tensor([0.7235, 0.5027])         &gt;&gt;&gt; model.similarity_fn_name         \"cosine\"         &gt;&gt;&gt; model.similarity_fn_name = \"euclidean\"         &gt;&gt;&gt; model.similarity_pairwise(embeddings[::2], embeddings[1::2])         tensor([-0.7437, -0.9973])</p> </li> <li> <p>tokenizer</p> <p>Property to get the tokenizer that is used by this model</p> </li> </ul>"},{"location":"api/models/ColBERT/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from pylate import models\n\n&gt;&gt;&gt; model = models.ColBERT(\n...     model_name_or_path=\"sentence-transformers/all-MiniLM-L6-v2\",\n...     device=\"cpu\",\n... )\n\n&gt;&gt;&gt; embeddings = model.encode(\"Hello, how are you?\")\n&gt;&gt;&gt; assert isinstance(embeddings, np.ndarray)\n\n&gt;&gt;&gt; embeddings = model.encode([\n...     \"Hello, how are you?\",\n...     \"How is the weather today?\"\n... ])\n\n&gt;&gt;&gt; assert len(embeddings) == 2\n&gt;&gt;&gt; assert isinstance(embeddings[0], np.ndarray)\n&gt;&gt;&gt; assert isinstance(embeddings[1], np.ndarray)\n\n&gt;&gt;&gt; embeddings = model.encode([\n...     [\n...         \"Hello, how are you?\",\n...         \"How is the weather today?\"\n...     ],\n...     [\n...         \"Hello, how are you?\",\n...         \"How is the weather today?\"\n...     ],\n... ])\n\n&gt;&gt;&gt; assert len(embeddings) == 2\n\n&gt;&gt;&gt; model.save_pretrained(\"test-model\")\n\n&gt;&gt;&gt; model = models.ColBERT(\"test-model\")\n\n&gt;&gt;&gt; embeddings = model.encode([\n...     \"Hello, how are you?\",\n...     \"How is the weather today?\"\n... ])\n\n&gt;&gt;&gt; assert len(embeddings) == 2\n&gt;&gt;&gt; assert isinstance(embeddings[0], np.ndarray)\n&gt;&gt;&gt; assert isinstance(embeddings[1], np.ndarray)\n</code></pre>"},{"location":"api/models/ColBERT/#methods","title":"Methods","text":"call <p>Call self as a function.</p> <p>Parameters</p> <ul> <li>args </li> <li>kwargs </li> </ul> active_adapter active_adapters <p>If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the PEFT official documentation: https://huggingface.co/docs/peft</p> <p>Gets the current active adapters of the model. In case of multi-adapter inference (combining multiple adapters for inference) returns the list of all active adapters so that users can deal with them accordingly.  For previous PEFT versions (that does not support multi-adapter inference), <code>module.active_adapter</code> will return a single string.</p> add_adapter <p>Adds a fresh new adapter to the current model for training purposes. If no adapter name is passed, a default name is assigned to the adapter to follow the convention of PEFT library (in PEFT we use \"default\" as the default adapter name).</p> <p>Requires peft as a backend to load the adapter weights and the underlying model to be compatible with PEFT.  Args:     args:         Positional arguments to pass to the underlying AutoModel <code>add_adapter</code> function. More information can be found in the transformers documentation         https://huggingface.co/docs/transformers/main/en/peft#transformers.integrations.PeftAdapterMixin.add_adapter     *kwargs:         Keyword arguments to pass to the underlying AutoModel <code>add_adapter</code> function. More information can be found in the transformers documentation         https://huggingface.co/docs/transformers/main/en/peft#transformers.integrations.PeftAdapterMixin.add_adapter</p> <p>Parameters</p> <ul> <li>args </li> <li>kwargs </li> </ul> add_module <p>Add a child module to the current module.</p> <p>The module can be accessed as an attribute using the given name.  Args:     name (str): name of the child module. The child module can be         accessed from this module using the given name     module (Module): child module to be added to the module.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>module     (Optional[ForwardRef('Module')])    </li> </ul> append <p>Append a given module to the end.</p> <p>Args:     module (nn.Module): module to append</p> <p>Parameters</p> <ul> <li>module     (torch.nn.modules.module.Module)    </li> </ul> apply <p>Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self.</p> <p>Typical use includes initializing the parameters of a model (see also :ref:<code>nn-init-doc</code>).  Args:     fn (:class:<code>Module</code> -&gt; None): function to be applied to each submodule  Returns:     Module: self  Example::      &gt;&gt;&gt; @torch.no_grad()     &gt;&gt;&gt; def init_weights(m):     &gt;&gt;&gt;     print(m)     &gt;&gt;&gt;     if type(m) == nn.Linear:     &gt;&gt;&gt;         m.weight.fill_(1.0)     &gt;&gt;&gt;         print(m.weight)     &gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))     &gt;&gt;&gt; net.apply(init_weights)     Linear(in_features=2, out_features=2, bias=True)     Parameter containing:     tensor([[1., 1.],             [1., 1.]], requires_grad=True)     Linear(in_features=2, out_features=2, bias=True)     Parameter containing:     tensor([[1., 1.],             [1., 1.]], requires_grad=True)     Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     )</p> <p>Parameters</p> <ul> <li>fn     (Callable[[ForwardRef('Module')], NoneType])    </li> </ul> bfloat16 <p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> buffers <p>Return an iterator over module buffers.</p> <p>Args:     recurse (bool): if True, then yields buffers of this module         and all submodules. Otherwise, yields only buffers that         are direct members of this module.  Yields:     torch.Tensor: module buffer  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for buf in model.buffers():     &gt;&gt;&gt;     print(type(buf), buf.size())      (20L,)      (20L, 1L, 5L, 5L) <p>Parameters</p> <ul> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> </ul> check_peft_compatible_model children <p>Return an iterator over immediate children modules.</p> <p>Yields:     Module: a child module</p> compile <p>Compile this Module's forward using :func:<code>torch.compile</code>.</p> <p>This Module's <code>__call__</code> method is compiled and all arguments are passed as-is to :func:<code>torch.compile</code>.  See :func:<code>torch.compile</code> for details on the arguments for this function.</p> <p>Parameters</p> <ul> <li>args </li> <li>kwargs </li> </ul> cpu <p>Move all model parameters and buffers to the CPU.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> cuda <p>Move all model parameters and buffers to the GPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing the optimizer if the module will live on GPU while being optimized.  .. note::     This method modifies the module in-place.  Args:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, torch.device, NoneType])     \u2013 defaults to <code>None</code>      Device (like \"cuda\", \"cpu\", \"mps\", \"npu\") that should be used for computation. If None, checks if a GPU can be used.</li> </ul> disable_adapters <p>Disable all adapters that are attached to the model. This leads to inferring with the base model only.</p> double <p>Casts all floating point parameters and buffers to <code>double</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> enable_adapters <p>Enable adapters that are attached to the model. The model will use <code>self.active_adapter()</code></p> encode <p>Computes sentence embeddings.</p> <p>Parameters</p> <ul> <li>sentences     ('str | list[str]')    </li> <li>prompt_name     ('str | None')     \u2013 defaults to <code>None</code> </li> <li>prompt     ('str | None')     \u2013 defaults to <code>None</code> </li> <li>batch_size     ('int')     \u2013 defaults to <code>32</code> </li> <li>show_progress_bar     ('bool')     \u2013 defaults to <code>None</code> </li> <li>precision     (\"Literal['float32', 'int8', 'uint8', 'binary', 'ubinary']\")     \u2013 defaults to <code>float32</code> </li> <li>convert_to_numpy     ('bool')     \u2013 defaults to <code>True</code> </li> <li>convert_to_tensor     ('bool')     \u2013 defaults to <code>False</code> </li> <li>padding     ('bool')     \u2013 defaults to <code>False</code> </li> <li>device     ('str')     \u2013 defaults to <code>None</code>      Device (like \"cuda\", \"cpu\", \"mps\", \"npu\") that should be used for computation. If None, checks if a GPU can be used.</li> <li>normalize_embeddings     ('bool')     \u2013 defaults to <code>True</code> </li> <li>is_query     ('bool')     \u2013 defaults to <code>True</code> </li> <li>pool_factor     ('int')     \u2013 defaults to <code>1</code> </li> <li>protected_tokens     ('int')     \u2013 defaults to <code>1</code> </li> </ul> encode_multi_process <p>Encodes a list of sentences using multiple processes and GPUs via :meth:<code>SentenceTransformer.encode &lt;sentence_transformers.SentenceTransformer.encode&gt;</code>. The sentences are chunked into smaller packages and sent to individual processes, which encode them on different GPUs or CPUs. This method is only suitable for encoding large sets of sentences.</p> <p>Parameters</p> <ul> <li>sentences     ('list[str]')    </li> <li>pool     ('dict[str, object]')    </li> <li>prompt_name     ('str | None')     \u2013 defaults to <code>None</code> </li> <li>prompt     ('str | None')     \u2013 defaults to <code>None</code> </li> <li>batch_size     ('int')     \u2013 defaults to <code>32</code> </li> <li>chunk_size     ('int')     \u2013 defaults to <code>None</code> </li> <li>precision     (\"Literal['float32', 'int8', 'uint8', 'binary', 'ubinary']\")     \u2013 defaults to <code>float32</code> </li> <li>normalize_embeddings     ('bool')     \u2013 defaults to <code>True</code> </li> <li>padding     ('bool')     \u2013 defaults to <code>False</code> </li> <li>is_query     ('bool')     \u2013 defaults to <code>True</code> </li> <li>pool_factor     ('int')     \u2013 defaults to <code>1</code> </li> <li>protected_tokens     ('int')     \u2013 defaults to <code>1</code> </li> </ul> eval <p>Set the module in evaluation mode.</p> <p>This has an effect only on certain modules. See the documentation of particular modules for details of their behaviors in training/evaluation mode, i.e. whether they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>, etc.  This is equivalent with :meth:<code>self.train(False) &lt;torch.nn.Module.train&gt;</code>.  See :ref:<code>locally-disable-grad-doc</code> for a comparison between <code>.eval()</code> and several similar mechanisms that may be confused with it.  Returns:     Module: self</p> evaluate <p>Evaluate the model based on an evaluator</p> <p>Args:     evaluator (SentenceEvaluator): The evaluator used to evaluate the model.     output_path (str, optional): The path where the evaluator can write the results. Defaults to None.  Returns:     The evaluation results.</p> <p>Parameters</p> <ul> <li>evaluator     ('SentenceEvaluator')    </li> <li>output_path     ('str')     \u2013 defaults to <code>None</code> </li> </ul> extend extra_repr <p>Return the extra representation of the module.</p> <p>To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.</p> fit <p>Deprecated training method from before Sentence Transformers v3.0, it is recommended to use :class:<code>~sentence_transformers.trainer.SentenceTransformerTrainer</code> instead. This method uses :class:<code>~sentence_transformers.trainer.SentenceTransformerTrainer</code> behind the scenes, but does not provide as much flexibility as the Trainer itself.</p> <p>This training approach uses a list of DataLoaders and Loss functions to train the model. Each DataLoader is sampled in turn for one batch. We sample only as many batches from each DataLoader as there are in the smallest one to make sure of equal training with each dataset, i.e. round robin sampling.  This method should produce equivalent results in v3.0+ as before v3.0, but if you encounter any issues with your existing training scripts, then you may wish to use :meth:<code>SentenceTransformer.old_fit &lt;sentence_transformers.SentenceTransformer.old_fit&gt;</code> instead. That uses the old training method from before v3.0.  Args:     train_objectives: Tuples of (DataLoader, LossFunction). Pass         more than one for multi-task learning     evaluator: An evaluator (sentence_transformers.evaluation)         evaluates the model performance during training on held-         out dev data. It is used to determine the best model         that is saved to disk.     epochs: Number of epochs for training     steps_per_epoch: Number of training steps per epoch. If set         to None (default), one epoch is equal the DataLoader         size from train_objectives.     scheduler: Learning rate scheduler. Available schedulers:         constantlr, warmupconstant, warmuplinear, warmupcosine,         warmupcosinewithhardrestarts     warmup_steps: Behavior depends on the scheduler. For         WarmupLinear (default), the learning rate is increased         from o up to the maximal learning rate. After these many         training steps, the learning rate is decreased linearly         back to zero.     optimizer_class: Optimizer     optimizer_params: Optimizer parameters     weight_decay: Weight decay for model parameters     evaluation_steps: If &gt; 0, evaluate the model using evaluator         after each number of training steps     output_path: Storage path for the model and evaluation files     save_best_model: If true, the best model (according to         evaluator) is stored at output_path     max_grad_norm: Used for gradient normalization.     use_amp: Use Automatic Mixed Precision (AMP). Only for         Pytorch &gt;= 1.6.0     callback: Callback function that is invoked after each         evaluation. It must accept the following three         parameters in this order: <code>score</code>, <code>epoch</code>, <code>steps</code>     show_progress_bar: If True, output a tqdm progress bar     checkpoint_path: Folder to save checkpoints during training     checkpoint_save_steps: Will save a checkpoint after so many         steps     checkpoint_save_total_limit: Total number of checkpoints to         store     resume_from_checkpoint: If true, searches for checkpoints         to continue training from.</p> <p>Parameters</p> <ul> <li>train_objectives     ('Iterable[tuple[DataLoader, nn.Module]]')    </li> <li>evaluator     ('SentenceEvaluator')     \u2013 defaults to <code>None</code> </li> <li>epochs     ('int')     \u2013 defaults to <code>1</code> </li> <li>steps_per_epoch     \u2013 defaults to <code>None</code> </li> <li>scheduler     ('str')     \u2013 defaults to <code>WarmupLinear</code> </li> <li>warmup_steps     ('int')     \u2013 defaults to <code>10000</code> </li> <li>optimizer_class     ('type[Optimizer]')     \u2013 defaults to <code>&lt;class 'torch.optim.adamw.AdamW'&gt;</code> </li> <li>optimizer_params     ('dict[str, object]')     \u2013 defaults to <code>{'lr': 2e-05}</code> </li> <li>weight_decay     ('float')     \u2013 defaults to <code>0.01</code> </li> <li>evaluation_steps     ('int')     \u2013 defaults to <code>0</code> </li> <li>output_path     ('str')     \u2013 defaults to <code>None</code> </li> <li>save_best_model     ('bool')     \u2013 defaults to <code>True</code> </li> <li>max_grad_norm     ('float')     \u2013 defaults to <code>1</code> </li> <li>use_amp     ('bool')     \u2013 defaults to <code>False</code> </li> <li>callback     ('Callable[[float, int, int], None]')     \u2013 defaults to <code>None</code> </li> <li>show_progress_bar     ('bool')     \u2013 defaults to <code>True</code> </li> <li>checkpoint_path     ('str')     \u2013 defaults to <code>None</code> </li> <li>checkpoint_save_steps     ('int')     \u2013 defaults to <code>500</code> </li> <li>checkpoint_save_total_limit     ('int')     \u2013 defaults to <code>0</code> </li> <li>resume_from_checkpoint     ('bool')     \u2013 defaults to <code>False</code> </li> </ul> float <p>Casts all floating point parameters and buffers to <code>float</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> forward <p>Define the computation performed at every call.</p> <p>Should be overridden by all subclasses.  .. note::     Although the recipe for forward pass needs to be defined within     this function, one should call the :class:<code>Module</code> instance afterwards     instead of this since the former takes care of running the     registered hooks while the latter silently ignores them.</p> <p>Parameters</p> <ul> <li>input     ('dict[str, Tensor]')    </li> <li>kwargs </li> </ul> get_adapter_state_dict <p>If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the PEFT official documentation: https://huggingface.co/docs/peft</p> <p>Gets the adapter state dict that should only contain the weights tensors of the specified adapter_name adapter. If no adapter_name is passed, the active adapter is used.  Args:     args:         Positional arguments to pass to the underlying AutoModel <code>get_adapter_state_dict</code> function. More information can be found in the transformers documentation         https://huggingface.co/docs/transformers/main/en/peft#transformers.integrations.PeftAdapterMixin.get_adapter_state_dict     *kwargs:         Keyword arguments to pass to the underlying AutoModel <code>get_adapter_state_dict</code> function. More information can be found in the transformers documentation         https://huggingface.co/docs/transformers/main/en/peft#transformers.integrations.PeftAdapterMixin.get_adapter_state_dict</p> <p>Parameters</p> <ul> <li>args </li> <li>kwargs </li> </ul> get_backend <p>Return the backend used for inference, which can be one of \"torch\", \"onnx\", or \"openvino\".</p> <p>Returns:     str: The backend used for inference.</p> get_buffer <p>Return the buffer given by <code>target</code> if it exists, otherwise throw an error.</p> <p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method's functionality as well as how to correctly specify <code>target</code>.  Args:     target: The fully-qualified string name of the buffer         to look for. (See <code>get_submodule</code> for how to specify a         fully-qualified string.)  Returns:     torch.Tensor: The buffer referenced by <code>target</code>  Raises:     AttributeError: If the target string references an invalid         path or resolves to something that is not a         buffer</p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> get_extra_state <p>Return any extra state to include in the module's state_dict.</p> <p>Implement this and a corresponding :func:<code>set_extra_state</code> for your module if you need to store extra state. This function is called when building the module's <code>state_dict()</code>.  Note that extra state should be picklable to ensure working serialization of the state_dict. We only provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes.  Returns:     object: Any extra state to store in the module's state_dict</p> get_max_seq_length <p>Returns the maximal sequence length that the model accepts. Longer inputs will be truncated.</p> <p>Returns:     Optional[int]: The maximal sequence length that the model accepts, or None if it is not defined.</p> get_parameter <p>Return the parameter given by <code>target</code> if it exists, otherwise throw an error.</p> <p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method's functionality as well as how to correctly specify <code>target</code>.  Args:     target: The fully-qualified string name of the Parameter         to look for. (See <code>get_submodule</code> for how to specify a         fully-qualified string.)  Returns:     torch.nn.Parameter: The Parameter referenced by <code>target</code>  Raises:     AttributeError: If the target string references an invalid         path or resolves to something that is not an         <code>nn.Parameter</code></p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> get_sentence_embedding_dimension <p>Returns the number of dimensions in the output of :meth:<code>SentenceTransformer.encode &lt;sentence_transformers.SentenceTransformer.encode&gt;</code>.</p> <p>Returns:     Optional[int]: The number of dimensions in the output of <code>encode</code>. If it's not known, it's <code>None</code>.</p> get_sentence_features get_submodule <p>Return the submodule given by <code>target</code> if it exists, otherwise throw an error.</p> <p>For example, let's say you have an <code>nn.Module</code> <code>A</code> that looks like this:  .. code-block:: text      A(         (net_b): Module(             (net_c): Module(                 (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))             )             (linear): Linear(in_features=100, out_features=200, bias=True)         )     )  (The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> which has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)  To check whether or not we have the <code>linear</code> submodule, we would call <code>get_submodule(\"net_b.linear\")</code>. To check whether we have the <code>conv</code> submodule, we would call <code>get_submodule(\"net_b.net_c.conv\")</code>.  The runtime of <code>get_submodule</code> is bounded by the degree of module nesting in <code>target</code>. A query against <code>named_modules</code> achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, <code>get_submodule</code> should always be used.  Args:     target: The fully-qualified string name of the submodule         to look for. (See above example for how to specify a         fully-qualified string.)  Returns:     torch.nn.Module: The submodule referenced by <code>target</code>  Raises:     AttributeError: If the target string references an invalid         path or resolves to something that is not an         <code>nn.Module</code></p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> gradient_checkpointing_enable half <p>Casts all floating point parameters and buffers to <code>half</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> has_peft_compatible_model insert insert_prefix_token <p>Inserts a prefix token at the beginning of each sequence in the input tensor.</p> <p>Parameters</p> <ul> <li>input_ids     ('torch.Tensor')    </li> <li>prefix_id     ('int')    </li> </ul> ipu <p>Move all model parameters and buffers to the IPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing the optimizer if the module will live on IPU while being optimized.  .. note::     This method modifies the module in-place.  Arguments:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, torch.device, NoneType])     \u2013 defaults to <code>None</code>      Device (like \"cuda\", \"cpu\", \"mps\", \"npu\") that should be used for computation. If None, checks if a GPU can be used.</li> </ul> load load_adapter <p>Load adapter weights from file or remote Hub folder.\" If you are not familiar with adapters and PEFT methods, we invite you to read more about them on PEFT official documentation: https://huggingface.co/docs/peft</p> <p>Requires peft as a backend to load the adapter weights and the underlying model to be compatible with PEFT.  Args:     args:         Positional arguments to pass to the underlying AutoModel <code>load_adapter</code> function. More information can be found in the transformers documentation         https://huggingface.co/docs/transformers/main/en/peft#transformers.integrations.PeftAdapterMixin.load_adapter     *kwargs:         Keyword arguments to pass to the underlying AutoModel <code>load_adapter</code> function. More information can be found in the transformers documentation         https://huggingface.co/docs/transformers/main/en/peft#transformers.integrations.PeftAdapterMixin.load_adapter</p> <p>Parameters</p> <ul> <li>args </li> <li>kwargs </li> </ul> load_state_dict <p>Copy parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants.</p> <p>If :attr:<code>strict</code> is <code>True</code>, then the keys of :attr:<code>state_dict</code> must exactly match the keys returned by this module's :meth:<code>~torch.nn.Module.state_dict</code> function.  .. warning::     If :attr:<code>assign</code> is <code>True</code> the optimizer must be created after     the call to :attr:<code>load_state_dict</code> unless     :func:<code>~torch.__future__.get_swap_module_params_on_conversion</code> is <code>True</code>.  Args:     state_dict (dict): a dict containing parameters and         persistent buffers.     strict (bool, optional): whether to strictly enforce that the keys         in :attr:<code>state_dict</code> match the keys returned by this module's         :meth:<code>~torch.nn.Module.state_dict</code> function. Default: <code>True</code>     assign (bool, optional): When set to <code>False</code>, the properties of the tensors         in the current module are preserved whereas setting it to <code>True</code> preserves         properties of the Tensors in the state dict. The only         exception is the <code>requires_grad</code> field of :class:<code>~torch.nn.Parameter</code>s         for which the value from the module is preserved.         Default: <code>False</code>  Returns:     <code>NamedTuple</code> with <code>missing_keys</code> and <code>unexpected_keys</code> fields:         * missing_keys is a list of str containing any keys that are expected             by this module but missing from the provided <code>state_dict</code>.         * unexpected_keys is a list of str containing the keys that are not             expected by this module but present in the provided <code>state_dict</code>.  Note:     If a parameter or buffer is registered as <code>None</code> and its corresponding key     exists in :attr:<code>state_dict</code>, :meth:<code>load_state_dict</code> will raise a     <code>RuntimeError</code>.</p> <p>Parameters</p> <ul> <li>state_dict     (Mapping[str, Any])    </li> <li>strict     (bool)     \u2013 defaults to <code>True</code> </li> <li>assign     (bool)     \u2013 defaults to <code>False</code> </li> </ul> modules <p>Return an iterator over all modules in the network.</p> <p>Yields:     Module: a module in the network  Note:     Duplicate modules are returned only once. In the following     example, <code>l</code> will be returned only once.  Example::      &gt;&gt;&gt; l = nn.Linear(2, 2)     &gt;&gt;&gt; net = nn.Sequential(l, l)     &gt;&gt;&gt; for idx, m in enumerate(net.modules()):     ...     print(idx, '-&gt;', m)      0 -&gt; Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     )     1 -&gt; Linear(in_features=2, out_features=2, bias=True)</p> mtia <p>Move all model parameters and buffers to the MTIA.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing the optimizer if the module will live on MTIA while being optimized.  .. note::     This method modifies the module in-place.  Arguments:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, torch.device, NoneType])     \u2013 defaults to <code>None</code>      Device (like \"cuda\", \"cpu\", \"mps\", \"npu\") that should be used for computation. If None, checks if a GPU can be used.</li> </ul> named_buffers <p>Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.</p> <p>Args:     prefix (str): prefix to prepend to all buffer names.     recurse (bool, optional): if True, then yields buffers of this module         and all submodules. Otherwise, yields only buffers that         are direct members of this module. Defaults to True.     remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.  Yields:     (str, torch.Tensor): Tuple containing the name and buffer  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, buf in self.named_buffers():     &gt;&gt;&gt;     if name in ['running_var']:     &gt;&gt;&gt;         print(buf.size())</p> <p>Parameters</p> <ul> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> named_children <p>Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.</p> <p>Yields:     (str, Module): Tuple containing a name and child module  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, module in model.named_children():     &gt;&gt;&gt;     if name in ['conv4', 'conv5']:     &gt;&gt;&gt;         print(module)</p> named_modules <p>Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.</p> <p>Args:     memo: a memo to store the set of modules already added to the result     prefix: a prefix that will be added to the name of the module     remove_duplicate: whether to remove the duplicated module instances in the result         or not  Yields:     (str, Module): Tuple of name and module  Note:     Duplicate modules are returned only once. In the following     example, <code>l</code> will be returned only once.  Example::      &gt;&gt;&gt; l = nn.Linear(2, 2)     &gt;&gt;&gt; net = nn.Sequential(l, l)     &gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):     ...     print(idx, '-&gt;', m)      0 -&gt; ('', Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     ))     1 -&gt; ('0', Linear(in_features=2, out_features=2, bias=True))</p> <p>Parameters</p> <ul> <li>memo     (Optional[Set[ForwardRef('Module')]])     \u2013 defaults to <code>None</code> </li> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> named_parameters <p>Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.</p> <p>Args:     prefix (str): prefix to prepend to all parameter names.     recurse (bool): if True, then yields parameters of this module         and all submodules. Otherwise, yields only parameters that         are direct members of this module.     remove_duplicate (bool, optional): whether to remove the duplicated         parameters in the result. Defaults to True.  Yields:     (str, Parameter): Tuple containing the name and parameter  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, param in self.named_parameters():     &gt;&gt;&gt;     if name in ['bias']:     &gt;&gt;&gt;         print(param.size())</p> <p>Parameters</p> <ul> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> old_fit <p>Deprecated training method from before Sentence Transformers v3.0, it is recommended to use :class:<code>sentence_transformers.trainer.SentenceTransformerTrainer</code> instead. This method should only be used if you encounter issues with your existing training scripts after upgrading to v3.0+.</p> <p>This training approach uses a list of DataLoaders and Loss functions to train the model. Each DataLoader is sampled in turn for one batch. We sample only as many batches from each DataLoader as there are in the smallest one to make sure of equal training with each dataset, i.e. round robin sampling.  Args:     train_objectives: Tuples of (DataLoader, LossFunction). Pass         more than one for multi-task learning     evaluator: An evaluator (sentence_transformers.evaluation)         evaluates the model performance during training on held-         out dev data. It is used to determine the best model         that is saved to disc.     epochs: Number of epochs for training     steps_per_epoch: Number of training steps per epoch. If set         to None (default), one epoch is equal the DataLoader         size from train_objectives.     scheduler: Learning rate scheduler. Available schedulers:         constantlr, warmupconstant, warmuplinear, warmupcosine,         warmupcosinewithhardrestarts     warmup_steps: Behavior depends on the scheduler. For         WarmupLinear (default), the learning rate is increased         from o up to the maximal learning rate. After these many         training steps, the learning rate is decreased linearly         back to zero.     optimizer_class: Optimizer     optimizer_params: Optimizer parameters     weight_decay: Weight decay for model parameters     evaluation_steps: If &gt; 0, evaluate the model using evaluator         after each number of training steps     output_path: Storage path for the model and evaluation files     save_best_model: If true, the best model (according to         evaluator) is stored at output_path     max_grad_norm: Used for gradient normalization.     use_amp: Use Automatic Mixed Precision (AMP). Only for         Pytorch &gt;= 1.6.0     callback: Callback function that is invoked after each         evaluation. It must accept the following three         parameters in this order: <code>score</code>, <code>epoch</code>, <code>steps</code>     show_progress_bar: If True, output a tqdm progress bar     checkpoint_path: Folder to save checkpoints during training     checkpoint_save_steps: Will save a checkpoint after so many         steps     checkpoint_save_total_limit: Total number of checkpoints to         store</p> <p>Parameters</p> <ul> <li>train_objectives     ('Iterable[tuple[DataLoader, nn.Module]]')    </li> <li>evaluator     ('SentenceEvaluator')     \u2013 defaults to <code>None</code> </li> <li>epochs     ('int')     \u2013 defaults to <code>1</code> </li> <li>steps_per_epoch     \u2013 defaults to <code>None</code> </li> <li>scheduler     ('str')     \u2013 defaults to <code>WarmupLinear</code> </li> <li>warmup_steps     ('int')     \u2013 defaults to <code>10000</code> </li> <li>optimizer_class     ('type[Optimizer]')     \u2013 defaults to <code>&lt;class 'torch.optim.adamw.AdamW'&gt;</code> </li> <li>optimizer_params     ('dict[str, object]')     \u2013 defaults to <code>{'lr': 2e-05}</code> </li> <li>weight_decay     ('float')     \u2013 defaults to <code>0.01</code> </li> <li>evaluation_steps     ('int')     \u2013 defaults to <code>0</code> </li> <li>output_path     ('str')     \u2013 defaults to <code>None</code> </li> <li>save_best_model     ('bool')     \u2013 defaults to <code>True</code> </li> <li>max_grad_norm     ('float')     \u2013 defaults to <code>1</code> </li> <li>use_amp     ('bool')     \u2013 defaults to <code>False</code> </li> <li>callback     ('Callable[[float, int, int], None]')     \u2013 defaults to <code>None</code> </li> <li>show_progress_bar     ('bool')     \u2013 defaults to <code>True</code> </li> <li>checkpoint_path     ('str')     \u2013 defaults to <code>None</code> </li> <li>checkpoint_save_steps     ('int')     \u2013 defaults to <code>500</code> </li> <li>checkpoint_save_total_limit     ('int')     \u2013 defaults to <code>0</code> </li> </ul> parameters <p>Return an iterator over module parameters.</p> <p>This is typically passed to an optimizer.  Args:     recurse (bool): if True, then yields parameters of this module         and all submodules. Otherwise, yields only parameters that         are direct members of this module.  Yields:     Parameter: module parameter  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for param in model.parameters():     &gt;&gt;&gt;     print(type(param), param.size())      (20L,)      (20L, 1L, 5L, 5L) <p>Parameters</p> <ul> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> </ul> pool_embeddings_hierarchical <p>Pools the embeddings hierarchically by clustering and averaging them.</p> <p>Parameters</p> <ul> <li>documents_embeddings     ('list[torch.Tensor]')    </li> <li>pool_factor     ('int')     \u2013 defaults to <code>1</code> </li> <li>protected_tokens     ('int')     \u2013 defaults to <code>1</code> </li> </ul> <p>Returns</p> <p>list[torch.Tensor]:     A list of pooled embeddings for each document.</p> pop push_to_hub <p>Uploads all elements of this Sentence Transformer to a new HuggingFace Hub repository.</p> <p>Args:     repo_id (str): Repository name for your model in the Hub, including the user or organization.     token (str, optional): An authentication token (See https://huggingface.co/settings/token)     private (bool, optional): Set to true, for hosting a private model     safe_serialization (bool, optional): If true, save the model using safetensors. If false, save the model the traditional PyTorch way     commit_message (str, optional): Message to commit while pushing.     local_model_path (str, optional): Path of the model locally. If set, this file path will be uploaded. Otherwise, the current model will be uploaded     exist_ok (bool, optional): If true, saving to an existing repository is OK. If false, saving only to a new repository is possible     replace_model_card (bool, optional): If true, replace an existing model card in the hub with the automatically created model card     train_datasets (List[str], optional): Datasets used to train the model. If set, the datasets will be added to the model card in the Hub.     revision (str, optional): Branch to push the uploaded files to     create_pr (bool, optional): If True, create a pull request instead of pushing directly to the main branch  Returns:     str: The url of the commit of your model in the repository on the Hugging Face Hub.</p> <p>Parameters</p> <ul> <li>repo_id     ('str')    </li> <li>token     ('str | None')     \u2013 defaults to <code>None</code>      Hugging Face authentication token to download private models.</li> <li>private     ('bool | None')     \u2013 defaults to <code>None</code> </li> <li>safe_serialization     ('bool')     \u2013 defaults to <code>True</code> </li> <li>commit_message     ('str | None')     \u2013 defaults to <code>None</code> </li> <li>local_model_path     ('str | None')     \u2013 defaults to <code>None</code> </li> <li>exist_ok     ('bool')     \u2013 defaults to <code>False</code> </li> <li>replace_model_card     ('bool')     \u2013 defaults to <code>False</code> </li> <li>train_datasets     ('list[str] | None')     \u2013 defaults to <code>None</code> </li> <li>revision     ('str | None')     \u2013 defaults to <code>None</code>      The specific model version to use. It can be a branch name, a tag name, or a commit id, for a stored model on Hugging Face.</li> <li>create_pr     ('bool')     \u2013 defaults to <code>False</code> </li> </ul> register_backward_hook <p>Register a backward hook on the module.</p> <p>This function is deprecated in favor of :meth:<code>~torch.nn.Module.register_full_backward_hook</code> and the behavior of this function will change in future versions.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]])    </li> </ul> register_buffer <p>Add a buffer to the module.</p> <p>This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's <code>running_mean</code> is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:<code>persistent</code> to <code>False</code>. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:<code>state_dict</code>.  Buffers can be accessed as attributes using given names.  Args:     name (str): name of the buffer. The buffer can be accessed         from this module using the given name     tensor (Tensor or None): buffer to be registered. If <code>None</code>, then operations         that run on buffers, such as :attr:<code>cuda</code>, are ignored. If <code>None</code>,         the buffer is not included in the module's :attr:<code>state_dict</code>.     persistent (bool): whether the buffer is part of this module's         :attr:<code>state_dict</code>.  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; self.register_buffer('running_mean', torch.zeros(num_features))</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>tensor     (Optional[torch.Tensor])    </li> <li>persistent     (bool)     \u2013 defaults to <code>True</code> </li> </ul> register_forward_hook <p>Register a forward hook on the module.</p> <p>The hook will be called every time after :func:<code>forward</code> has computed an output.  If <code>with_kwargs</code> is <code>False</code> or not specified, the input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the <code>forward</code>. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func:<code>forward</code> is called. The hook should have the following signature::      hook(module, args, output) -&gt; None or modified output  If <code>with_kwargs</code> is <code>True</code>, the forward hook will be passed the <code>kwargs</code> given to the forward function and be expected to return the output possibly modified. The hook should have the following signature::      hook(module, args, kwargs, output) -&gt; None or modified output  Args:     hook (Callable): The user defined hook to be registered.     prepend (bool): If <code>True</code>, the provided <code>hook</code> will be fired         before all existing <code>forward</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>forward</code> hooks on         this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>forward</code> hooks registered with         :func:<code>register_module_forward_hook</code> will fire before all hooks         registered by this method.         Default: <code>False</code>     with_kwargs (bool): If <code>True</code>, the <code>hook</code> will be passed the         kwargs given to the forward function.         Default: <code>False</code>     always_call (bool): If <code>True</code> the <code>hook</code> will be run regardless of         whether an exception is raised while calling the Module.         Default: <code>False</code>  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> <li>with_kwargs     (bool)     \u2013 defaults to <code>False</code> </li> <li>always_call     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_forward_pre_hook <p>Register a forward pre-hook on the module.</p> <p>The hook will be called every time before :func:<code>forward</code> is invoked.  If <code>with_kwargs</code> is false or not specified, the input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the <code>forward</code>. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned (unless that value is already a tuple). The hook should have the following signature::      hook(module, args) -&gt; None or modified input  If <code>with_kwargs</code> is true, the forward pre-hook will be passed the kwargs given to the forward function. And if the hook modifies the input, both the args and kwargs should be returned. The hook should have the following signature::      hook(module, args, kwargs) -&gt; None or a tuple of modified input and kwargs  Args:     hook (Callable): The user defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>forward_pre</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>forward_pre</code> hooks         on this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>forward_pre</code> hooks registered with         :func:<code>register_module_forward_pre_hook</code> will fire before all         hooks registered by this method.         Default: <code>False</code>     with_kwargs (bool): If true, the <code>hook</code> will be passed the kwargs         given to the forward function.         Default: <code>False</code>  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> <li>with_kwargs     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_full_backward_hook <p>Register a backward hook on the module.</p> <p>The hook will be called every time the gradients with respect to a module are computed, i.e. the hook will execute if and only if the gradients with respect to module outputs are computed. The hook should have the following signature::      hook(module, grad_input, grad_output) -&gt; tuple(Tensor) or None  The :attr:<code>grad_input</code> and :attr:<code>grad_output</code> are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr:<code>grad_input</code> in subsequent computations. :attr:<code>grad_input</code> will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr:<code>grad_input</code> and :attr:<code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.  For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function.  .. warning ::     Modifying inputs or outputs inplace is not allowed when using backward hooks and     will raise an error.  Args:     hook (Callable): The user-defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>backward</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>backward</code> hooks on         this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>backward</code> hooks registered with         :func:<code>register_module_full_backward_hook</code> will fire before         all hooks registered by this method.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_full_backward_pre_hook <p>Register a backward pre-hook on the module.</p> <p>The hook will be called every time the gradients for the module are computed. The hook should have the following signature::      hook(module, grad_output) -&gt; tuple[Tensor] or None  The :attr:<code>grad_output</code> is a tuple. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the output that will be used in place of :attr:<code>grad_output</code> in subsequent computations. Entries in :attr:<code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.  For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function.  .. warning ::     Modifying inputs inplace is not allowed when using backward hooks and     will raise an error.  Args:     hook (Callable): The user-defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>backward_pre</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>backward_pre</code> hooks         on this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>backward_pre</code> hooks registered with         :func:<code>register_module_full_backward_pre_hook</code> will fire before         all hooks registered by this method.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_load_state_dict_post_hook <p>Register a post-hook to be run after module's :meth:<code>~nn.Module.load_state_dict</code> is called.</p> <p>It should have the following signature::     hook(module, incompatible_keys) -&gt; None  The <code>module</code> argument is the current module that this hook is registered on, and the <code>incompatible_keys</code> argument is a <code>NamedTuple</code> consisting of attributes <code>missing_keys</code> and <code>unexpected_keys</code>. <code>missing_keys</code> is a <code>list</code> of <code>str</code> containing the missing keys and <code>unexpected_keys</code> is a <code>list</code> of <code>str</code> containing the unexpected keys.  The given incompatible_keys can be modified inplace if needed.  Note that the checks performed when calling :func:<code>load_state_dict</code> with <code>strict=True</code> are affected by modifications the hook makes to <code>missing_keys</code> or <code>unexpected_keys</code>, as expected. Additions to either set of keys will result in an error being thrown when <code>strict=True</code>, and clearing out both missing and unexpected keys will avoid an error.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook </li> </ul> register_load_state_dict_pre_hook <p>Register a pre-hook to be run before module's :meth:<code>~nn.Module.load_state_dict</code> is called.</p> <p>It should have the following signature::     hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -&gt; None  # noqa: B950  Arguments:     hook (Callable): Callable hook that will be invoked before         loading the state dict.</p> <p>Parameters</p> <ul> <li>hook </li> </ul> register_module <p>Alias for :func:<code>add_module</code>.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>module     (Optional[ForwardRef('Module')])    </li> </ul> register_parameter <p>Add a parameter to the module.</p> <p>The parameter can be accessed as an attribute using given name.  Args:     name (str): name of the parameter. The parameter can be accessed         from this module using the given name     param (Parameter or None): parameter to be added to the module. If         <code>None</code>, then operations that run on parameters, such as :attr:<code>cuda</code>,         are ignored. If <code>None</code>, the parameter is not included in the         module's :attr:<code>state_dict</code>.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>param     (Optional[torch.nn.parameter.Parameter])    </li> </ul> register_state_dict_post_hook <p>Register a post-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method.</p> <p>It should have the following signature::     hook(module, state_dict, prefix, local_metadata) -&gt; None  The registered hooks can modify the <code>state_dict</code> inplace.</p> <p>Parameters</p> <ul> <li>hook </li> </ul> register_state_dict_pre_hook <p>Register a pre-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method.</p> <p>It should have the following signature::     hook(module, prefix, keep_vars) -&gt; None  The registered hooks can be used to perform pre-processing before the <code>state_dict</code> call is made.</p> <p>Parameters</p> <ul> <li>hook </li> </ul> requires_grad_ <p>Change if autograd should record operations on parameters in this module.</p> <p>This method sets the parameters' :attr:<code>requires_grad</code> attributes in-place.  This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).  See :ref:<code>locally-disable-grad-doc</code> for a comparison between <code>.requires_grad_()</code> and several similar mechanisms that may be confused with it.  Args:     requires_grad (bool): whether autograd should record operations on                           parameters in this module. Default: <code>True</code>.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>requires_grad     (bool)     \u2013 defaults to <code>True</code> </li> </ul> save <p>Saves a model and its configuration files to a directory, so that it can be loaded with <code>SentenceTransformer(path)</code> again.</p> <p>Args:     path (str): Path on disc where the model will be saved.     model_name (str, optional): Optional model name.     create_model_card (bool, optional): If True, create a README.md with basic information about this model.     train_datasets (list[str], optional): Optional list with the names of the datasets used to train the model.     safe_serialization (bool, optional): If True, save the model using safetensors. If False, save the model         the traditional (but unsafe) PyTorch way.</p> <p>Parameters</p> <ul> <li>path     ('str')    </li> <li>model_name     ('str | None')     \u2013 defaults to <code>None</code> </li> <li>create_model_card     ('bool')     \u2013 defaults to <code>True</code> </li> <li>train_datasets     ('list[str] | None')     \u2013 defaults to <code>None</code> </li> <li>safe_serialization     ('bool')     \u2013 defaults to <code>True</code> </li> </ul> save_pretrained <p>Saves a model and its configuration files to a directory, so that it can be loaded with <code>SentenceTransformer(path)</code> again.</p> <p>Args:     path (str): Path on disc where the model will be saved.     model_name (str, optional): Optional model name.     create_model_card (bool, optional): If True, create a README.md with basic information about this model.     train_datasets (List[str], optional): Optional list with the names of the datasets used to train the model.     safe_serialization (bool, optional): If True, save the model using safetensors. If False, save the model         the traditional (but unsafe) PyTorch way.</p> <p>Parameters</p> <ul> <li>path     ('str')    </li> <li>model_name     ('str | None')     \u2013 defaults to <code>None</code> </li> <li>create_model_card     ('bool')     \u2013 defaults to <code>True</code> </li> <li>train_datasets     ('list[str] | None')     \u2013 defaults to <code>None</code> </li> <li>safe_serialization     ('bool')     \u2013 defaults to <code>True</code> </li> </ul> save_to_hub <p>DEPRECATED, use <code>push_to_hub</code> instead.</p> <p>Uploads all elements of this Sentence Transformer to a new HuggingFace Hub repository.  Args:     repo_id (str): Repository name for your model in the Hub, including the user or organization.     token (str, optional): An authentication token (See https://huggingface.co/settings/token)     private (bool, optional): Set to true, for hosting a private model     safe_serialization (bool, optional): If true, save the model using safetensors. If false, save the model the traditional PyTorch way     commit_message (str, optional): Message to commit while pushing.     local_model_path (str, optional): Path of the model locally. If set, this file path will be uploaded. Otherwise, the current model will be uploaded     exist_ok (bool, optional): If true, saving to an existing repository is OK. If false, saving only to a new repository is possible     replace_model_card (bool, optional): If true, replace an existing model card in the hub with the automatically created model card     train_datasets (List[str], optional): Datasets used to train the model. If set, the datasets will be added to the model card in the Hub.  Returns:     str: The url of the commit of your model in the repository on the Hugging Face Hub.</p> <p>Parameters</p> <ul> <li>repo_id     ('str')    </li> <li>organization     ('str | None')     \u2013 defaults to <code>None</code> </li> <li>token     ('str | None')     \u2013 defaults to <code>None</code>      Hugging Face authentication token to download private models.</li> <li>private     ('bool | None')     \u2013 defaults to <code>None</code> </li> <li>safe_serialization     ('bool')     \u2013 defaults to <code>True</code> </li> <li>commit_message     ('str')     \u2013 defaults to <code>Add new SentenceTransformer model.</code> </li> <li>local_model_path     ('str | None')     \u2013 defaults to <code>None</code> </li> <li>exist_ok     ('bool')     \u2013 defaults to <code>False</code> </li> <li>replace_model_card     ('bool')     \u2013 defaults to <code>False</code> </li> <li>train_datasets     ('list[str] | None')     \u2013 defaults to <code>None</code> </li> </ul> set_adapter <p>Sets a specific adapter by forcing the model to use a that adapter and disable the other adapters.</p> <p>Args:     args:         Positional arguments to pass to the underlying AutoModel <code>set_adapter</code> function. More information can be found in the transformers documentation         https://huggingface.co/docs/transformers/main/en/peft#transformers.integrations.PeftAdapterMixin.set_adapter     *kwargs:         Keyword arguments to pass to the underlying AutoModel <code>set_adapter</code> function. More information can be found in the transformers documentation         https://huggingface.co/docs/transformers/main/en/peft#transformers.integrations.PeftAdapterMixin.set_adapter</p> <p>Parameters</p> <ul> <li>args </li> <li>kwargs </li> </ul> set_extra_state <p>Set extra state contained in the loaded <code>state_dict</code>.</p> <p>This function is called from :func:<code>load_state_dict</code> to handle any extra state found within the <code>state_dict</code>. Implement this function and a corresponding :func:<code>get_extra_state</code> for your module if you need to store extra state within its <code>state_dict</code>.  Args:     state (dict): Extra state from the <code>state_dict</code></p> <p>Parameters</p> <ul> <li>state     (Any)    </li> </ul> set_pooling_include_prompt <p>Sets the <code>include_prompt</code> attribute in the pooling layer in the model, if there is one.</p> <p>This is useful for INSTRUCTOR models, as the prompt should be excluded from the pooling strategy for these models.  Args:     include_prompt (bool): Whether to include the prompt in the pooling layer.  Returns:     None</p> <p>Parameters</p> <ul> <li>include_prompt     ('bool')    </li> </ul> set_submodule <p>Set the submodule given by <code>target</code> if it exists, otherwise throw an error.</p> <p>For example, let's say you have an <code>nn.Module</code> <code>A</code> that looks like this:  .. code-block:: text      A(         (net_b): Module(             (net_c): Module(                 (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))             )             (linear): Linear(in_features=100, out_features=200, bias=True)         )     )  (The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)  To overide the <code>Conv2d</code> with a new submodule <code>Linear</code>, you would call <code>set_submodule(\"net_b.net_c.conv\", nn.Linear(33, 16))</code>.  Args:     target: The fully-qualified string name of the submodule         to look for. (See above example for how to specify a         fully-qualified string.)     module: The module to set the submodule to.  Raises:     ValueError: If the target string is empty     AttributeError: If the target string references an invalid         path or resolves to something that is not an         <code>nn.Module</code></p> <p>Parameters</p> <ul> <li>target     (str)    </li> <li>module     ('Module')    </li> </ul> share_memory <p>See :meth:<code>torch.Tensor.share_memory_</code>.</p> skiplist_mask <p>Create a mask for the set of input_ids that are in the skiplist.</p> <p>Parameters</p> <ul> <li>input_ids     ('torch.Tensor')    </li> <li>skiplist     ('list[int]')    </li> </ul> smart_batching_collate <p>Transforms a batch from a SmartBatchingDataset to a batch of tensors for the model Here, batch is a list of InputExample instances: [InputExample(...), ...]</p> <p>Args:     batch: a batch from a SmartBatchingDataset  Returns:     a batch of tensors for the model</p> <p>Parameters</p> <ul> <li>batch     ('list[InputExample]')    </li> </ul> start_multi_process_pool <p>Starts a multi-process pool to process the encoding with several independent processes. This method is recommended if you want to encode on multiple GPUs or CPUs. It is advised to start only one process per GPU. This method works together with encode_multi_process and stop_multi_process_pool.</p> <p>Parameters</p> <ul> <li>target_devices     ('list[str]')     \u2013 defaults to <code>None</code> </li> </ul> <p>Returns</p> <p>dict:     A dictionary with the target processes, an input queue, and an output queue.</p> state_dict <p>Return a dictionary containing references to the whole state of the module.</p> <p>Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to <code>None</code> are not included.  .. note::     The returned object is a shallow copy. It contains references     to the module's parameters and buffers.  .. warning::     Currently <code>state_dict()</code> also accepts positional arguments for     <code>destination</code>, <code>prefix</code> and <code>keep_vars</code> in order. However,     this is being deprecated and keyword arguments will be enforced in     future releases.  .. warning::     Please avoid the use of argument <code>destination</code> as it is not     designed for end-users.  Args:     destination (dict, optional): If provided, the state of module will         be updated into the dict and the same object is returned.         Otherwise, an <code>OrderedDict</code> will be created and returned.         Default: <code>None</code>.     prefix (str, optional): a prefix added to parameter and buffer         names to compose the keys in state_dict. Default: <code>''</code>.     keep_vars (bool, optional): by default the :class:<code>~torch.Tensor</code> s         returned in the state dict are detached from autograd. If it's         set to <code>True</code>, detaching will not be performed.         Default: <code>False</code>.  Returns:     dict:         a dictionary containing a whole state of the module  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; module.state_dict().keys()     ['bias', 'weight']</p> <p>Parameters</p> <ul> <li>args </li> <li>destination     \u2013 defaults to <code>None</code> </li> <li>prefix     \u2013 defaults to ``    </li> <li>keep_vars     \u2013 defaults to <code>False</code> </li> </ul> stop_multi_process_pool <p>Stops all processes started with start_multi_process_pool.</p> <p>Args:     pool (Dict[str, object]): A dictionary containing the input queue, output queue, and process list.  Returns:     None</p> <ul> <li>pool     (\"dict[Literal['input', 'output', 'processes'], Any]\")    </li> </ul> to <p>Move and/or cast the parameters and buffers.</p> <p>This can be called as  .. function:: to(device=None, dtype=None, non_blocking=False)    :noindex:  .. function:: to(dtype, non_blocking=False)    :noindex:  .. function:: to(tensor, non_blocking=False)    :noindex:  .. function:: to(memory_format=torch.channels_last)    :noindex:  Its signature is similar to :meth:<code>torch.Tensor.to</code>, but only accepts floating point or complex :attr:<code>dtype</code>\\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr:<code>dtype</code> (if given). The integral parameters and buffers will be moved :attr:<code>device</code>, if that is given, but with dtypes unchanged. When :attr:<code>non_blocking</code> is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices.  See below for examples.  .. note::     This method modifies the module in-place.  Args:     device (:class:<code>torch.device</code>): the desired device of the parameters         and buffers in this module     dtype (:class:<code>torch.dtype</code>): the desired floating point or complex dtype of         the parameters and buffers in this module     tensor (torch.Tensor): Tensor whose dtype and device are the desired         dtype and device for all parameters and buffers in this module     memory_format (:class:<code>torch.memory_format</code>): the desired memory         format for 4D parameters and buffers in this module (keyword         only argument)  Returns:     Module: self  Examples::      &gt;&gt;&gt; # xdoctest: +IGNORE_WANT(\"non-deterministic\")     &gt;&gt;&gt; linear = nn.Linear(2, 2)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1913, -0.3420],             [-0.5113, -0.2325]])     &gt;&gt;&gt; linear.to(torch.double)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1913, -0.3420],             [-0.5113, -0.2325]], dtype=torch.float64)     &gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)     &gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")     &gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1914, -0.3420],             [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')     &gt;&gt;&gt; cpu = torch.device(\"cpu\")     &gt;&gt;&gt; linear.to(cpu)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1914, -0.3420],             [-0.5112, -0.2324]], dtype=torch.float16)      &gt;&gt;&gt; linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.3741+0.j,  0.2382+0.j],             [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)     &gt;&gt;&gt; linear(torch.ones(3, 2, dtype=torch.cdouble))     tensor([[0.6122+0.j, 0.1150+0.j],             [0.6122+0.j, 0.1150+0.j],             [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)</p> <p>Parameters</p> <ul> <li>args </li> <li>kwargs </li> </ul> to_empty <p>Move the parameters and buffers to the specified device without copying storage.</p> <p>Args:     device (:class:<code>torch.device</code>): The desired device of the parameters         and buffers in this module.     recurse (bool): Whether parameters and buffers of submodules should         be recursively moved to the specified device.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, str, torch.device, NoneType])       Device (like \"cuda\", \"cpu\", \"mps\", \"npu\") that should be used for computation. If None, checks if a GPU can be used.</li> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> </ul> tokenize <p>Tokenizes the input texts.</p> <p>Args:     texts (Union[list[str], list[dict], list[tuple[str, str]]]): A list of texts to be tokenized.     is_query (bool): Flag to indicate if the texts are queries. Defaults to True.     pad_document (bool): Flag to indicate if documents should be padded to max length. Defaults to False.  Returns:     dict[str, torch.Tensor]: A dictionary of tensors with the tokenized texts, including \"input_ids\",         \"attention_mask\", and optionally \"token_type_ids\".</p> <p>Parameters</p> <ul> <li>texts     ('list[str] | list[dict] | list[tuple[str, str]]')    </li> <li>is_query     ('bool')     \u2013 defaults to <code>True</code> </li> <li>pad_document     ('bool')     \u2013 defaults to <code>False</code> </li> </ul> train <p>Set the module in training mode.</p> <p>This has an effect only on certain modules. See the documentation of particular modules for details of their behaviors in training/evaluation mode, i.e., whether they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>, etc.  Args:     mode (bool): whether to set training mode (<code>True</code>) or evaluation                  mode (<code>False</code>). Default: <code>True</code>.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>mode     (bool)     \u2013 defaults to <code>True</code> </li> </ul> truncate_sentence_embeddings <p>In this context, :meth:<code>SentenceTransformer.encode &lt;sentence_transformers.SentenceTransformer.encode&gt;</code> outputs sentence embeddings truncated at dimension <code>truncate_dim</code>.</p> <p>This may be useful when you are using the same model for different applications where different dimensions are needed.  Args:     truncate_dim (int, optional): The dimension to truncate sentence embeddings to. <code>None</code> does no truncation.  Example:     ::          from sentence_transformers import SentenceTransformer          model = SentenceTransformer(\"all-mpnet-base-v2\")          with model.truncate_sentence_embeddings(truncate_dim=16):             embeddings_truncated = model.encode([\"hello there\", \"hiya\"])         assert embeddings_truncated.shape[-1] == 16</p> <p>Parameters</p> <ul> <li>truncate_dim     ('int | None')       The dimension to truncate sentence embeddings to. <code>None</code> does no truncation. Truncation is only applicable during inference when :meth:<code>SentenceTransformer.encode</code> is called.</li> </ul> type <p>Casts all parameters and buffers to :attr:<code>dst_type</code>.</p> <p>.. note::     This method modifies the module in-place.  Args:     dst_type (type or string): the desired type  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>dst_type     (Union[torch.dtype, str])    </li> </ul> xpu <p>Move all model parameters and buffers to the XPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized.  .. note::     This method modifies the module in-place.  Arguments:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, torch.device, NoneType])     \u2013 defaults to <code>None</code>      Device (like \"cuda\", \"cpu\", \"mps\", \"npu\") that should be used for computation. If None, checks if a GPU can be used.</li> </ul> zero_grad <p>Reset gradients of all model parameters.</p> <p>See similar function under :class:<code>torch.optim.Optimizer</code> for more context.  Args:     set_to_none (bool): instead of setting to zero, set the grads to None.         See :meth:<code>torch.optim.Optimizer.zero_grad</code> for details.</p> <p>Parameters</p> <ul> <li>set_to_none     (bool)     \u2013 defaults to <code>True</code> </li> </ul>"},{"location":"api/models/Dense/","title":"Dense","text":"<p>Performs linear projection on the token embeddings to a lower dimension.</p>"},{"location":"api/models/Dense/#parameters","title":"Parameters","text":"<ul> <li> <p>in_features ('int')</p> <p>Size of the embeddings in output of the tansformer.</p> </li> <li> <p>out_features ('int')</p> <p>Size of the output embeddings after linear projection</p> </li> <li> <p>bias ('bool') \u2013 defaults to <code>True</code></p> <p>Add a bias vector</p> </li> <li> <p>activation_function \u2013 defaults to <code>Identity()</code></p> </li> <li> <p>init_weight ('torch.Tensor') \u2013 defaults to <code>None</code></p> <p>Initial value for the matrix of the linear layer</p> </li> <li> <p>init_bias ('torch.Tensor') \u2013 defaults to <code>None</code></p> <p>Initial value for the bias of the linear layer.</p> </li> </ul>"},{"location":"api/models/Dense/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from pylate import models\n\n&gt;&gt;&gt; model = models.Dense(\n...     in_features=768,\n...     out_features=128,\n... )\n\n&gt;&gt;&gt; features = {\n...     \"token_embeddings\": torch.randn(2, 768),\n... }\n\n&gt;&gt;&gt; projected_features = model(features)\n\n&gt;&gt;&gt; assert projected_features[\"token_embeddings\"].shape == (2, 128)\n&gt;&gt;&gt; assert isinstance(model, DenseSentenceTransformer)\n</code></pre>"},{"location":"api/models/Dense/#methods","title":"Methods","text":"call <p>Call self as a function.</p> <p>Parameters</p> <ul> <li>args </li> <li>kwargs </li> </ul> add_module <p>Add a child module to the current module.</p> <p>The module can be accessed as an attribute using the given name.  Args:     name (str): name of the child module. The child module can be         accessed from this module using the given name     module (Module): child module to be added to the module.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>module     (Optional[ForwardRef('Module')])    </li> </ul> apply <p>Apply <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self.</p> <p>Typical use includes initializing the parameters of a model (see also :ref:<code>nn-init-doc</code>).  Args:     fn (:class:<code>Module</code> -&gt; None): function to be applied to each submodule  Returns:     Module: self  Example::      &gt;&gt;&gt; @torch.no_grad()     &gt;&gt;&gt; def init_weights(m):     &gt;&gt;&gt;     print(m)     &gt;&gt;&gt;     if type(m) == nn.Linear:     &gt;&gt;&gt;         m.weight.fill_(1.0)     &gt;&gt;&gt;         print(m.weight)     &gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))     &gt;&gt;&gt; net.apply(init_weights)     Linear(in_features=2, out_features=2, bias=True)     Parameter containing:     tensor([[1., 1.],             [1., 1.]], requires_grad=True)     Linear(in_features=2, out_features=2, bias=True)     Parameter containing:     tensor([[1., 1.],             [1., 1.]], requires_grad=True)     Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     )</p> <p>Parameters</p> <ul> <li>fn     (Callable[[ForwardRef('Module')], NoneType])    </li> </ul> bfloat16 <p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> buffers <p>Return an iterator over module buffers.</p> <p>Args:     recurse (bool): if True, then yields buffers of this module         and all submodules. Otherwise, yields only buffers that         are direct members of this module.  Yields:     torch.Tensor: module buffer  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for buf in model.buffers():     &gt;&gt;&gt;     print(type(buf), buf.size())      (20L,)      (20L, 1L, 5L, 5L) <p>Parameters</p> <ul> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> </ul> children <p>Return an iterator over immediate children modules.</p> <p>Yields:     Module: a child module</p> compile <p>Compile this Module's forward using :func:<code>torch.compile</code>.</p> <p>This Module's <code>__call__</code> method is compiled and all arguments are passed as-is to :func:<code>torch.compile</code>.  See :func:<code>torch.compile</code> for details on the arguments for this function.</p> <p>Parameters</p> <ul> <li>args </li> <li>kwargs </li> </ul> cpu <p>Move all model parameters and buffers to the CPU.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> cuda <p>Move all model parameters and buffers to the GPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing the optimizer if the module will live on GPU while being optimized.  .. note::     This method modifies the module in-place.  Args:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, torch.device, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> double <p>Casts all floating point parameters and buffers to <code>double</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> eval <p>Set the module in evaluation mode.</p> <p>This has an effect only on certain modules. See the documentation of particular modules for details of their behaviors in training/evaluation mode, i.e. whether they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>, etc.  This is equivalent with :meth:<code>self.train(False) &lt;torch.nn.Module.train&gt;</code>.  See :ref:<code>locally-disable-grad-doc</code> for a comparison between <code>.eval()</code> and several similar mechanisms that may be confused with it.  Returns:     Module: self</p> extra_repr <p>Return the extra representation of the module.</p> <p>To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.</p> float <p>Casts all floating point parameters and buffers to <code>float</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> forward <p>Performs linear projection on the token embeddings.</p> <p>Parameters</p> <ul> <li>features     ('dict[str, torch.Tensor]')    </li> </ul> from_sentence_transformers <p>Converts a SentenceTransformer Dense model to a Dense model. Our Dense model does not have the activation function.</p> <ul> <li>dense     ('DenseSentenceTransformer')    </li> </ul> from_stanford_weights <p>Load the weight of the Dense layer using weights from a stanford-nlp checkpoint.</p> <p>Parameters</p> <ul> <li>model_name_or_path     ('str | os.PathLike')    </li> <li>cache_folder     ('str | os.PathLike | None')     \u2013 defaults to <code>None</code> </li> <li>revision     ('str | None')     \u2013 defaults to <code>None</code> </li> <li>local_files_only     ('bool | None')     \u2013 defaults to <code>None</code> </li> <li>token     ('str | bool | None')     \u2013 defaults to <code>None</code> </li> <li>use_auth_token     ('str | bool | None')     \u2013 defaults to <code>None</code> </li> </ul> get_buffer <p>Return the buffer given by <code>target</code> if it exists, otherwise throw an error.</p> <p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method's functionality as well as how to correctly specify <code>target</code>.  Args:     target: The fully-qualified string name of the buffer         to look for. (See <code>get_submodule</code> for how to specify a         fully-qualified string.)  Returns:     torch.Tensor: The buffer referenced by <code>target</code>  Raises:     AttributeError: If the target string references an invalid         path or resolves to something that is not a         buffer</p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> get_config_dict get_extra_state <p>Return any extra state to include in the module's state_dict.</p> <p>Implement this and a corresponding :func:<code>set_extra_state</code> for your module if you need to store extra state. This function is called when building the module's <code>state_dict()</code>.  Note that extra state should be picklable to ensure working serialization of the state_dict. We only provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes.  Returns:     object: Any extra state to store in the module's state_dict</p> get_parameter <p>Return the parameter given by <code>target</code> if it exists, otherwise throw an error.</p> <p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method's functionality as well as how to correctly specify <code>target</code>.  Args:     target: The fully-qualified string name of the Parameter         to look for. (See <code>get_submodule</code> for how to specify a         fully-qualified string.)  Returns:     torch.nn.Parameter: The Parameter referenced by <code>target</code>  Raises:     AttributeError: If the target string references an invalid         path or resolves to something that is not an         <code>nn.Parameter</code></p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> get_sentence_embedding_dimension get_submodule <p>Return the submodule given by <code>target</code> if it exists, otherwise throw an error.</p> <p>For example, let's say you have an <code>nn.Module</code> <code>A</code> that looks like this:  .. code-block:: text      A(         (net_b): Module(             (net_c): Module(                 (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))             )             (linear): Linear(in_features=100, out_features=200, bias=True)         )     )  (The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> which has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)  To check whether or not we have the <code>linear</code> submodule, we would call <code>get_submodule(\"net_b.linear\")</code>. To check whether we have the <code>conv</code> submodule, we would call <code>get_submodule(\"net_b.net_c.conv\")</code>.  The runtime of <code>get_submodule</code> is bounded by the degree of module nesting in <code>target</code>. A query against <code>named_modules</code> achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, <code>get_submodule</code> should always be used.  Args:     target: The fully-qualified string name of the submodule         to look for. (See above example for how to specify a         fully-qualified string.)  Returns:     torch.nn.Module: The submodule referenced by <code>target</code>  Raises:     AttributeError: If the target string references an invalid         path or resolves to something that is not an         <code>nn.Module</code></p> <p>Parameters</p> <ul> <li>target     (str)    </li> </ul> half <p>Casts all floating point parameters and buffers to <code>half</code> datatype.</p> <p>.. note::     This method modifies the module in-place.  Returns:     Module: self</p> ipu <p>Move all model parameters and buffers to the IPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing the optimizer if the module will live on IPU while being optimized.  .. note::     This method modifies the module in-place.  Arguments:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, torch.device, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> load <p>Load a Dense layer.</p> <ul> <li>input_path </li> </ul> load_state_dict <p>Copy parameters and buffers from :attr:<code>state_dict</code> into this module and its descendants.</p> <p>If :attr:<code>strict</code> is <code>True</code>, then the keys of :attr:<code>state_dict</code> must exactly match the keys returned by this module's :meth:<code>~torch.nn.Module.state_dict</code> function.  .. warning::     If :attr:<code>assign</code> is <code>True</code> the optimizer must be created after     the call to :attr:<code>load_state_dict</code> unless     :func:<code>~torch.__future__.get_swap_module_params_on_conversion</code> is <code>True</code>.  Args:     state_dict (dict): a dict containing parameters and         persistent buffers.     strict (bool, optional): whether to strictly enforce that the keys         in :attr:<code>state_dict</code> match the keys returned by this module's         :meth:<code>~torch.nn.Module.state_dict</code> function. Default: <code>True</code>     assign (bool, optional): When set to <code>False</code>, the properties of the tensors         in the current module are preserved whereas setting it to <code>True</code> preserves         properties of the Tensors in the state dict. The only         exception is the <code>requires_grad</code> field of :class:<code>~torch.nn.Parameter</code>s         for which the value from the module is preserved.         Default: <code>False</code>  Returns:     <code>NamedTuple</code> with <code>missing_keys</code> and <code>unexpected_keys</code> fields:         * missing_keys is a list of str containing any keys that are expected             by this module but missing from the provided <code>state_dict</code>.         * unexpected_keys is a list of str containing the keys that are not             expected by this module but present in the provided <code>state_dict</code>.  Note:     If a parameter or buffer is registered as <code>None</code> and its corresponding key     exists in :attr:<code>state_dict</code>, :meth:<code>load_state_dict</code> will raise a     <code>RuntimeError</code>.</p> <p>Parameters</p> <ul> <li>state_dict     (Mapping[str, Any])    </li> <li>strict     (bool)     \u2013 defaults to <code>True</code> </li> <li>assign     (bool)     \u2013 defaults to <code>False</code> </li> </ul> modules <p>Return an iterator over all modules in the network.</p> <p>Yields:     Module: a module in the network  Note:     Duplicate modules are returned only once. In the following     example, <code>l</code> will be returned only once.  Example::      &gt;&gt;&gt; l = nn.Linear(2, 2)     &gt;&gt;&gt; net = nn.Sequential(l, l)     &gt;&gt;&gt; for idx, m in enumerate(net.modules()):     ...     print(idx, '-&gt;', m)      0 -&gt; Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     )     1 -&gt; Linear(in_features=2, out_features=2, bias=True)</p> mtia <p>Move all model parameters and buffers to the MTIA.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing the optimizer if the module will live on MTIA while being optimized.  .. note::     This method modifies the module in-place.  Arguments:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, torch.device, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> named_buffers <p>Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.</p> <p>Args:     prefix (str): prefix to prepend to all buffer names.     recurse (bool, optional): if True, then yields buffers of this module         and all submodules. Otherwise, yields only buffers that         are direct members of this module. Defaults to True.     remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.  Yields:     (str, torch.Tensor): Tuple containing the name and buffer  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, buf in self.named_buffers():     &gt;&gt;&gt;     if name in ['running_var']:     &gt;&gt;&gt;         print(buf.size())</p> <p>Parameters</p> <ul> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> named_children <p>Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.</p> <p>Yields:     (str, Module): Tuple containing a name and child module  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, module in model.named_children():     &gt;&gt;&gt;     if name in ['conv4', 'conv5']:     &gt;&gt;&gt;         print(module)</p> named_modules <p>Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.</p> <p>Args:     memo: a memo to store the set of modules already added to the result     prefix: a prefix that will be added to the name of the module     remove_duplicate: whether to remove the duplicated module instances in the result         or not  Yields:     (str, Module): Tuple of name and module  Note:     Duplicate modules are returned only once. In the following     example, <code>l</code> will be returned only once.  Example::      &gt;&gt;&gt; l = nn.Linear(2, 2)     &gt;&gt;&gt; net = nn.Sequential(l, l)     &gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):     ...     print(idx, '-&gt;', m)      0 -&gt; ('', Sequential(       (0): Linear(in_features=2, out_features=2, bias=True)       (1): Linear(in_features=2, out_features=2, bias=True)     ))     1 -&gt; ('0', Linear(in_features=2, out_features=2, bias=True))</p> <p>Parameters</p> <ul> <li>memo     (Optional[Set[ForwardRef('Module')]])     \u2013 defaults to <code>None</code> </li> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> named_parameters <p>Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.</p> <p>Args:     prefix (str): prefix to prepend to all parameter names.     recurse (bool): if True, then yields parameters of this module         and all submodules. Otherwise, yields only parameters that         are direct members of this module.     remove_duplicate (bool, optional): whether to remove the duplicated         parameters in the result. Defaults to True.  Yields:     (str, Parameter): Tuple containing the name and parameter  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for name, param in self.named_parameters():     &gt;&gt;&gt;     if name in ['bias']:     &gt;&gt;&gt;         print(param.size())</p> <p>Parameters</p> <ul> <li>prefix     (str)     \u2013 defaults to ``    </li> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> <li>remove_duplicate     (bool)     \u2013 defaults to <code>True</code> </li> </ul> parameters <p>Return an iterator over module parameters.</p> <p>This is typically passed to an optimizer.  Args:     recurse (bool): if True, then yields parameters of this module         and all submodules. Otherwise, yields only parameters that         are direct members of this module.  Yields:     Parameter: module parameter  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; for param in model.parameters():     &gt;&gt;&gt;     print(type(param), param.size())      (20L,)      (20L, 1L, 5L, 5L) <p>Parameters</p> <ul> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> </ul> register_backward_hook <p>Register a backward hook on the module.</p> <p>This function is deprecated in favor of :meth:<code>~torch.nn.Module.register_full_backward_hook</code> and the behavior of this function will change in future versions.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]])    </li> </ul> register_buffer <p>Add a buffer to the module.</p> <p>This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's <code>running_mean</code> is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:<code>persistent</code> to <code>False</code>. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:<code>state_dict</code>.  Buffers can be accessed as attributes using given names.  Args:     name (str): name of the buffer. The buffer can be accessed         from this module using the given name     tensor (Tensor or None): buffer to be registered. If <code>None</code>, then operations         that run on buffers, such as :attr:<code>cuda</code>, are ignored. If <code>None</code>,         the buffer is not included in the module's :attr:<code>state_dict</code>.     persistent (bool): whether the buffer is part of this module's         :attr:<code>state_dict</code>.  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; self.register_buffer('running_mean', torch.zeros(num_features))</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>tensor     (Optional[torch.Tensor])    </li> <li>persistent     (bool)     \u2013 defaults to <code>True</code> </li> </ul> register_forward_hook <p>Register a forward hook on the module.</p> <p>The hook will be called every time after :func:<code>forward</code> has computed an output.  If <code>with_kwargs</code> is <code>False</code> or not specified, the input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the <code>forward</code>. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func:<code>forward</code> is called. The hook should have the following signature::      hook(module, args, output) -&gt; None or modified output  If <code>with_kwargs</code> is <code>True</code>, the forward hook will be passed the <code>kwargs</code> given to the forward function and be expected to return the output possibly modified. The hook should have the following signature::      hook(module, args, kwargs, output) -&gt; None or modified output  Args:     hook (Callable): The user defined hook to be registered.     prepend (bool): If <code>True</code>, the provided <code>hook</code> will be fired         before all existing <code>forward</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>forward</code> hooks on         this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>forward</code> hooks registered with         :func:<code>register_module_forward_hook</code> will fire before all hooks         registered by this method.         Default: <code>False</code>     with_kwargs (bool): If <code>True</code>, the <code>hook</code> will be passed the         kwargs given to the forward function.         Default: <code>False</code>     always_call (bool): If <code>True</code> the <code>hook</code> will be run regardless of         whether an exception is raised while calling the Module.         Default: <code>False</code>  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> <li>with_kwargs     (bool)     \u2013 defaults to <code>False</code> </li> <li>always_call     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_forward_pre_hook <p>Register a forward pre-hook on the module.</p> <p>The hook will be called every time before :func:<code>forward</code> is invoked.  If <code>with_kwargs</code> is false or not specified, the input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the <code>forward</code>. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned (unless that value is already a tuple). The hook should have the following signature::      hook(module, args) -&gt; None or modified input  If <code>with_kwargs</code> is true, the forward pre-hook will be passed the kwargs given to the forward function. And if the hook modifies the input, both the args and kwargs should be returned. The hook should have the following signature::      hook(module, args, kwargs) -&gt; None or a tuple of modified input and kwargs  Args:     hook (Callable): The user defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>forward_pre</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>forward_pre</code> hooks         on this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>forward_pre</code> hooks registered with         :func:<code>register_module_forward_pre_hook</code> will fire before all         hooks registered by this method.         Default: <code>False</code>     with_kwargs (bool): If true, the <code>hook</code> will be passed the kwargs         given to the forward function.         Default: <code>False</code>  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> <li>with_kwargs     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_full_backward_hook <p>Register a backward hook on the module.</p> <p>The hook will be called every time the gradients with respect to a module are computed, i.e. the hook will execute if and only if the gradients with respect to module outputs are computed. The hook should have the following signature::      hook(module, grad_input, grad_output) -&gt; tuple(Tensor) or None  The :attr:<code>grad_input</code> and :attr:<code>grad_output</code> are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr:<code>grad_input</code> in subsequent computations. :attr:<code>grad_input</code> will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr:<code>grad_input</code> and :attr:<code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.  For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function.  .. warning ::     Modifying inputs or outputs inplace is not allowed when using backward hooks and     will raise an error.  Args:     hook (Callable): The user-defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>backward</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>backward</code> hooks on         this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>backward</code> hooks registered with         :func:<code>register_module_full_backward_hook</code> will fire before         all hooks registered by this method.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_full_backward_pre_hook <p>Register a backward pre-hook on the module.</p> <p>The hook will be called every time the gradients for the module are computed. The hook should have the following signature::      hook(module, grad_output) -&gt; tuple[Tensor] or None  The :attr:<code>grad_output</code> is a tuple. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the output that will be used in place of :attr:<code>grad_output</code> in subsequent computations. Entries in :attr:<code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.  For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function.  .. warning ::     Modifying inputs inplace is not allowed when using backward hooks and     will raise an error.  Args:     hook (Callable): The user-defined hook to be registered.     prepend (bool): If true, the provided <code>hook</code> will be fired before         all existing <code>backward_pre</code> hooks on this         :class:<code>torch.nn.modules.Module</code>. Otherwise, the provided         <code>hook</code> will be fired after all existing <code>backward_pre</code> hooks         on this :class:<code>torch.nn.modules.Module</code>. Note that global         <code>backward_pre</code> hooks registered with         :func:<code>register_module_full_backward_pre_hook</code> will fire before         all hooks registered by this method.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook     (Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]])    </li> <li>prepend     (bool)     \u2013 defaults to <code>False</code> </li> </ul> register_load_state_dict_post_hook <p>Register a post-hook to be run after module's :meth:<code>~nn.Module.load_state_dict</code> is called.</p> <p>It should have the following signature::     hook(module, incompatible_keys) -&gt; None  The <code>module</code> argument is the current module that this hook is registered on, and the <code>incompatible_keys</code> argument is a <code>NamedTuple</code> consisting of attributes <code>missing_keys</code> and <code>unexpected_keys</code>. <code>missing_keys</code> is a <code>list</code> of <code>str</code> containing the missing keys and <code>unexpected_keys</code> is a <code>list</code> of <code>str</code> containing the unexpected keys.  The given incompatible_keys can be modified inplace if needed.  Note that the checks performed when calling :func:<code>load_state_dict</code> with <code>strict=True</code> are affected by modifications the hook makes to <code>missing_keys</code> or <code>unexpected_keys</code>, as expected. Additions to either set of keys will result in an error being thrown when <code>strict=True</code>, and clearing out both missing and unexpected keys will avoid an error.  Returns:     :class:<code>torch.utils.hooks.RemovableHandle</code>:         a handle that can be used to remove the added hook by calling         <code>handle.remove()</code></p> <p>Parameters</p> <ul> <li>hook </li> </ul> register_load_state_dict_pre_hook <p>Register a pre-hook to be run before module's :meth:<code>~nn.Module.load_state_dict</code> is called.</p> <p>It should have the following signature::     hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -&gt; None  # noqa: B950  Arguments:     hook (Callable): Callable hook that will be invoked before         loading the state dict.</p> <p>Parameters</p> <ul> <li>hook </li> </ul> register_module <p>Alias for :func:<code>add_module</code>.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>module     (Optional[ForwardRef('Module')])    </li> </ul> register_parameter <p>Add a parameter to the module.</p> <p>The parameter can be accessed as an attribute using given name.  Args:     name (str): name of the parameter. The parameter can be accessed         from this module using the given name     param (Parameter or None): parameter to be added to the module. If         <code>None</code>, then operations that run on parameters, such as :attr:<code>cuda</code>,         are ignored. If <code>None</code>, the parameter is not included in the         module's :attr:<code>state_dict</code>.</p> <p>Parameters</p> <ul> <li>name     (str)    </li> <li>param     (Optional[torch.nn.parameter.Parameter])    </li> </ul> register_state_dict_post_hook <p>Register a post-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method.</p> <p>It should have the following signature::     hook(module, state_dict, prefix, local_metadata) -&gt; None  The registered hooks can modify the <code>state_dict</code> inplace.</p> <p>Parameters</p> <ul> <li>hook </li> </ul> register_state_dict_pre_hook <p>Register a pre-hook for the :meth:<code>~torch.nn.Module.state_dict</code> method.</p> <p>It should have the following signature::     hook(module, prefix, keep_vars) -&gt; None  The registered hooks can be used to perform pre-processing before the <code>state_dict</code> call is made.</p> <p>Parameters</p> <ul> <li>hook </li> </ul> requires_grad_ <p>Change if autograd should record operations on parameters in this module.</p> <p>This method sets the parameters' :attr:<code>requires_grad</code> attributes in-place.  This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).  See :ref:<code>locally-disable-grad-doc</code> for a comparison between <code>.requires_grad_()</code> and several similar mechanisms that may be confused with it.  Args:     requires_grad (bool): whether autograd should record operations on                           parameters in this module. Default: <code>True</code>.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>requires_grad     (bool)     \u2013 defaults to <code>True</code> </li> </ul> save set_extra_state <p>Set extra state contained in the loaded <code>state_dict</code>.</p> <p>This function is called from :func:<code>load_state_dict</code> to handle any extra state found within the <code>state_dict</code>. Implement this function and a corresponding :func:<code>get_extra_state</code> for your module if you need to store extra state within its <code>state_dict</code>.  Args:     state (dict): Extra state from the <code>state_dict</code></p> <p>Parameters</p> <ul> <li>state     (Any)    </li> </ul> set_submodule <p>Set the submodule given by <code>target</code> if it exists, otherwise throw an error.</p> <p>For example, let's say you have an <code>nn.Module</code> <code>A</code> that looks like this:  .. code-block:: text      A(         (net_b): Module(             (net_c): Module(                 (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))             )             (linear): Linear(in_features=100, out_features=200, bias=True)         )     )  (The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)  To overide the <code>Conv2d</code> with a new submodule <code>Linear</code>, you would call <code>set_submodule(\"net_b.net_c.conv\", nn.Linear(33, 16))</code>.  Args:     target: The fully-qualified string name of the submodule         to look for. (See above example for how to specify a         fully-qualified string.)     module: The module to set the submodule to.  Raises:     ValueError: If the target string is empty     AttributeError: If the target string references an invalid         path or resolves to something that is not an         <code>nn.Module</code></p> <p>Parameters</p> <ul> <li>target     (str)    </li> <li>module     ('Module')    </li> </ul> share_memory <p>See :meth:<code>torch.Tensor.share_memory_</code>.</p> state_dict <p>Return a dictionary containing references to the whole state of the module.</p> <p>Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to <code>None</code> are not included.  .. note::     The returned object is a shallow copy. It contains references     to the module's parameters and buffers.  .. warning::     Currently <code>state_dict()</code> also accepts positional arguments for     <code>destination</code>, <code>prefix</code> and <code>keep_vars</code> in order. However,     this is being deprecated and keyword arguments will be enforced in     future releases.  .. warning::     Please avoid the use of argument <code>destination</code> as it is not     designed for end-users.  Args:     destination (dict, optional): If provided, the state of module will         be updated into the dict and the same object is returned.         Otherwise, an <code>OrderedDict</code> will be created and returned.         Default: <code>None</code>.     prefix (str, optional): a prefix added to parameter and buffer         names to compose the keys in state_dict. Default: <code>''</code>.     keep_vars (bool, optional): by default the :class:<code>~torch.Tensor</code> s         returned in the state dict are detached from autograd. If it's         set to <code>True</code>, detaching will not be performed.         Default: <code>False</code>.  Returns:     dict:         a dictionary containing a whole state of the module  Example::      &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")     &gt;&gt;&gt; module.state_dict().keys()     ['bias', 'weight']</p> <p>Parameters</p> <ul> <li>args </li> <li>destination     \u2013 defaults to <code>None</code> </li> <li>prefix     \u2013 defaults to ``    </li> <li>keep_vars     \u2013 defaults to <code>False</code> </li> </ul> to <p>Move and/or cast the parameters and buffers.</p> <p>This can be called as  .. function:: to(device=None, dtype=None, non_blocking=False)    :noindex:  .. function:: to(dtype, non_blocking=False)    :noindex:  .. function:: to(tensor, non_blocking=False)    :noindex:  .. function:: to(memory_format=torch.channels_last)    :noindex:  Its signature is similar to :meth:<code>torch.Tensor.to</code>, but only accepts floating point or complex :attr:<code>dtype</code>\\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr:<code>dtype</code> (if given). The integral parameters and buffers will be moved :attr:<code>device</code>, if that is given, but with dtypes unchanged. When :attr:<code>non_blocking</code> is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices.  See below for examples.  .. note::     This method modifies the module in-place.  Args:     device (:class:<code>torch.device</code>): the desired device of the parameters         and buffers in this module     dtype (:class:<code>torch.dtype</code>): the desired floating point or complex dtype of         the parameters and buffers in this module     tensor (torch.Tensor): Tensor whose dtype and device are the desired         dtype and device for all parameters and buffers in this module     memory_format (:class:<code>torch.memory_format</code>): the desired memory         format for 4D parameters and buffers in this module (keyword         only argument)  Returns:     Module: self  Examples::      &gt;&gt;&gt; # xdoctest: +IGNORE_WANT(\"non-deterministic\")     &gt;&gt;&gt; linear = nn.Linear(2, 2)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1913, -0.3420],             [-0.5113, -0.2325]])     &gt;&gt;&gt; linear.to(torch.double)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1913, -0.3420],             [-0.5113, -0.2325]], dtype=torch.float64)     &gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)     &gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")     &gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1914, -0.3420],             [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')     &gt;&gt;&gt; cpu = torch.device(\"cpu\")     &gt;&gt;&gt; linear.to(cpu)     Linear(in_features=2, out_features=2, bias=True)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.1914, -0.3420],             [-0.5112, -0.2324]], dtype=torch.float16)      &gt;&gt;&gt; linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)     &gt;&gt;&gt; linear.weight     Parameter containing:     tensor([[ 0.3741+0.j,  0.2382+0.j],             [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)     &gt;&gt;&gt; linear(torch.ones(3, 2, dtype=torch.cdouble))     tensor([[0.6122+0.j, 0.1150+0.j],             [0.6122+0.j, 0.1150+0.j],             [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)</p> <p>Parameters</p> <ul> <li>args </li> <li>kwargs </li> </ul> to_empty <p>Move the parameters and buffers to the specified device without copying storage.</p> <p>Args:     device (:class:<code>torch.device</code>): The desired device of the parameters         and buffers in this module.     recurse (bool): Whether parameters and buffers of submodules should         be recursively moved to the specified device.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, str, torch.device, NoneType])    </li> <li>recurse     (bool)     \u2013 defaults to <code>True</code> </li> </ul> train <p>Set the module in training mode.</p> <p>This has an effect only on certain modules. See the documentation of particular modules for details of their behaviors in training/evaluation mode, i.e., whether they are affected, e.g. :class:<code>Dropout</code>, :class:<code>BatchNorm</code>, etc.  Args:     mode (bool): whether to set training mode (<code>True</code>) or evaluation                  mode (<code>False</code>). Default: <code>True</code>.  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>mode     (bool)     \u2013 defaults to <code>True</code> </li> </ul> type <p>Casts all parameters and buffers to :attr:<code>dst_type</code>.</p> <p>.. note::     This method modifies the module in-place.  Args:     dst_type (type or string): the desired type  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>dst_type     (Union[torch.dtype, str])    </li> </ul> xpu <p>Move all model parameters and buffers to the XPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized.  .. note::     This method modifies the module in-place.  Arguments:     device (int, optional): if specified, all parameters will be         copied to that device  Returns:     Module: self</p> <p>Parameters</p> <ul> <li>device     (Union[int, torch.device, NoneType])     \u2013 defaults to <code>None</code> </li> </ul> zero_grad <p>Reset gradients of all model parameters.</p> <p>See similar function under :class:<code>torch.optim.Optimizer</code> for more context.  Args:     set_to_none (bool): instead of setting to zero, set the grads to None.         See :meth:<code>torch.optim.Optimizer.zero_grad</code> for details.</p> <p>Parameters</p> <ul> <li>set_to_none     (bool)     \u2013 defaults to <code>True</code> </li> </ul>"},{"location":"api/rank/rerank/","title":"rerank","text":"<p>Rerank the documents based on the queries embeddings.</p>"},{"location":"api/rank/rerank/#parameters","title":"Parameters","text":"<ul> <li> <p>documents_ids ('list[list[int | str]]')</p> <p>The documents ids.</p> </li> <li> <p>queries_embeddings ('list[list[float | int] | np.ndarray | torch.Tensor]')</p> <p>The queries embeddings which is a dictionary of queries and their embeddings.</p> </li> <li> <p>documents_embeddings ('list[list[float | int] | np.ndarray | torch.Tensor]')</p> <p>The documents embeddings which is a dictionary of documents ids and their embeddings.</p> </li> <li> <p>device ('str') \u2013 defaults to <code>None</code></p> </li> </ul>"},{"location":"api/rank/rerank/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from pylate import models, rank\n\n&gt;&gt;&gt; model = models.ColBERT(\n...     model_name_or_path=\"sentence-transformers/all-MiniLM-L6-v2\", device=\"cpu\"\n... )\n\n&gt;&gt;&gt; queries = [\n...     \"query A\",\n...     \"query B\",\n... ]\n\n&gt;&gt;&gt; documents = [\n...     [\"document A\", \"document B\"],\n...     [\"document 1\", \"document C\", \"document B\"],\n... ]\n\n&gt;&gt;&gt; documents_ids = [\n...    [1, 2],\n...    [1, 3, 2],\n... ]\n\n&gt;&gt;&gt; queries_embeddings = model.encode(\n...     queries,\n...     is_query=True,\n...     batch_size=1,\n... )\n\n&gt;&gt;&gt; documents_embeddings = model.encode(\n...     documents,\n...     is_query=False,\n...     batch_size=1,\n... )\n\n&gt;&gt;&gt; reranked_documents = rank.rerank(\n...     documents_ids=documents_ids,\n...     queries_embeddings=queries_embeddings,\n...     documents_embeddings=documents_embeddings,\n... )\n\n&gt;&gt;&gt; assert isinstance(reranked_documents, list)\n&gt;&gt;&gt; assert len(reranked_documents) == 2\n&gt;&gt;&gt; assert len(reranked_documents[0]) == 2\n&gt;&gt;&gt; assert len(reranked_documents[1]) == 3\n&gt;&gt;&gt; assert isinstance(reranked_documents[0], list)\n&gt;&gt;&gt; assert isinstance(reranked_documents[0][0], dict)\n&gt;&gt;&gt; assert \"id\" in reranked_documents[0][0]\n&gt;&gt;&gt; assert \"score\" in reranked_documents[0][0]\n</code></pre>"},{"location":"api/retrieve/ColBERT/","title":"ColBERT","text":"<p>ColBERT retriever.</p>"},{"location":"api/retrieve/ColBERT/#parameters","title":"Parameters","text":"<ul> <li>index ('Voyager | PLAID')</li> </ul>"},{"location":"api/retrieve/ColBERT/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from pylate import indexes, models, retrieve\n\n&gt;&gt;&gt; model = models.ColBERT(\n...     model_name_or_path=\"sentence-transformers/all-MiniLM-L6-v2\",\n...     device=\"cpu\",\n... )\n\n&gt;&gt;&gt; documents_ids = [\"1\", \"2\"]\n\n&gt;&gt;&gt; documents = [\n...     \"fruits are healthy.\",\n...     \"fruits are good for health.\",\n... ]\n\n&gt;&gt;&gt; documents_embeddings = model.encode(\n...     sentences=documents,\n...     batch_size=1,\n...     is_query=False,\n... )\n\n&gt;&gt;&gt; index = indexes.Voyager(\n...     index_folder=\"test_indexes\",\n...     index_name=\"colbert\",\n...     override=True,\n...     embedding_size=128,\n... )\n\n&gt;&gt;&gt; index = index.add_documents(\n...     documents_ids=documents_ids,\n...     documents_embeddings=documents_embeddings,\n... )\n\n&gt;&gt;&gt; retriever = retrieve.ColBERT(index=index)\n\n&gt;&gt;&gt; queries_embeddings = model.encode(\n...     [\"fruits are healthy.\", \"fruits are good for health.\"],\n...     batch_size=1,\n...     is_query=True,\n... )\n\n&gt;&gt;&gt; results = retriever.retrieve(\n...     queries_embeddings=queries_embeddings,\n...     k=2,\n...     device=\"cpu\",\n... )\n\n&gt;&gt;&gt; assert isinstance(results, list)\n&gt;&gt;&gt; assert len(results) == 2\n\n&gt;&gt;&gt; queries_embeddings = model.encode(\n...     \"fruits are healthy.\",\n...     batch_size=1,\n...     is_query=True,\n... )\n\n&gt;&gt;&gt; results = retriever.retrieve(\n...     queries_embeddings=queries_embeddings,\n...     k=2,\n...     device=\"cpu\",\n... )\n\n&gt;&gt;&gt; assert isinstance(results, list)\n&gt;&gt;&gt; assert len(results) == 1\n</code></pre>"},{"location":"api/retrieve/ColBERT/#methods","title":"Methods","text":"retrieve <p>Retrieve documents for a list of queries.</p> <p>Parameters</p> <ul> <li>queries_embeddings     ('list[list | np.ndarray | torch.Tensor]')    </li> <li>k     ('int')     \u2013 defaults to <code>10</code> </li> <li>k_token     ('int')     \u2013 defaults to <code>100</code> </li> <li>device     ('str | None')     \u2013 defaults to <code>None</code> </li> <li>batch_size     ('int')     \u2013 defaults to <code>50</code> </li> </ul>"},{"location":"api/scores/SimilarityFunction/","title":"SimilarityFunction","text":"<p>Enum class for supported score functions. The following functions are supported: - <code>SimilarityFunction.MAXSIM</code> (<code>\"MaxSim\"</code>): Max similarity</p>"},{"location":"api/scores/SimilarityFunction/#parameters","title":"Parameters","text":"<ul> <li>values</li> </ul>"},{"location":"api/scores/colbert-kd-scores/","title":"colbert_kd_scores","text":"<p>Computes the ColBERT scores between queries and documents embeddings. This scoring function is dedicated to the knowledge distillation pipeline.</p>"},{"location":"api/scores/colbert-kd-scores/#parameters","title":"Parameters","text":"<ul> <li> <p>queries_embeddings ('list | np.ndarray | torch.Tensor')</p> </li> <li> <p>documents_embeddings ('list | np.ndarray | torch.Tensor')</p> </li> <li> <p>mask ('torch.Tensor') \u2013 defaults to <code>None</code></p> </li> </ul>"},{"location":"api/scores/colbert-kd-scores/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; import torch\n\n&gt;&gt;&gt; queries_embeddings = torch.tensor([\n...     [[1.], [0.], [0.], [0.]],\n...     [[0.], [2.], [0.], [0.]],\n...     [[0.], [0.], [3.], [0.]],\n... ])\n\n&gt;&gt;&gt; documents_embeddings = torch.tensor([\n...     [[[10.], [0.], [1.]], [[20.], [0.], [1.]], [[30.], [0.], [1.]]],\n...     [[[0.], [100.], [1.]], [[0.], [200.], [1.]], [[0.], [300.], [1.]]],\n...     [[[1.], [0.], [1000.]], [[1.], [0.], [2000.]], [[10.], [0.], [3000.]]],\n... ])\n&gt;&gt;&gt; mask = torch.tensor([\n...     [[1., 1., 1.], [1., 1., 1.], [1., 1., 1.]],\n...     [[1., 1., 1.], [1., 1., 1.], [1., 1., 1.]],\n...     [[1., 1., 1.], [1., 1., 1.], [1., 1., 0.]],\n... ])\n&gt;&gt;&gt; colbert_kd_scores(\n...     queries_embeddings=queries_embeddings,\n...     documents_embeddings=documents_embeddings,\n...     mask=mask\n... )\ntensor([[  10.,   20.,   30.],\n        [ 200.,  400.,  600.],\n        [3000., 6000., 30.]])\n</code></pre>"},{"location":"api/scores/colbert-scores-pairwise/","title":"colbert_scores_pairwise","text":"<p>Computes the ColBERT score for each query-document pair. The score is computed as the sum of maximum similarities between the query and the document for corresponding pairs.</p>"},{"location":"api/scores/colbert-scores-pairwise/#parameters","title":"Parameters","text":"<ul> <li> <p>queries_embeddings ('torch.Tensor')</p> <p>The first tensor. The queries embeddings. Shape: (batch_size, num tokens queries, embedding_size)</p> </li> <li> <p>documents_embeddings ('torch.Tensor')</p> <p>The second tensor. The documents embeddings. Shape: (batch_size, num tokens documents, embedding_size)</p> </li> </ul>"},{"location":"api/scores/colbert-scores-pairwise/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; import torch\n\n&gt;&gt;&gt; queries_embeddings = torch.tensor([\n...     [[1.], [0.], [0.], [0.]],\n...     [[0.], [2.], [0.], [0.]],\n...     [[0.], [0.], [3.], [0.]],\n... ])\n\n&gt;&gt;&gt; documents_embeddings = torch.tensor([\n...     [[10.], [0.], [1.]],\n...     [[0.], [100.], [1.]],\n...     [[1.], [0.], [1000.]],\n... ])\n\n&gt;&gt;&gt; scores = colbert_scores_pairwise(\n...     queries_embeddings=queries_embeddings,\n...     documents_embeddings=documents_embeddings\n... )\n\n&gt;&gt;&gt; scores\ntensor([  10.,  200., 3000.])\n</code></pre>"},{"location":"api/scores/colbert-scores/","title":"colbert_scores","text":"<p>Computes the ColBERT scores between queries and documents embeddings. The score is computed as the sum of maximum similarities between the query and the document.</p>"},{"location":"api/scores/colbert-scores/#parameters","title":"Parameters","text":"<ul> <li> <p>queries_embeddings ('list | np.ndarray | torch.Tensor')</p> <p>The first tensor. The queries embeddings. Shape: (batch_size, num tokens queries, embedding_size)</p> </li> <li> <p>documents_embeddings ('list | np.ndarray | torch.Tensor')</p> <p>The second tensor. The documents embeddings. Shape: (batch_size, num tokens documents, embedding_size)</p> </li> <li> <p>mask ('torch.Tensor') \u2013 defaults to <code>None</code></p> </li> </ul>"},{"location":"api/scores/colbert-scores/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; import torch\n\n&gt;&gt;&gt; queries_embeddings = torch.tensor([\n...     [[1.], [0.], [0.], [0.]],\n...     [[0.], [2.], [0.], [0.]],\n...     [[0.], [0.], [3.], [0.]],\n... ])\n\n&gt;&gt;&gt; documents_embeddings = torch.tensor([\n...     [[10.], [0.], [1.]],\n...     [[0.], [100.], [1.]],\n...     [[1.], [0.], [1000.]],\n... ])\n\n&gt;&gt;&gt; scores = colbert_scores(\n...     queries_embeddings=queries_embeddings,\n...     documents_embeddings=documents_embeddings\n... )\n\n&gt;&gt;&gt; scores\ntensor([[  10.,  100., 1000.],\n        [  20.,  200., 2000.],\n        [  30.,  300., 3000.]])\n</code></pre>"},{"location":"api/utils/ColBERTCollator/","title":"ColBERTCollator","text":"<p>Collator for ColBERT model.</p>"},{"location":"api/utils/ColBERTCollator/#parameters","title":"Parameters","text":"<ul> <li> <p>tokenize_fn ('Callable')</p> <p>The function to tokenize the input text.</p> </li> <li> <p>valid_label_columns ('list[str] | None') \u2013 defaults to <code>None</code></p> <p>The name of the columns that contain the labels: scores or labels.</p> </li> </ul>"},{"location":"api/utils/ColBERTCollator/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from pylate import models, utils\n\n&gt;&gt;&gt; model = models.ColBERT(\n...     model_name_or_path=\"sentence-transformers/all-MiniLM-L6-v2\", device=\"cpu\"\n... )\n\n&gt;&gt;&gt; collator = utils.ColBERTCollator(\n...     tokenize_fn=model.tokenize,\n... )\n\n&gt;&gt;&gt; features = [\n...     {\n...         \"query\": \"fruits are healthy.\",\n...         \"positive\": \"fruits are good for health.\",\n...         \"negative\": \"fruits are bad for health.\",\n...         \"label\": [0.7, 0.3]\n...     }\n... ]\n\n&gt;&gt;&gt; features = collator(features=features)\n\n&gt;&gt;&gt; fields = [\n...     \"query_input_ids\",\n...     \"positive_input_ids\",\n...     \"negative_input_ids\",\n...     \"query_attention_mask\",\n...     \"positive_attention_mask\",\n...     \"negative_attention_mask\",\n...     \"query_token_type_ids\",\n...     \"positive_token_type_ids\",\n...     \"negative_token_type_ids\",\n... ]\n\n&gt;&gt;&gt; for field in fields:\n...     assert field in features\n...     assert isinstance(features[field], torch.Tensor)\n...     assert features[field].ndim == 2\n</code></pre>"},{"location":"api/utils/ColBERTCollator/#methods","title":"Methods","text":"call <p>Collate a list of features into a batch.</p> <p>Parameters</p> <ul> <li>features     ('list[dict]')    </li> </ul>"},{"location":"api/utils/KDProcessing/","title":"KDProcessing","text":"<p>Dataset processing class for knowledge distillation training.</p>"},{"location":"api/utils/KDProcessing/#parameters","title":"Parameters","text":"<ul> <li> <p>queries ('datasets.Dataset | datasets.DatasetDict')</p> <p>Queries dataset.</p> </li> <li> <p>documents ('datasets.Dataset | datasets.DatasetDict')</p> <p>Documents dataset.</p> </li> <li> <p>split ('str') \u2013 defaults to <code>train</code></p> <p>Split to use for the queries and documents datasets. Used only if the queries and documents are of type <code>datasets.DatasetDict</code>.</p> </li> <li> <p>n_ways ('int') \u2013 defaults to <code>32</code></p> <p>Number of scores to keep for the distillation.</p> </li> </ul>"},{"location":"api/utils/KDProcessing/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from datasets import load_dataset\n&gt;&gt;&gt; from pylate import utils\n\n&gt;&gt;&gt; train = load_dataset(\n...    path=\"lightonai/lighton-ms-marco-mini\",\n...    name=\"train\",\n...    split=\"train\",\n... )\n\n&gt;&gt;&gt; queries = load_dataset(\n...    path=\"lightonai/lighton-ms-marco-mini\",\n...    name=\"queries\",\n...    split=\"train\",\n... )\n\n&gt;&gt;&gt; documents = load_dataset(\n...    path=\"lightonai/lighton-ms-marco-mini\",\n...    name=\"documents\",\n...    split=\"train\",\n... )\n\n&gt;&gt;&gt; train.set_transform(\n...    utils.KDProcessing(\n...        queries=queries, documents=documents\n...    ).transform,\n... )\n\n&gt;&gt;&gt; for sample in train:\n...     assert \"documents\" in sample and isinstance(sample[\"documents\"], list)\n...     assert \"query\" in sample and isinstance(sample[\"query\"], str)\n...     assert \"scores\" in sample and isinstance(sample[\"scores\"], list)\n</code></pre>"},{"location":"api/utils/KDProcessing/#methods","title":"Methods","text":"map <p>Process a single example.</p> <p>Parameters</p> <ul> <li>example     ('dict')    </li> </ul> transform <p>Update the input dataset with the queries and documents.</p> <p>Parameters</p> <ul> <li>examples     ('dict')    </li> </ul>"},{"location":"api/utils/all-gather-with-gradients/","title":"all_gather_with_gradients","text":"<p>Gathers a tensor from each distributed rank into a list. All the tensors will retain gradients. This is the same as <code>all_gather</code>, but all the tensors will retain gradients and is used to compute contrastive with local queries only to lower the memory usage, see https://github.com/mlfoundations/open_clip/issues/616</p> <ul> <li> <p>If torch.distributed is available and initialized, gather all the tensors (with gradients) from each rank into a list </p> </li> <li> <p>If torch.distributed is either unavailable, uninitialized, or   <code>world_size == 1</code>, it returns a list containing only the   original tensor and throws a warning to notify the user (helpful when using a single GPU setup).</p> </li> </ul>"},{"location":"api/utils/all-gather-with-gradients/#parameters","title":"Parameters","text":"<ul> <li>tensor ('torch.Tensor')</li> </ul>"},{"location":"api/utils/all-gather/","title":"all_gather","text":"<p>Gathers a tensor from each distributed rank into a list. The tensor for the local rank is the original one, with the gradients while the others have no gradients.</p> <ul> <li> <p>If torch.distributed is available and initialized:   1. Creates a list of tensors (each sized like the input <code>tensor</code>).   2. Gathers tensors from each rank into that list.   3. Replaces the local tensor in the list with the original tensor that retains gradients. </p> </li> <li> <p>If torch.distributed is either unavailable, uninitialized, or   <code>world_size == 1</code>, it returns a list containing only the   original tensor and throws a warning to notify the user (helpful when using a single GPU setup).</p> </li> </ul>"},{"location":"api/utils/all-gather/#parameters","title":"Parameters","text":"<ul> <li>tensor ('torch.Tensor')</li> </ul>"},{"location":"api/utils/convert-to-tensor/","title":"convert_to_tensor","text":"<p>Converts a list or numpy array to a torch tensor.</p>"},{"location":"api/utils/convert-to-tensor/#parameters","title":"Parameters","text":"<ul> <li> <p>x ('torch.Tensor | np.ndarray | list[torch.Tensor | np.ndarray | list | float]')</p> <p>The input data. It can be a torch tensor, a numpy array, or a list of torch tensors, numpy arrays, or lists.</p> </li> </ul>"},{"location":"api/utils/convert-to-tensor/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import torch\n\n&gt;&gt;&gt; x = torch.tensor([[1., 1., 1.], [2., 2., 2.]])\n&gt;&gt;&gt; convert_to_tensor(x)\ntensor([[1., 1., 1.],\n        [2., 2., 2.]])\n\n&gt;&gt;&gt; x = np.array([[1., 1., 1.], [2., 2., 2.]], dtype=np.float32)\n&gt;&gt;&gt; convert_to_tensor(x)\ntensor([[1., 1., 1.],\n        [2., 2., 2.]])\n\n&gt;&gt;&gt; x = []\n&gt;&gt;&gt; convert_to_tensor(x)\ntensor([])\n\n&gt;&gt;&gt; x = [np.array([1., 1., 1.])]\n&gt;&gt;&gt; convert_to_tensor(x)\ntensor([[1., 1., 1.]])\n\n&gt;&gt;&gt; x = [[1., 1., 1.]]\n&gt;&gt;&gt; convert_to_tensor(x)\ntensor([[1., 1., 1.]])\n\n&gt;&gt;&gt; x = [torch.tensor([1., 1., 1.]), torch.tensor([2., 2., 2.])]\n&gt;&gt;&gt; convert_to_tensor(x)\ntensor([[1., 1., 1.],\n        [2., 2., 2.]])\n\n&gt;&gt;&gt; x = np.array([], dtype=np.float32)\n&gt;&gt;&gt; convert_to_tensor(x)\ntensor([])\n</code></pre>"},{"location":"api/utils/get-rank/","title":"get_rank","text":"<p>Returns the current rank in a distributed training.</p>"},{"location":"api/utils/get-world-size/","title":"get_world_size","text":"<p>Returns the world size in a distributed training.</p>"},{"location":"api/utils/iter-batch/","title":"iter_batch","text":"<p>Iterate over a list of elements by batch.</p>"},{"location":"api/utils/iter-batch/#parameters","title":"Parameters","text":"<ul> <li> <p>X ('list[str]')</p> </li> <li> <p>batch_size ('int')</p> </li> <li> <p>tqdm_bar ('bool') \u2013 defaults to <code>True</code></p> </li> <li> <p>desc ('str') \u2013 defaults to ``</p> </li> </ul>"},{"location":"api/utils/iter-batch/#examples","title":"Examples","text":"<pre><code>&gt;&gt;&gt; from pylate import utils\n\n&gt;&gt;&gt; X = [\n...  \"element 0\",\n...  \"element 1\",\n...  \"element 2\",\n...  \"element 3\",\n...  \"element 4\",\n... ]\n\n&gt;&gt;&gt; n_samples = 0\n&gt;&gt;&gt; for batch in utils.iter_batch(X, batch_size=2):\n...     n_samples += len(batch)\n\n&gt;&gt;&gt; n_samples\n5\n</code></pre>"},{"location":"documentation/datasets/","title":"Datasets","text":"<p>PyLate is designed to be compatible with Hugging Face datasets, enabling seamless integration for tasks like knowledge distillation and contrastive model training. Below are examples of how to load and prepare datasets for these specific training objectives.</p>"},{"location":"documentation/datasets/#contrastive-dataset","title":"Contrastive Dataset","text":"<p>Contrastive training requires datasets that include a query, a positive document (relevant to the query), and a negative document (irrelevant to the query). This is the standard triplet format used by Sentence Transformers, making PyLate's contrastive training compatible with all existing triplet datasets.</p>"},{"location":"documentation/datasets/#loading-a-pre-built-contrastive-dataset","title":"Loading a pre-built contrastive dataset","text":"<p>You can directly download an existing contrastive dataset from Hugging Face's hub, such as the msmarco-bm25 triplet dataset.</p> <pre><code>from datasets import load_dataset\n\ndataset = load_dataset(\"sentence-transformers/msmarco-bm25\", \"triplet\", split=\"train\")\n\ntrain_dataset, test_dataset = dataset.train_test_split(test_size=0.001)\n</code></pre> <p>Then we can shuffle the dataset:</p> <pre><code>train_dataset = train_dataset.shuffle(seed=42)\n</code></pre> <p>And select a subset of the dataset if needed:</p> <pre><code>train_dataset = train_dataset.select(range(10_000))\n</code></pre>"},{"location":"documentation/datasets/#creating-a-contrastive-dataset-from-list","title":"Creating a contrastive dataset from list","text":"<p>If you want to create a custom contrastive dataset, you can do so by manually specifying the query, positive, and negative samples.</p> <pre><code>from datasets import Dataset\n\ndataset = [\n    {\n        \"query\": \"example query 1\",\n        \"positive\": \"example positive document 1\",\n        \"negative\": \"example negative document 1\",\n    },\n    {\n        \"query\": \"example query 2\",\n        \"positive\": \"example positive document 2\",\n        \"negative\": \"example negative document 2\",\n    },\n    {\n        \"query\": \"example query 3\",\n        \"positive\": \"example positive document 3\",\n        \"negative\": \"example negative document 3\",\n    },\n]\n\ndataset = Dataset.from_list(mapping=dataset)\n\ntrain_dataset, test_dataset = dataset.train_test_split(test_size=0.3)\n</code></pre> Tip <p>Note that PyLate supports more than one negative per query, simply add the additional negatives after the first one in the row. <pre><code>{\n        \"query\": \"example query 1\",\n        \"positive\": \"example positive document 1\",\n        \"negative_1\": \"example negative document 1\",\n        \"negative_2\": \"example negative document 2\",\n}\n</code></pre></p>"},{"location":"documentation/datasets/#loading-a-contrastive-dataset-from-a-local-parquet-file","title":"Loading a contrastive dataset from a local parquet file","text":"<p>To load a local dataset stored in a Parquet file:</p> <pre><code>from datasets import load_dataset\n\ndataset = load_dataset(\n    path=\"parquet\",\n    data_files=\"dataset.parquet\",\n    split=\"train\"\n)\n\ntrain_dataset, test_dataset = dataset.train_test_split(test_size=0.001)\n</code></pre>"},{"location":"documentation/datasets/#knowledge-distillation-dataset","title":"Knowledge distillation dataset","text":"<p>For fine-tuning a model using knowledge distillation loss, three distinct dataset files are required: train, queries, and documents.</p> Info <p>Each file contains unique and complementary information necessary for the distillation process:</p> <ul> <li><code>train</code>: Contains three columns: <code>['query_id', 'document_ids', 'scores']</code><ul> <li><code>query_id</code> refers to the query identifier.</li> <li><code>document_ids</code> is a list of document IDs relevant to the query.</li> <li><code>scores</code> corresponds to the relevance scores between the query and each document.</li> </ul> </li> </ul>"},{"location":"documentation/datasets/#train","title":"Train","text":"<p>Example entry:</p> <pre><code>{\n    \"query_id\": 54528,\n    \"document_ids\": [\n        6862419,\n        335116,\n        339186,\n        7509316,\n        7361291,\n        7416534,\n        5789936,\n        5645247,\n    ],\n    \"scores\": [\n        0.4546215673141326,\n        0.6575686537173476,\n        0.26825184192900203,\n        0.5256195579370395,\n        0.879939718687207,\n        0.7894968184862693,\n        0.6450100468854655,\n        0.5823844608171467,\n    ],\n}\n</code></pre> Warning <p>Ensure that the length of <code>document_ids</code> matches the length of <code>scores</code>.</p>"},{"location":"documentation/datasets/#queries","title":"Queries","text":"<ul> <li><code>queries</code>: Contains two columns: <code>['query_id', 'text']</code></li> </ul> <p>Example entry:</p> <pre><code>{\"query_id\": 749480, \"text\": \"example query 1\"}\n</code></pre>"},{"location":"documentation/datasets/#documents","title":"Documents","text":"<ul> <li><code>documents</code>: contains two columns: <code>['document_ids', 'text']</code></li> </ul> <p>Example entry:</p> <pre><code>{\n    \"document_id\": 136062,\n    \"text\": \"example document 1\",\n}\n</code></pre>"},{"location":"documentation/datasets/#loading-a-pre-built-knowledge-distillation-dataset","title":"Loading a pre-built knowledge distillation dataset","text":"<p>You can directly download an existing knowledge distillation dataset from Hugging Face's hub, such as the English MS MARCO dataset with BGE M3 scores or the French version. Simply load the different files by giving the respective names to the <code>load_dataset</code> function:</p> <pre><code>from datasets import load_dataset\n\ntrain = load_dataset(\n    \"lightonai/ms-marco-en-bge\",\n    \"train\",\n    split=\"train\",\n)\n\nqueries = load_dataset(\n    \"lightonai/ms-marco-en-bge\",\n    \"queries\",\n    split=\"train\",\n)\n\ndocuments = load_dataset(\n    \"lightonai/ms-marco-en-bge\",\n    \"documents\",\n    split=\"train\",\n)\n</code></pre>"},{"location":"documentation/datasets/#knowledge-distillation-dataset-from-list","title":"Knowledge distillation dataset from list","text":"<p>You can also create custom datasets from list in Python. This example demonstrates how to build the <code>train</code>, <code>queries</code>, and <code>documents</code> datasets</p> <pre><code>from datasets import Dataset\n\ndataset = [\n    {\n        \"query_id\": 54528,\n        \"document_ids\": [\n            6862419,\n            335116,\n            339186,\n            7509316,\n            7361291,\n            7416534,\n            5789936,\n            5645247,\n        ],\n        \"scores\": [\n            0.4546215673141326,\n            0.6575686537173476,\n            0.26825184192900203,\n            0.5256195579370395,\n            0.879939718687207,\n            0.7894968184862693,\n            0.6450100468854655,\n            0.5823844608171467,\n        ],\n    },\n    {\n        \"query_id\": 749480,\n        \"document_ids\": [\n            6862419,\n            335116,\n            339186,\n            7509316,\n            7361291,\n            7416534,\n            5789936,\n            5645247,\n        ],\n        \"scores\": [\n            0.2546215673141326,\n            0.7575686537173476,\n            0.96825184192900203,\n            0.0256195579370395,\n            0.779939718687207,\n            0.2894968184862693,\n            0.1450100468854655,\n            0.7823844608171467,\n        ],\n    },\n]\n\n\ndataset = Dataset.from_list(mapping=dataset)\n\ndocuments = [\n    {\"document_id\": 6862419, \"text\": \"example document 1\"},\n    {\"document_id\": 335116, \"text\": \"example document 2\"},\n    {\"document_id\": 339186, \"text\": \"example document 3\"},\n    {\"document_id\": 7509316, \"text\": \"example document 4\"},\n    {\"document_id\": 7361291, \"text\": \"example document 5\"},\n    {\"document_id\": 7416534, \"text\": \"example document 6\"},\n    {\"document_id\": 5789936, \"text\": \"example document 7\"},\n    {\"document_id\": 5645247, \"text\": \"example document 8\"},\n]\n\nqueries = [\n    {\"query_id\": 749480, \"text\": \"example query 1\"},\n    {\"query_id\": 54528, \"text\": \"example query 2\"},\n]\n\ndocuments = Dataset.from_list(mapping=documents)\n\nqueries = Dataset.from_list(mapping=queries)\n</code></pre>"},{"location":"documentation/evaluation/","title":"Evaluation","text":""},{"location":"documentation/evaluation/#retrieval-evaluation","title":"Retrieval evaluation","text":"<p>This guide demonstrates an end-to-end pipeline to evaluate the performance of the ColBERT model on retrieval tasks. The pipeline involves three key steps: indexing documents, retrieving top-k documents for a given set of queries, and evaluating the retrieval results using standard metrics.</p>"},{"location":"documentation/evaluation/#beir-retrieval-evaluation-pipeline","title":"BEIR Retrieval Evaluation Pipeline","text":"<pre><code>from pylate import evaluation, indexes, models, retrieve\n\n# Step 1: Initialize the ColBERT model\n\ndataset = \"scifact\" # Choose the dataset you want to evaluate\nmodel = models.ColBERT(\n    model_name_or_path=\"lightonai/GTE-ModernColBERT-v1\",\n    device=\"cuda\" # \"cpu\" or \"cuda\" or \"mps\"\n)\n\n# Step 2: Create a Voyager index\nindex = indexes.Voyager(\n    index_folder=\"pylate-index\",\n    index_name=dataset,\n    override=True,  # Overwrite any existing index\n)\n\n# Step 3: Load the documents, queries, and relevance judgments (qrels)\ndocuments, queries, qrels = evaluation.load_beir(\n    dataset,  # Specify the dataset (e.g., \"scifact\")\n    split=\"test\",  # Specify the split (e.g., \"test\")\n)\n\n# Step 4: Encode the documents\ndocuments_embeddings = model.encode(\n    [document[\"text\"] for document in documents],\n    batch_size=32,\n    is_query=False,  # Indicate that these are documents\n    show_progress_bar=True,\n)\n\n# Step 5: Add document embeddings to the index\nindex.add_documents(\n    documents_ids=[document[\"id\"] for document in documents],\n    documents_embeddings=documents_embeddings,\n)\n\n# Step 6: Encode the queries\nqueries_embeddings = model.encode(\n    queries,\n    batch_size=32,\n    is_query=True,  # Indicate that these are queries\n    show_progress_bar=True,\n)\n\n# Step 7: Retrieve top-k documents\nretriever = retrieve.ColBERT(index=index)\nscores = retriever.retrieve(\n    queries_embeddings=queries_embeddings,\n    k=100,  # Retrieve the top 100 matches for each query\n)\n\n# Step 8: Evaluate the retrieval results\nresults = evaluation.evaluate(\n    scores=scores,\n    qrels=qrels,\n    queries=queries,\n    metrics=[f\"ndcg@{k}\" for k in [1, 3, 5, 10, 100]] # NDCG for different k values\n    + [f\"hits@{k}\" for k in [1, 3, 5, 10, 100]]       # Hits at different k values\n    + [\"map\"]                                         # Mean Average Precision (MAP)\n    + [\"recall@10\", \"recall@100\"]                     # Recall at k\n    + [\"precision@10\", \"precision@100\"],              # Precision at k\n)\n\nprint(results)\n</code></pre> <p>The output is a dictionary containing various evaluation metrics. Here\u2019s a sample output:</p> <pre><code>{\n    \"ndcg@1\": 0.47333333333333333,\n    \"ndcg@3\": 0.543862513095773,\n    \"ndcg@5\": 0.5623210323686343,\n    \"ndcg@10\": 0.5891793972249917,\n    \"ndcg@100\": 0.5891793972249917,\n    \"hits@1\": 0.47333333333333333,\n    \"hits@3\": 0.64,\n    \"hits@5\": 0.7033333333333334,\n    \"hits@10\": 0.8,\n    \"hits@100\": 0.8,\n    \"map\": 0.5442202380952381,\n    \"recall@10\": 0.7160555555555556,\n    \"recall@100\": 0.7160555555555556,\n    \"precision@10\": 0.08,\n    \"precision@100\": 0.008000000000000002,\n}\n</code></pre> Info <ol> <li> <p>is_query flag: Always set is_query=True when encoding queries and is_query=False when encoding documents. This ensures the model applies the correct prefixes for queries and documents.</p> </li> <li> <p>Evaluation metrics: The pipeline supports a wide range of evaluation metrics, including NDCG, hits, MAP, recall, and precision, with different cutoff points.</p> </li> <li> <p>Relevance judgments (qrels): The qrels are used to calculate how well the retrieved documents match the ground truth.</p> </li> </ol>"},{"location":"documentation/evaluation/#beir-datasets","title":"BEIR datasets","text":"<p>The following table lists the datasets available in the BEIR benchmark along with their names, types, number of queries, corpus size, and relevance degree per query. Source: BEIR Datasets</p> Table Dataset BEIR-Name Type Queries Corpus MSMARCO msmarco train, dev, test 6,980 8,840,000 TREC-COVID trec-covid test 50 171,000 NFCorpus nfcorpus train, dev, test 323 3,600 BioASQ bioasq train, test 500 14,910,000 NQ nq train, test 3,452 2,680,000 HotpotQA hotpotqa train, dev, test 7,405 5,230,000 FiQA-2018 fiqa train, dev, test 648 57,000 Signal-1M(RT) signal1m test 97 2,860,000 TREC-NEWS trec-news test 57 595,000 Robust04 robust04 test 249 528,000 ArguAna arguana test 1,406 8,670 Touche-2020 webis-touche2020 test 49 382,000 CQADupstack cqadupstack test 13,145 457,000 Quora quora dev, test 10,000 523,000 DBPedia dbpedia-entity dev, test 400 4,630,000 SCIDOCS scidocs test 1,000 25,000 FEVER fever train, dev, test 6,666 5,420,000 Climate-FEVER climate-fever test 1,535 5,420,000 SciFact scifact train, test 300 5,000"},{"location":"documentation/evaluation/#custom-datasets","title":"Custom datasets","text":"<p>You can also run evaluation on your custom dataset using the following structure:</p> <ul> <li><code>corpus.jsonl</code>: each row contains a json element with two properties: <code>['_id', 'text']</code><ul> <li><code>_id</code> refers to the document identifier.</li> <li><code>text</code> contains the text of the document.</li> <li>(an additional <code>title</code> field can also be added if necessary)</li> </ul> </li> <li><code>queries.jsonl</code>: each row contains a json element with two properties: <code>['_id', 'text']</code><ul> <li><code>_id</code> refers to the query identifier.</li> <li><code>text</code> contains the text of the query.</li> </ul> </li> <li><code>qrels</code> folder contains tsv files with three columns: <code>['query-id', 'doc-id', 'score']</code><ul> <li><code>query-id</code> refers to the query identifier.</li> <li><code>doc-id</code> refers to the document identifier.</li> <li><code>score</code> contains the relation between the query and the document (1 if relevant, else 0) The name of the tsv corresponds to the split (e.g, \"dev\").</li> </ul> </li> </ul> <p>You can then use the same pipeline as with BEIR datasets by changing the loading of the data in step 3:</p> <pre><code>documents, queries, qrels = evaluation.load_custom_dataset(\n    \"custom_dataset\", split=\"dev\"\n)\n</code></pre>"},{"location":"documentation/evaluation/#metrics","title":"Metrics","text":"<p>PyLate evaluation is based on Ranx Python library to compute standard Information Retrieval metrics. The following metrics are supported:</p> Table Metric Alias @k Hits hits Yes Hit Rate / Success hit_rate Yes Precision precision Yes Recall recall Yes F1 f1 Yes R-Precision r_precision No Bpref bpref No Rank-biased Precision rbp No Mean Reciprocal Rank mrr Yes Mean Average Precision map Yes DCG dcg Yes DCG Burges dcg_burges Yes NDCG ndcg Yes NDCG Burges ndcg_burges Yes <p>For any details about the metrics, please refer to Ranx documentation.</p> <p>Sample code to evaluate the retrieval results using specific metrics:</p> <pre><code>results = evaluation.evaluate(\n    scores=scores,\n    qrels=qrels,\n    queries=queries,\n    metrics=[f\"ndcg@{k}\" for k in [1, 3, 5, 10, 100]] # NDCG for different k values\n    + [f\"hits@{k}\" for k in [1, 3, 5, 10, 100]]       # Hits at different k values\n    + [\"map\"]                                         # Mean Average Precision (MAP)\n    + [\"recall@10\", \"recall@100\"]                     # Recall at k\n    + [\"precision@10\", \"precision@100\"],              # Precision at k\n)\n</code></pre>"},{"location":"documentation/fastapi/","title":"Serve the embeddings of a PyLate model using FastAPI","text":"<p>The <code>server.py</code> script (located in the <code>server</code> folder) allows to create a FastAPI server to serve the embeddings of a PyLate model. To use it, you need to install the api dependencies: <code>pip install \"pylate[api]\"</code> Then, run <code>python server.py</code> to launch the server.</p> <p>You can then send requests to the API like so: <pre><code>curl -X POST http://localhost:8002/v1/embeddings \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"input\": [\"Query 1\", \"Query 2\"],\n    \"model\": \"lightonai/GTE-ModernColBERT-v1\",\n    \"is_query\": false\n  }'\n</code></pre> If you want to encode queries, simply set <code>\u00ecs_query</code> to <code>True</code>.</p> Tip <p>Note that the server leverages batched, so you can do batch processing by sending multiple separate calls and it will create batches dynamically to fill up the GPU.</p> <p>For now, the server only support one loaded model, which you can define by using the <code>--model</code> argument when launching the server.</p>"},{"location":"documentation/retrieval/","title":"Retrieval","text":""},{"location":"documentation/retrieval/#colbert-retrieval","title":"ColBERT Retrieval","text":"<p>PyLate provides a streamlined interface to index and retrieve documents using ColBERT models.</p>"},{"location":"documentation/retrieval/#plaid","title":"PLAID","text":"<p>This index leverages PLAID to reduce storage cost and increase speed of retrieval.</p>"},{"location":"documentation/retrieval/#indexing-documents","title":"Indexing documents","text":"<p>First, load the ColBERT model and initialize the PLAID index, then encode and index your documents:</p> <pre><code>from pylate import indexes, models, retrieve\n\n# Step 1: Load the ColBERT model\nmodel = models.ColBERT(\n    model_name_or_path=\"lightonai/GTE-ModernColBERT-v1\",\n)\n\n# Step 2: Initialize the PLAID index\nindex = indexes.PLAID(\n    index_folder=\"pylate-index\",\n    index_name=\"index\",\n    override=True,  # This overwrites the existing index if any\n)\n\n# Step 3: Encode the documents\ndocuments_ids = [\"1\", \"2\", \"3\"]\ndocuments = [\"document 1 text\", \"document 2 text\", \"document 3 text\"]\n\ndocuments_embeddings = model.encode(\n    documents,\n    batch_size=32,\n    is_query=False,  # Ensure that it is set to False to indicate that these are documents, not queries\n    show_progress_bar=True,\n)\n\n# Step 4: Add document embeddings to the index by providing embeddings and corresponding ids\nindex.add_documents(\n    documents_ids=documents_ids,\n    documents_embeddings=documents_embeddings,\n)\n</code></pre> Tip <p>Note that PLAID benefits from indexing all the documents during the creation of the index (allowing to get the most accurate kmeans computation). Subsequent addition of documents are supported as an experimental feature.</p> <p>Note that you do not have to recreate the index and encode the documents every time. Once you have created an index and added the documents, you can re-use the index later by loading it:</p> <pre><code># To load an index, simply instantiate it with the correct folder/name and without overriding it\nindex = indexes.PLAID(\n    index_folder=\"pylate-index\",\n    index_name=\"index\",\n)\n</code></pre> Tip"},{"location":"documentation/retrieval/#pooling-document-embeddings","title":"Pooling document embeddings","text":"<p>In this blog post, we showed that similar tokens in document embeddings can be pooled together to reduce the overall cost of ColBERT indexing without without losing much performance.</p> <p>You can use this feature by setting the <code>pool_factor</code> parameter when encoding the documents to only keep 1 / <code>pool_factor</code> tokens.</p> <p>The results show that using a <code>pool_factor</code> of 2 cut the memory requirement of the index in half with virtually 0 performance drop. Higher compression can be achieved at the cost of some performance, please refer to the blog post for all the details and results.</p> <p>This simple modification to the encoding call thus save a lot of space with a very contained impact on the performances:</p> <pre><code>documents_embeddings = model.encode(\n    documents,\n    batch_size=32,\n    is_query=False,  # Ensure that it is set to False to indicate that these are documents, not queries\n    pool_factor=2,\n    show_progress_bar=True,\n)\n</code></pre>"},{"location":"documentation/retrieval/#retrieving-top-k-documents-for-queries","title":"Retrieving top-k documents for queries","text":"<p>Once the documents are indexed, you can retrieve the top-k most relevant documents for a given set of queries. To do so, initialize the ColBERT retriever with the index you want to search in, encode the queries and then retrieve the top-k documents to get the top matches ids and relevance scores:</p> <pre><code># Step 1: Initialize the ColBERT retriever\nretriever = retrieve.ColBERT(index=index)\n\n# Step 2: Encode the queries\nqueries_embeddings = model.encode(\n    [\"query for document 3\", \"query for document 1\"],\n    batch_size=32,\n    is_query=True,  #  # Ensure that it is set to False to indicate that these are queries\n    show_progress_bar=True,\n)\n\n# Step 3: Retrieve top-k documents\nscores = retriever.retrieve(\n    queries_embeddings=queries_embeddings,\n    k=10,  # Retrieve the top 10 matches for each query\n)\n\nprint(scores)\n</code></pre> <p>Example output</p> <pre><code>[\n    [   # Candidates for the first query\n        {\"id\": \"3\", \"score\": 11.266985893249512},\n        {\"id\": \"1\", \"score\": 10.303335189819336},\n        {\"id\": \"2\", \"score\": 9.502392768859863},\n    ],\n    [   # Candidates for the second query\n        {\"id\": \"1\", \"score\": 10.88800048828125},\n        {\"id\": \"3\", \"score\": 9.950843811035156},\n        {\"id\": \"2\", \"score\": 9.602447509765625},\n    ],\n]\n</code></pre>"},{"location":"documentation/retrieval/#remove-documents-from-the-index","title":"Remove documents from the index","text":"<p>To remove documents from the index, use the <code>remove_documents</code> method. Provide the document IDs you want to remove from the index:</p> <pre><code>index.remove_documents([\"1\", \"2\"])\n</code></pre>"},{"location":"documentation/retrieval/#parameters-affecting-the-retrieval-performance","title":"Parameters affecting the retrieval performance","text":"<p>The retrieval is not an exact search, which mean that certain parameters can affect the quality of the approximate search:</p> <ul> <li><code>nbits</code>: the number of bits to use for the quantization of the residual. Usually set to 2, larger values (4-8) will decrease the quantization errors but create larger indexes.</li> <li><code>kmeans_niters</code>: the number of iterations to use for the k-means clustering. Larger values will create better centroids but takes longer to create (and default value is often more than enough to converge)</li> <li><code>ndocs</code>: the number of candidate documents to score. Larger values will make querying slower and use more memory, but might results in better results.</li> <li><code>centroid_score_threshold</code>: the threshold scores for centroid pruning. Larger values will consider more candidate centroids and so can find better neighbors but will be slower.</li> <li><code>ncells</code>: the maximum numbers of cells to keep during search. Larger values will consider more centroids and more candidate but will be slower.</li> </ul>"},{"location":"documentation/retrieval/#voyager-index","title":"Voyager index","text":"<p>We also provide an index leveraging the Voyager HNSW index to efficiently handle document embeddings and enable fast retrieval. This was the default index before we implemented PLAID and suggest using PLAID if possible. To use it, simply replace <code>PLAID</code> by <code>Voyager</code>:</p> <pre><code>index = indexes.Voyager(\n    index_folder=\"pylate-index\",\n    index_name=\"index\",\n    override=True,  # This overwrites the existing index if any\n)\n</code></pre>"},{"location":"documentation/retrieval/#parameters-affecting-the-retrieval-performance_1","title":"Parameters affecting the retrieval performance","text":"<p>The retrieval is not an exact search, which mean that certain parameters can affect the quality of the approximate search:</p> <ul> <li><code>M</code>: the maximum number of connections of a node in the graph. Higher values will improve recall and reduce retrieval time but will increase memory usage and the creation time of the index.</li> <li><code>ef_construction</code>: the maximum number of neighbors for a node during the creation of the index. Higher values increase the quality of the index but increase the creation time of the index.</li> <li><code>ef_search</code>: the maximum number of neighbors for a node during the search. Higher values increase the quality of the search but also the search time.</li> </ul> <p>Refer to HNSW documentation for more details.</p> Info <p>Another parameter that significantly influences search quality is k_token. This parameter determines the number of neighbors retrieved for each query token. Higher values of k_token will consider more candidates, leading to better results but at the cost of slower search performance.</p> <pre><code>index = indexes.Voyager(\n    index_folder=\"pylate-index\",\n    index_name=\"index\",\n    override=True,  # This overwrites the existing index if any\n    M=M,\n    ef_construction=ef_construction,\n    ef_search=ef_search,\n)\n\nscores = retriever.retrieve(\n    queries_embeddings=queries_embeddings,\n    k=10,  # Retrieve the top 10 matches for each query\n    k_token=200 # retrieve 200 candidates per query token\n)\n</code></pre>"},{"location":"documentation/retrieval/#colbert-reranking","title":"ColBERT reranking","text":"<p>If you only want to use the ColBERT model to perform reranking on top of your first-stage retrieval pipeline without building an index, you can simply use rank function and pass the queries and documents to rerank:</p> <pre><code>from pylate import rank, models\n\nqueries = [\n    \"query A\",\n    \"query B\",\n]\n\ndocuments = [\n    [\"document A\", \"document B\"],\n    [\"document 1\", \"document C\", \"document B\"],\n]\n\ndocuments_ids = [\n    [1, 2],\n    [1, 3, 2],\n]\n\nmodel = models.ColBERT(\n    model_name_or_path=\"lightonai/GTE-ModernColBERT-v1\",\n)\n\nqueries_embeddings = model.encode(\n    queries,\n    is_query=True,\n)\n\ndocuments_embeddings = model.encode(\n    documents,\n    is_query=False,\n)\n\nreranked_documents = rank.rerank(\n    documents_ids=documents_ids,\n    queries_embeddings=queries_embeddings,\n    documents_embeddings=documents_embeddings,\n)\n</code></pre> <p>Sample output:</p> <pre><code>[\n    [\n        {\"id\": 1, \"score\": 13.866096496582031},\n        {\"id\": 2, \"score\": 7.363473415374756}\n    ],\n    [\n        {\"id\": 2, \"score\": 16.025644302368164},\n        {\"id\": 3, \"score\": 7.144075870513916},\n        {\"id\": 1, \"score\": 4.203659534454346},\n    ],\n]\n</code></pre>"},{"location":"documentation/training/","title":"ColBERT Training","text":"<p>PyLate training is based on Sentence Transformer (and thus transformers) trainer, enabling a lot of functionality such multi-GPU and FP16/BF16 training as well as logging to Weights &amp; Biases out-of-the-box. This allows efficient, and scalable training.</p> Info <p>There are two primary ways to train ColBERT models using PyLate:</p> <ol> <li> <p>Contrastive Loss: Simplest method, it only requires a dataset containing triplets, each consisting of a query, a positive document (relevant to the query), and a negative document (irrelevant to the query). This method trains the model to maximize the similarity between the query and the positive document, while minimizing it with the negative document.</p> </li> <li> <p>Knowledge Distillation: To train a ColBERT model using knowledge distillation, you need to provide a dataset with three components: queries, documents, and the relevance scores between them. This method compresses the knowledge of a larger model / more accurate model (cross-encoder) into a smaller one, using the relevance scores to guide the training process.</p> </li> </ol>"},{"location":"documentation/training/#contrastive-training","title":"Contrastive Training","text":"<p>ColBERT was originally trained using contrastive learning. This approach involves teaching the model to distinguish between relevant (positive) and irrelevant (negative) documents for a given query. The model is trained to maximize the similarity between a query and its corresponding positive document while minimizing the similarity with irrelevant documents.</p> <p>PyLate uses contrastive learning with a triplet dataset, where each query is paired with one positive and one negative example. This makes it fully compatible with any triplet datasets from the sentence-transformers library.</p> <p>During training, the model is optimized to maximize the similarity between the query and its positive example while minimizing the similarity with all negative examples and the positives from other queries in the batch. This approach leverages in-batch negatives for more effective learning.</p> <p>Here is an example of code to run contrastive training with PyLate:</p> <pre><code>import torch\nfrom datasets import load_dataset\nfrom sentence_transformers import (\n    SentenceTransformerTrainer,\n    SentenceTransformerTrainingArguments,\n)\n\nfrom pylate import evaluation, losses, models, utils\n\n# Define model parameters for contrastive training\nmodel_name = \"bert-base-uncased\"  # Choose the pre-trained model you want to use as base\nbatch_size = 32  # Larger batch size often improves results, but requires more memory\n\nnum_train_epochs = 1  # Adjust based on your requirements\n# Set the run name for logging and output directory\nrun_name = \"contrastive-bert-base-uncased\"\noutput_dir = f\"output/{run_name}\"\n\n# 1. Here we define our ColBERT model. If not a ColBERT model, will add a linear layer to the base encoder.\nmodel = models.ColBERT(model_name_or_path=model_name)\n\n# Compiling the model makes the training faster\nmodel = torch.compile(model)\n\n# Load dataset\ndataset = load_dataset(\"sentence-transformers/msmarco-bm25\", \"triplet\", split=\"train\")\n# Split the dataset (this dataset does not have a validation set, so we split the training set)\nsplits = dataset.train_test_split(test_size=0.01)\ntrain_dataset = splits[\"train\"]\neval_dataset = splits[\"test\"]\n\n# Define the loss function\ntrain_loss = losses.Contrastive(model=model)\n\n# Initialize the evaluator\ndev_evaluator = evaluation.ColBERTTripletEvaluator(\n    anchors=eval_dataset[\"query\"],\n    positives=eval_dataset[\"positive\"],\n    negatives=eval_dataset[\"negative\"],\n)\n\n# Configure the training arguments (e.g., batch size, evaluation strategy, logging steps)\nargs = SentenceTransformerTrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=num_train_epochs,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    fp16=True,  # Set to False if you get an error that your GPU can't run on FP16\n    bf16=False,  # Set to True if you have a GPU that supports BF16\n    run_name=run_name,  # Will be used in W&amp;B if `wandb` is installed\n    learning_rate=3e-6,\n)\n\n# Initialize the trainer for the contrastive training\ntrainer = SentenceTransformerTrainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    loss=train_loss,\n    evaluator=dev_evaluator,\n    data_collator=utils.ColBERTCollator(model.tokenize),\n)\n# Start the training process\ntrainer.train()\n</code></pre> Tip <p>Please note that temperature parameter has a very high importance in contrastive learning. A low temperature allows to focus more on the hardest elements in the batch, creating more discriminative representations but is more sensible to false negative. A temperature around 0.02 is often used in the literature: <pre><code>train_loss = losses.Contrastive(model=model, temperature=0.02)\n</code></pre></p> Tip <p>As contrastive learning is not compatible with gradient accumulation, you can leverage GradCache to emulate bigger batch sizes without requiring more memory by using the <code>CachedContrastiveLoss</code> to define a mini_batch_size while increasing the <code>per_device_train_batch_size</code>: <pre><code>train_loss = losses.CachedContrastive(\n        model=model, mini_batch_size=mini_batch_size\n)\n</code></pre></p> Tip <p>Finally, if you are in a multi-GPU setting, you can gather all the elements from the different GPUs to create even bigger batch sizes by setting <code>gather_across_devices</code> to <code>True</code> (for both <code>Contrastive</code> and <code>CachedContrastive</code> losses): <pre><code>train_loss = losses.Contrastive(model=model, gather_across_devices=True)\n</code></pre></p> Tip <p>Please note that for multi-GPU training, running <code>python training.py</code> will use Data Parallel (DP) by default. We strongly suggest using using Distributed Data Parallelism (DDP) using accelerate or torchrun: <code>accelerate launch --num_processes num_gpu training.py</code>.</p> <p>Refer to this documentation for more information.</p>"},{"location":"documentation/training/#knowledge-distillation-training","title":"Knowledge Distillation Training","text":"<p>Training late-interaction models, such as ColBERT, has been shown to benefit from knowledge distillation compared to simpler contrastive learning approaches. Knowledge distillation training focuses on teaching ColBERT models to replicate the outputs of a more capable teacher model (e.g., a cross-encoder). This is achieved using a dataset that includes queries, documents, and the scores assigned by the teacher model to each query/document pair.</p> <p>Below is an example of code to run knowledge distillation training using PyLate:</p> <pre><code>import torch\nfrom datasets import load_dataset\nfrom sentence_transformers import (\n    SentenceTransformerTrainer,\n    SentenceTransformerTrainingArguments,\n)\n\nfrom pylate import losses, models, utils\n\n# Load the datasets required for knowledge distillation (train, queries, documents)\ntrain = load_dataset(\n    path=\"lightonai/ms-marco-en-bge\",\n    name=\"train\",\n)\n\nqueries = load_dataset(\n    path=\"lightonai/ms-marco-en-bge\",\n    name=\"queries\",\n)\n\ndocuments = load_dataset(\n    path=\"lightonai/ms-marco-en-bge\",\n    name=\"documents\",\n)\n\n# Set the transformation to load the documents/queries texts using the corresponding ids on the fly\ntrain.set_transform(\n    utils.KDProcessing(queries=queries, documents=documents).transform,\n)\n\n# Define the base model, training parameters, and output directory\nmodel_name = \"bert-base-uncased\"  # Choose the pre-trained model you want to use as base\nbatch_size = 16\nnum_train_epochs = 1\n# Set the run name for logging and output directory\nrun_name = \"knowledge-distillation-bert-base\"\noutput_dir = f\"output/{run_name}\"\n\n# Initialize the ColBERT model from the base model\nmodel = models.ColBERT(model_name_or_path=model_name)\n\n# Compiling the model to make the training faster\nmodel = torch.compile(model)\n\n# Configure the training arguments (e.g., epochs, batch size, learning rate)\nargs = SentenceTransformerTrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=num_train_epochs,\n    per_device_train_batch_size=batch_size,\n    fp16=True,  # Set to False if you get an error that your GPU can't run on FP16\n    bf16=False,  # Set to True if you have a GPU that supports BF16\n    run_name=run_name,\n    learning_rate=1e-5,\n)\n\n# Use the Distillation loss function for training\ntrain_loss = losses.Distillation(model=model)\n\n# Initialize the trainer\ntrainer = SentenceTransformerTrainer(\n    model=model,\n    args=args,\n    train_dataset=train,\n    loss=train_loss,\n    data_collator=utils.ColBERTCollator(tokenize_fn=model.tokenize),\n)\n\n# Start the training process\ntrainer.train()\n</code></pre> Tip <p>Please note that for multi-GPU training, running <code>python training.py</code> will use Data Parallel (DP) by default. We strongly suggest using using Distributed Data Parallelism (DDP) using accelerate or torchrun: <code>accelerate launch --num_processes num_gpu training.py</code>.</p> <p>Refer to this documentation for more information.</p>"},{"location":"documentation/training/#nanobeir-evaluator","title":"NanoBEIR evaluator","text":"<p>If you are training an English retrieval model, PyLate now features NanoBEIREvaluator, an evaluator that allows to run small versions of the BEIR datasets to get an idea of the performance on BEIR without taking too long to run. <pre><code>evaluator=evaluation.NanoBEIREvaluator(),\n</code></pre> You can select a subset of all the sets to run by adding the dataset names: <code>evaluation.NanoBEIREvaluator(dataset_names=[\"SciFact\"])</code></p>"},{"location":"documentation/training/#colbert-parameters","title":"ColBERT parameters","text":"<p>All the parameters of the ColBERT modeling can be found here. Important parameters to consider are:</p> Info <ul> <li><code>model_name_or_path</code> the name of the base encoder model or PyLate model to init from.</li> <li><code>embedding_size</code> the output size of the projection layer. Large values give more capacity to the model but are heavier to store.</li> <li><code>query_prefix</code> and <code>document_prefix</code> represents the strings that will be prepended to query and document respectively.</li> <li><code>query_length</code> and <code>document_length</code> set the maximum size of queries and documents. Queries will be padded/truncated to the maximum length while documents are only truncated.</li> <li><code>attend_to_expansion_tokens</code> define whether the model will attend to the query expansion tokens (padding of queries) or if only the expansion tokens will attend to the other tokens. In the original ColBERT, the tokens do not attend to expansion tokens.</li> <li><code>skiplist_words</code> is list of words to skip from the documents scoring (note that these tokens are used for encoding and are only skipped during the scoring), the default is the list of string.punctuation as in the original ColBERT.</li> </ul>"},{"location":"documentation/training/#sentence-transformers-training-arguments","title":"Sentence Transformers Training Arguments","text":"<p>PyLate is built on top of SentenceTransformer, so you can use the same arguments you are already familiar with to control the training process. The table below lists the arguments available in the SentenceTransformerTrainingArguments class. For more details, please refer to the SentenceTransformers documentation.</p> Table Parameter Name Definition Training Performance Observing Performance <code>output_dir</code> <code>str</code> The output directory where the model predictions and checkpoints will be written. <code>overwrite_output_dir</code> <code>bool</code>, optional, defaults to <code>False</code> If <code>True</code>, overwrite the content of the output directory. Use this to continue training if <code>output_dir</code> points to a checkpoint directory. <code>do_train</code> <code>bool</code>, optional, defaults to <code>False</code> Whether to run training or not. Intended to be used by your training/evaluation scripts. <code>do_eval</code> <code>bool</code>, optional Whether to run evaluation on the validation set. Will be <code>True</code> if <code>eval_strategy</code> is not <code>\"no\"</code>. Intended to be used by your training/evaluation scripts. <code>do_predict</code> <code>bool</code>, optional, defaults to <code>False</code> Whether to run predictions on the test set or not. Intended to be used by your training/evaluation scripts. <code>eval_strategy</code> <code>str</code> or <code>~trainer_utils.IntervalStrategy</code>, optional, defaults to <code>\"no\"</code> The evaluation strategy to adopt during training. Possible values are <code>\"no\"</code>, <code>\"steps\"</code>, or <code>\"epoch\"</code>. \u2705 <code>prediction_loss_only</code> <code>bool</code>, optional, defaults to <code>False</code> When performing evaluation and generating predictions, only returns the loss. <code>per_device_train_batch_size</code> <code>int</code>, optional, defaults to 8 The batch size per GPU/XPU/TPU/MPS/NPU core/CPU for training. \u2705 <code>per_device_eval_batch_size</code> <code>int</code>, optional, defaults to 8 The batch size per GPU/XPU/TPU/MPS/NPU core/CPU for evaluation. \u2705 <code>gradient_accumulation_steps</code> <code>int</code>, optional, defaults to 1 Number of updates steps to accumulate gradients before performing a backward/update pass. \u2705 <code>eval_accumulation_steps</code> <code>int</code>, optional Number of predictions steps to accumulate the output tensors before moving the results to CPU. \u2705 <code>eval_delay</code> <code>float</code>, optional Number of epochs or steps to wait before the first evaluation depending on <code>eval_strategy</code>. <code>torch_empty_cache_steps</code> <code>int</code>, optional Number of steps to wait before calling <code>torch.&lt;device&gt;.empty_cache()</code> to avoid CUDA out-of-memory errors. <code>learning_rate</code> <code>float</code>, optional, defaults to 5e-5 The initial learning rate for <code>AdamW</code> optimizer. \u2705 <code>num_train_epochs</code> <code>float</code>, optional, defaults to 3.0 Total number of training epochs to perform. \u2705 <code>max_steps</code> <code>int</code>, optional, defaults to -1 If set to a positive number, the total number of training steps to perform. Overrides <code>num_train_epochs</code>. \u2705 <code>lr_scheduler_type</code> <code>str</code> or <code>SchedulerType</code>, optional, defaults to <code>\"linear\"</code> The scheduler type to use. \u2705 <code>lr_scheduler_kwargs</code> <code>dict</code>, optional, defaults to {} Extra arguments for the learning rate scheduler. <code>warmup_ratio</code> <code>float</code>, optional, defaults to 0.0 Ratio of total training steps used for linear warmup from 0 to <code>learning_rate</code>. \u2705 <code>warmup_steps</code> <code>int</code>, optional, defaults to 0 Number of steps used for linear warmup from 0 to <code>learning_rate</code>. Overrides any effect of <code>warmup_ratio</code>. <code>log_level</code> <code>str</code>, optional, defaults to <code>passive</code> Logger log level to use on the main process. \u2705 <code>log_level_replica</code> <code>str</code>, optional, defaults to <code>\"warning\"</code> Logger log level to use on replicas. Same choices as <code>log_level</code>. <code>log_on_each_node</code> <code>bool</code>, optional, defaults to <code>True</code> Whether to log using <code>log_level</code> once per node or only on the main node. <code>logging_dir</code> <code>str</code>, optional TensorBoard log directory. <code>logging_strategy</code> <code>str</code> or <code>~trainer_utils.IntervalStrategy</code>, optional, defaults to <code>\"steps\"</code> The logging strategy to adopt during training. Possible values are <code>\"no\"</code>, <code>\"epoch\"</code>, or <code>\"steps\"</code>. \u2705 <code>logging_first_step</code> <code>bool</code>, optional, defaults to <code>False</code> Whether to log the first <code>global_step</code> or not. <code>logging_steps</code> <code>int</code> or <code>float</code>, optional, defaults to 500 Number of update steps between two logs if <code>logging_strategy=\"steps\"</code>. \u2705 <code>logging_nan_inf_filter</code> <code>bool</code>, optional, defaults to <code>True</code> Whether to filter <code>nan</code> and <code>inf</code> losses for logging. <code>save_strategy</code> <code>str</code> or <code>~trainer_utils.IntervalStrategy</code>, optional, defaults to <code>\"steps\"</code> The checkpoint save strategy to adopt during training. \u2705 <code>save_steps</code> <code>int</code> or <code>float</code>, optional, defaults to 500 Number of update steps before two checkpoint saves if <code>save_strategy=\"steps\"</code>. \u2705 <code>save_total_limit</code> <code>int</code>, optional Limit for total number of checkpoints. \u2705 <code>save_safetensors</code> <code>bool</code>, optional, defaults to <code>True</code> Use safetensors saving and loading for state dicts instead of default <code>torch.load</code> and <code>torch.save</code>. <code>save_on_each_node</code> <code>bool</code>, optional, defaults to <code>False</code> Whether to save models and checkpoints on each node or only on the main one during multi-node distributed training. <code>seed</code> <code>int</code>, optional, defaults to 42 Random seed set at the beginning of training for reproducibility. <code>auto_find_batch_size</code> <code>bool</code>, optional, defaults to <code>False</code> Whether to find a batch size that will fit into memory automatically. \u2705 <code>fp16</code> <code>bool</code>, optional, defaults to <code>False</code> Whether to use fp16 16-bit (mixed) precision training instead of 32-bit training. \u2705 <code>bf16</code> <code>bool</code>, optional, defaults to <code>False</code> Whether to use bf16 16-bit (mixed) precision training instead of 32-bit training. \u2705 <code>push_to_hub</code> <code>bool</code>, optional, defaults to <code>False</code> Whether to push the model to the Hub every time the model is saved. \u2705 <code>hub_model_id</code> <code>str</code>, optional The name of the repository to keep in sync with the local <code>output_dir</code>. \u2705 <code>hub_strategy</code> <code>str</code> or <code>~trainer_utils.HubStrategy</code>, optional, defaults to <code>\"every_save\"</code> Defines the scope of what is pushed to the Hub and when. \u2705 <code>hub_private_repo</code> <code>bool</code>, optional, defaults to <code>False</code> If <code>True</code>, the Hub repo will be set to private. \u2705 <code>load_best_model_at_end</code> <code>bool</code>, optional, defaults to <code>False</code> Whether or not to load the best model found during training at the end of training. \u2705 <code>report_to</code> <code>str</code> or <code>List[str]</code>, optional, defaults to <code>\"all\"</code> The list of integrations to report the results and logs to. \u2705"},{"location":"documentation/training/#sentence-transformer-trainer-arguments","title":"Sentence Transformer Trainer arguments","text":"Table Parameter Name Definition model <code>~sentence_transformers.SentenceTransformer</code>, optional The model to train, evaluate, or use for predictions. If not provided, a <code>model_init</code> must be passed. args <code>~sentence_transformers.training_args.SentenceTransformerTrainingArguments</code>, optional The arguments to tweak for training. Defaults to a basic instance of <code>SentenceTransformerTrainingArguments</code> with the <code>output_dir</code> set to a directory named tmp_trainer in the current directory if not provided. train_dataset <code>datasets.Dataset</code>, <code>datasets.DatasetDict</code>, or <code>Dict[str, datasets.Dataset]</code>, optional The dataset to use for training. Must have a format accepted by your loss function. Refer to <code>Training Overview &gt; Dataset Format</code>. eval_dataset <code>datasets.Dataset</code>, <code>datasets.DatasetDict</code>, or <code>Dict[str, datasets.Dataset]</code>, optional The dataset to use for evaluation. Must have a format accepted by your loss function. Refer to <code>Training Overview &gt; Dataset Format</code>. loss <code>torch.nn.Module</code>, <code>Dict[str, torch.nn.Module]</code>, Callable, or Dict[str, Callable], optional The loss function to use for training. It can be a loss class instance, a dictionary mapping dataset names to loss instances, a function returning a loss instance given a model, or a dictionary mapping dataset names to such functions. Defaults to <code>CoSENTLoss</code> if not provided. evaluator <code>~sentence_transformers.evaluation.SentenceEvaluator</code> or <code>List[~sentence_transformers.evaluation.SentenceEvaluator]</code>, optional The evaluator instance for useful metrics during training. Can be used with or without an <code>eval_dataset</code>. A list of evaluators will be wrapped in a <code>SequentialEvaluator</code> to run sequentially. Generally, evaluator metrics are more useful than loss values from <code>eval_dataset</code>. callbacks <code>List[transformers.TrainerCallback]</code>, optional A list of callbacks to customize the training loop. Adds to the list of default callbacks. To remove a default callback, use the <code>Trainer.remove_callback</code> method. optimizers <code>Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]</code>, optional, defaults to <code>(None, None)</code> A tuple containing the optimizer and scheduler to use. Defaults to an instance of <code>torch.optim.AdamW</code> for the model and a scheduler given by <code>transformers.get_linear_schedule_with_warmup</code>, controlled by <code>args</code>."},{"location":"models/models/","title":"Available models","text":"<p>Tip</p> <p>Following an update, all the models trained using the stanford-nlp ColBERT library or RAGatouille should be compatible with PyLate natively (including their configurations). You can simply load the model in PyLate:</p> <p><pre><code>from pylate import models\n\nmodel = models.ColBERT(\n    model_name_or_path=\"colbert-ir/colbertv2.0\",\n)\n</code></pre> or <pre><code>model = models.ColBERT(\n    model_name_or_path=\"jinaai/jina-colbert-v2\",\n    trust_remote_code=True,\n)\n</code></pre></p> <p>Here is a list of some of the pre-trained ColBERT models available in PyLate along with their results on BEIR:</p> Table Model BEIR AVG NFCorpus SciFact SCIDOCS FiQA2018 TRECCOVID HotpotQA Touche2020 ArguAna ClimateFEVER FEVER QuoraRetrieval NQ DBPedia lightonai/colbertv2.0 50.02 33.8 69.3 15.4 35.6 73.3 66.7 26.3 46.3 17.6 78.5 85.2 56.2 44.6 answerdotai/answerai-colbert-small-v1 53.79 37.3 74.77 18.42 41.15 84.59 76.11 25.69 50.09 33.07 90.96 87.72 59.1 45.58 jinaai/jina-colbert-v2 53.1 34.6 67.8 18.6 40.8 83.4 76.6 27.4 36.6 23.9 80.05 88.7 64.0 47.1 GTE-ModernColBERT-v1 54.89 37.93 76.34 19.06 48.51 83.59 77.32 31.23 48.51 30.62 87.44 86.61 61.8 48.3 Note <p><code>lightonai/colbertv2.0</code> is the original ColBERTv2 model made compatible with PyLate before we supported loading directly model from Stanford-NLP. We thank Omar Khattab for allowing us to share the model on PyLate.</p>"},{"location":"parse/__main__/","title":"main","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"This script is responsible for building the API reference. The API reference is located in\ndocs/api. The script scans through all the modules, classes, and functions. It processes\nthe __doc__ of each object and formats it so that MkDocs can process it in turn.\n\"\"\"\n</pre> \"\"\"This script is responsible for building the API reference. The API reference is located in docs/api. The script scans through all the modules, classes, and functions. It processes the __doc__ of each object and formats it so that MkDocs can process it in turn. \"\"\" In\u00a0[\u00a0]: Copied! <pre>from __future__ import annotations\n</pre> from __future__ import annotations In\u00a0[\u00a0]: Copied! <pre>import functools\nimport importlib\nimport inspect\nimport os\nimport pathlib\nimport re\nimport shutil\n</pre> import functools import importlib import inspect import os import pathlib import re import shutil In\u00a0[\u00a0]: Copied! <pre>from numpydoc.docscrape import ClassDoc, FunctionDoc\n</pre> from numpydoc.docscrape import ClassDoc, FunctionDoc In\u00a0[\u00a0]: Copied! <pre>package = \"pylate\"\n</pre> package = \"pylate\" In\u00a0[\u00a0]: Copied! <pre>shutil.copy(\"README.md\", \"docs/index.md\")\n</pre> shutil.copy(\"README.md\", \"docs/index.md\") In\u00a0[\u00a0]: Copied! <pre>with open(\"docs/index.md\", mode=\"r\") as file:\n    content = file.read()\n</pre> with open(\"docs/index.md\", mode=\"r\") as file:     content = file.read() In\u00a0[\u00a0]: Copied! <pre>with open(\"docs/index.md\", mode=\"w\") as file:\n    file.write(content.replace(\"docs/img/logo.png\", \"img/logo.png\"))\n</pre> with open(\"docs/index.md\", mode=\"w\") as file:     file.write(content.replace(\"docs/img/logo.png\", \"img/logo.png\")) In\u00a0[\u00a0]: Copied! <pre>def paragraph(text):\n    return f\"{text}\\n\"\n</pre> def paragraph(text):     return f\"{text}\\n\" In\u00a0[\u00a0]: Copied! <pre>def h1(text):\n    return paragraph(f\"# {text}\")\n</pre> def h1(text):     return paragraph(f\"# {text}\") In\u00a0[\u00a0]: Copied! <pre>def h2(text):\n    return paragraph(f\"## {text}\")\n</pre> def h2(text):     return paragraph(f\"## {text}\") In\u00a0[\u00a0]: Copied! <pre>def h3(text):\n    return paragraph(f\"### {text}\")\n</pre> def h3(text):     return paragraph(f\"### {text}\") In\u00a0[\u00a0]: Copied! <pre>def h4(text):\n    return paragraph(f\"#### {text}\")\n</pre> def h4(text):     return paragraph(f\"#### {text}\") In\u00a0[\u00a0]: Copied! <pre>def link(caption, href):\n    return f\"[{caption}]({href})\"\n</pre> def link(caption, href):     return f\"[{caption}]({href})\" In\u00a0[\u00a0]: Copied! <pre>def code(text):\n    return f\"`{text}`\"\n</pre> def code(text):     return f\"`{text}`\" In\u00a0[\u00a0]: Copied! <pre>def li(text):\n    return f\"- {text}\\n\"\n</pre> def li(text):     return f\"- {text}\\n\" In\u00a0[\u00a0]: Copied! <pre>def snake_to_kebab(text):\n    return text.replace(\"_\", \"-\")\n</pre> def snake_to_kebab(text):     return text.replace(\"_\", \"-\") In\u00a0[\u00a0]: Copied! <pre>def inherit_docstring(c, meth):\n    \"\"\"Since Python 3.5, inspect.getdoc is supposed to return the docstring from a parent class\n    if a class has none. However this doesn't seem to work for Cython classes.\n    \"\"\"\n\n    doc = None\n\n    for ancestor in inspect.getmro(c):\n        try:\n            ancestor_meth = getattr(ancestor, meth)\n        except AttributeError:\n            break\n        doc = inspect.getdoc(ancestor_meth)\n        if doc:\n            break\n\n    return doc\n</pre> def inherit_docstring(c, meth):     \"\"\"Since Python 3.5, inspect.getdoc is supposed to return the docstring from a parent class     if a class has none. However this doesn't seem to work for Cython classes.     \"\"\"      doc = None      for ancestor in inspect.getmro(c):         try:             ancestor_meth = getattr(ancestor, meth)         except AttributeError:             break         doc = inspect.getdoc(ancestor_meth)         if doc:             break      return doc In\u00a0[\u00a0]: Copied! <pre>def inherit_signature(c, method_name):\n    m = getattr(c, method_name)\n    sig = inspect.signature(m)\n\n    params = []\n\n    for param in sig.parameters.values():\n        if param.name == \"self\" or param.annotation is not param.empty:\n            params.append(param)\n            continue\n\n        for ancestor in inspect.getmro(c):\n            try:\n                ancestor_meth = inspect.signature(getattr(ancestor, m.__name__))\n            except AttributeError:\n                break\n            try:\n                ancestor_param = ancestor_meth.parameters[param.name]\n            except KeyError:\n                break\n            if ancestor_param.annotation is not param.empty:\n                param = param.replace(annotation=ancestor_param.annotation)\n                break\n\n        params.append(param)\n\n    return_annotation = sig.return_annotation\n    if return_annotation is inspect._empty:\n        for ancestor in inspect.getmro(c):\n            try:\n                ancestor_meth = inspect.signature(getattr(ancestor, m.__name__))\n            except AttributeError:\n                break\n            if ancestor_meth.return_annotation is not inspect._empty:\n                return_annotation = ancestor_meth.return_annotation\n                break\n\n    return sig.replace(parameters=params, return_annotation=return_annotation)\n</pre> def inherit_signature(c, method_name):     m = getattr(c, method_name)     sig = inspect.signature(m)      params = []      for param in sig.parameters.values():         if param.name == \"self\" or param.annotation is not param.empty:             params.append(param)             continue          for ancestor in inspect.getmro(c):             try:                 ancestor_meth = inspect.signature(getattr(ancestor, m.__name__))             except AttributeError:                 break             try:                 ancestor_param = ancestor_meth.parameters[param.name]             except KeyError:                 break             if ancestor_param.annotation is not param.empty:                 param = param.replace(annotation=ancestor_param.annotation)                 break          params.append(param)      return_annotation = sig.return_annotation     if return_annotation is inspect._empty:         for ancestor in inspect.getmro(c):             try:                 ancestor_meth = inspect.signature(getattr(ancestor, m.__name__))             except AttributeError:                 break             if ancestor_meth.return_annotation is not inspect._empty:                 return_annotation = ancestor_meth.return_annotation                 break      return sig.replace(parameters=params, return_annotation=return_annotation) In\u00a0[\u00a0]: Copied! <pre>def pascal_to_kebab(string):\n    string = re.sub(\"(.)([A-Z][a-z]+)\", r\"\\1-\\2\", string)\n    string = re.sub(\"(.)([0-9]+)\", r\"\\1-\\2\", string)\n    return re.sub(\"([a-z0-9])([A-Z])\", r\"\\1-\\2\", string).lower()\n</pre> def pascal_to_kebab(string):     string = re.sub(\"(.)([A-Z][a-z]+)\", r\"\\1-\\2\", string)     string = re.sub(\"(.)([0-9]+)\", r\"\\1-\\2\", string)     return re.sub(\"([a-z0-9])([A-Z])\", r\"\\1-\\2\", string).lower() In\u00a0[\u00a0]: Copied! <pre>class Linkifier:\n    def __init__(self):\n        path_index = {}\n        name_index = {}\n\n        modules = {\n            module: importlib.import_module(f\"{package}.{module}\")\n            for module in importlib.import_module(f\"{package}\").__all__\n        }\n\n        def index_module(mod_name, mod, path):\n            path = os.path.join(path, mod_name)\n            dotted_path = path.replace(\"/\", \".\")\n\n            for func_name, func in inspect.getmembers(mod, inspect.isfunction):\n                for e in (\n                    f\"{mod_name}.{func_name}\",\n                    f\"{dotted_path}.{func_name}\",\n                    f\"{func.__module__}.{func_name}\",\n                ):\n                    path_index[e] = os.path.join(path, snake_to_kebab(func_name))\n                    name_index[e] = f\"{dotted_path}.{func_name}\"\n\n            for klass_name, klass in inspect.getmembers(mod, inspect.isclass):\n                for e in (\n                    f\"{mod_name}.{klass_name}\",\n                    f\"{dotted_path}.{klass_name}\",\n                    f\"{klass.__module__}.{klass_name}\",\n                ):\n                    path_index[e] = os.path.join(path, klass_name)\n                    name_index[e] = f\"{dotted_path}.{klass_name}\"\n\n            for submod_name, submod in inspect.getmembers(mod, inspect.ismodule):\n                if submod_name not in mod.__all__ or submod_name == \"typing\":\n                    continue\n                for e in (f\"{mod_name}.{submod_name}\", f\"{dotted_path}.{submod_name}\"):\n                    path_index[e] = os.path.join(path, snake_to_kebab(submod_name))\n\n                # Recurse\n                index_module(submod_name, submod, path=path)\n\n        for mod_name, mod in modules.items():\n            index_module(mod_name, mod, path=\"\")\n\n        # Prepend {package} to each index entry\n        for k in list(path_index.keys()):\n            path_index[f\"{package}.{k}\"] = path_index[k]\n        for k in list(name_index.keys()):\n            name_index[f\"{package}.{k}\"] = name_index[k]\n\n        self.path_index = path_index\n        self.name_index = name_index\n\n    def linkify(self, text, use_fences, depth):\n        path = self.path_index.get(text)\n        name = self.name_index.get(text)\n        if path and name:\n            backwards = \"../\" * (depth + 1)\n            if use_fences:\n                return f\"[`{name}`]({backwards}{path})\"\n            return f\"[{name}]({backwards}{path})\"\n        return None\n\n    def linkify_fences(self, text, depth):\n        between_fences = re.compile(\"`[\\w\\.]+\\.\\w+`\")\n        return between_fences.sub(\n            lambda x: self.linkify(x.group().strip(\"`\"), True, depth) or x.group(), text\n        )\n\n    def linkify_dotted(self, text, depth):\n        dotted = re.compile(\"\\w+\\.[\\.\\w]+\")\n        return dotted.sub(\n            lambda x: self.linkify(x.group(), False, depth) or x.group(), text\n        )\n</pre> class Linkifier:     def __init__(self):         path_index = {}         name_index = {}          modules = {             module: importlib.import_module(f\"{package}.{module}\")             for module in importlib.import_module(f\"{package}\").__all__         }          def index_module(mod_name, mod, path):             path = os.path.join(path, mod_name)             dotted_path = path.replace(\"/\", \".\")              for func_name, func in inspect.getmembers(mod, inspect.isfunction):                 for e in (                     f\"{mod_name}.{func_name}\",                     f\"{dotted_path}.{func_name}\",                     f\"{func.__module__}.{func_name}\",                 ):                     path_index[e] = os.path.join(path, snake_to_kebab(func_name))                     name_index[e] = f\"{dotted_path}.{func_name}\"              for klass_name, klass in inspect.getmembers(mod, inspect.isclass):                 for e in (                     f\"{mod_name}.{klass_name}\",                     f\"{dotted_path}.{klass_name}\",                     f\"{klass.__module__}.{klass_name}\",                 ):                     path_index[e] = os.path.join(path, klass_name)                     name_index[e] = f\"{dotted_path}.{klass_name}\"              for submod_name, submod in inspect.getmembers(mod, inspect.ismodule):                 if submod_name not in mod.__all__ or submod_name == \"typing\":                     continue                 for e in (f\"{mod_name}.{submod_name}\", f\"{dotted_path}.{submod_name}\"):                     path_index[e] = os.path.join(path, snake_to_kebab(submod_name))                  # Recurse                 index_module(submod_name, submod, path=path)          for mod_name, mod in modules.items():             index_module(mod_name, mod, path=\"\")          # Prepend {package} to each index entry         for k in list(path_index.keys()):             path_index[f\"{package}.{k}\"] = path_index[k]         for k in list(name_index.keys()):             name_index[f\"{package}.{k}\"] = name_index[k]          self.path_index = path_index         self.name_index = name_index      def linkify(self, text, use_fences, depth):         path = self.path_index.get(text)         name = self.name_index.get(text)         if path and name:             backwards = \"../\" * (depth + 1)             if use_fences:                 return f\"[`{name}`]({backwards}{path})\"             return f\"[{name}]({backwards}{path})\"         return None      def linkify_fences(self, text, depth):         between_fences = re.compile(\"`[\\w\\.]+\\.\\w+`\")         return between_fences.sub(             lambda x: self.linkify(x.group().strip(\"`\"), True, depth) or x.group(), text         )      def linkify_dotted(self, text, depth):         dotted = re.compile(\"\\w+\\.[\\.\\w]+\")         return dotted.sub(             lambda x: self.linkify(x.group(), False, depth) or x.group(), text         ) In\u00a0[\u00a0]: Copied! <pre>def concat_lines(lines):\n    return inspect.cleandoc(\" \".join(\"\\n\\n\" if line == \"\" else line for line in lines))\n</pre> def concat_lines(lines):     return inspect.cleandoc(\" \".join(\"\\n\\n\" if line == \"\" else line for line in lines)) In\u00a0[\u00a0]: Copied! <pre>def print_docstring(obj, file, depth):\n    \"\"\"Prints a classes's docstring to a file.\"\"\"\n\n    doc = ClassDoc(obj) if inspect.isclass(obj) else FunctionDoc(obj)\n\n    printf = functools.partial(print, file=file)\n\n    printf(h1(obj.__name__))\n    printf(linkifier.linkify_fences(paragraph(concat_lines(doc[\"Summary\"])), depth))\n    printf(\n        linkifier.linkify_fences(\n            paragraph(concat_lines(doc[\"Extended Summary\"])), depth\n        )\n    )\n\n    # We infer the type annotations from the signatures, and therefore rely on the signature\n    # instead of the docstring for documenting parameters\n    try:\n        signature = inspect.signature(obj)\n    except ValueError:\n        signature = (\n            inspect.Signature()\n        )  # TODO: this is necessary for Cython classes, but it's not correct\n    params_desc = {param.name: \" \".join(param.desc) for param in doc[\"Parameters\"]}\n\n    # Parameters\n    if signature.parameters:\n        printf(h2(\"Parameters\"))\n    for param in signature.parameters.values():\n        # Name\n        printf(f\"- **{param.name}**\", end=\"\")\n        # Type annotation\n        if param.annotation is not param.empty:\n            anno = inspect.formatannotation(param.annotation)\n            anno = linkifier.linkify_dotted(anno, depth)\n            printf(f\" (*{anno}*)\", end=\"\")\n        # Default value\n        if param.default is not param.empty:\n            printf(f\" \u2013 defaults to `{param.default}`\", end=\"\")\n        printf(\"\\n\", file=file)\n        # Description\n        if param.name in params_desc:\n            desc = params_desc[param.name]\n            if desc:\n                printf(f\"    {desc}\\n\")\n    printf(\"\")\n\n    # Attributes\n    if doc[\"Attributes\"]:\n        printf(h2(\"Attributes\"))\n    for attr in doc[\"Attributes\"]:\n        # Name\n        printf(f\"- **{attr.name}**\", end=\"\")\n        # Type annotation\n        if attr.type:\n            printf(f\" (*{attr.type}*)\", end=\"\")\n        printf(\"\\n\", file=file)\n        # Description\n        desc = \" \".join(attr.desc)\n        if desc:\n            printf(f\"    {desc}\\n\")\n    printf(\"\")\n\n    # Examples\n    if doc[\"Examples\"]:\n        printf(h2(\"Examples\"))\n\n        in_code = False\n        after_space = False\n\n        for line in inspect.cleandoc(\"\\n\".join(doc[\"Examples\"])).splitlines():\n            if (\n                in_code\n                and after_space\n                and line\n                and not line.startswith(\"&gt;&gt;&gt;\")\n                and not line.startswith(\"...\")\n            ):\n                printf(\"```\\n\")\n                in_code = False\n                after_space = False\n\n            if not in_code and line.startswith(\"&gt;&gt;&gt;\"):\n                printf(\"```python\")\n                in_code = True\n\n            after_space = False\n            if not line:\n                after_space = True\n\n            printf(line)\n\n        if in_code:\n            printf(\"```\")\n    printf(\"\")\n\n    # Methods\n    if inspect.isclass(obj) and doc[\"Methods\"]:\n        printf(h2(\"Methods\"))\n        printf_indent = lambda x, **kwargs: printf(f\"    {x}\", **kwargs)  # noqa: E731\n\n        for meth in doc[\"Methods\"]:\n            printf(paragraph(f'???- note \"{meth.name}\"'))\n\n            # Parse method docstring\n            docstring = inherit_docstring(c=obj, meth=meth.name)\n            if not docstring:\n                continue\n            meth_doc = FunctionDoc(func=None, doc=docstring)\n\n            printf_indent(paragraph(\" \".join(meth_doc[\"Summary\"])))\n            if meth_doc[\"Extended Summary\"]:\n                printf_indent(paragraph(\" \".join(meth_doc[\"Extended Summary\"])))\n\n            # We infer the type annotations from the signatures, and therefore rely on the signature\n            # instead of the docstring for documenting parameters\n            signature = inherit_signature(obj, meth.name)\n            params_desc = {\n                param.name: \" \".join(param.desc) for param in doc[\"Parameters\"]\n            }\n\n            # Parameters\n            if (\n                len(signature.parameters) &gt; 1\n            ):  # signature is never empty, but self doesn't count\n                printf_indent(\"**Parameters**\\n\")\n            for param in signature.parameters.values():\n                if param.name == \"self\":\n                    continue\n                # Name\n                printf_indent(f\"- **{param.name}**\", end=\"\")\n                # Type annotation\n                if param.annotation is not param.empty:\n                    printf_indent(\n                        f\" (*{inspect.formatannotation(param.annotation)}*)\", end=\"\"\n                    )\n                # Default value\n                if param.default is not param.empty:\n                    printf_indent(f\" \u2013 defaults to `{param.default}`\", end=\"\")\n                printf_indent(\"\", file=file)\n                # Description\n                desc = params_desc.get(param.name)\n                if desc:\n                    printf_indent(f\"    {desc}\")\n            printf_indent(\"\")\n\n            # Returns\n            if meth_doc[\"Returns\"]:\n                printf_indent(\"**Returns**\\n\")\n                return_val = meth_doc[\"Returns\"][0]\n                if signature.return_annotation is not inspect._empty:\n                    if inspect.isclass(signature.return_annotation):\n                        printf_indent(\n                            f\"*{signature.return_annotation.__name__}*: \", end=\"\"\n                        )\n                    else:\n                        printf_indent(f\"*{signature.return_annotation}*: \", end=\"\")\n                printf_indent(return_val.type)\n                printf_indent(\"\")\n\n    # Notes\n    if doc[\"Notes\"]:\n        printf(h2(\"Notes\"))\n        printf(paragraph(\"\\n\".join(doc[\"Notes\"])))\n\n    # References\n    if doc[\"References\"]:\n        printf(h2(\"References\"))\n        printf(paragraph(\"\\n\".join(doc[\"References\"])))\n</pre> def print_docstring(obj, file, depth):     \"\"\"Prints a classes's docstring to a file.\"\"\"      doc = ClassDoc(obj) if inspect.isclass(obj) else FunctionDoc(obj)      printf = functools.partial(print, file=file)      printf(h1(obj.__name__))     printf(linkifier.linkify_fences(paragraph(concat_lines(doc[\"Summary\"])), depth))     printf(         linkifier.linkify_fences(             paragraph(concat_lines(doc[\"Extended Summary\"])), depth         )     )      # We infer the type annotations from the signatures, and therefore rely on the signature     # instead of the docstring for documenting parameters     try:         signature = inspect.signature(obj)     except ValueError:         signature = (             inspect.Signature()         )  # TODO: this is necessary for Cython classes, but it's not correct     params_desc = {param.name: \" \".join(param.desc) for param in doc[\"Parameters\"]}      # Parameters     if signature.parameters:         printf(h2(\"Parameters\"))     for param in signature.parameters.values():         # Name         printf(f\"- **{param.name}**\", end=\"\")         # Type annotation         if param.annotation is not param.empty:             anno = inspect.formatannotation(param.annotation)             anno = linkifier.linkify_dotted(anno, depth)             printf(f\" (*{anno}*)\", end=\"\")         # Default value         if param.default is not param.empty:             printf(f\" \u2013 defaults to `{param.default}`\", end=\"\")         printf(\"\\n\", file=file)         # Description         if param.name in params_desc:             desc = params_desc[param.name]             if desc:                 printf(f\"    {desc}\\n\")     printf(\"\")      # Attributes     if doc[\"Attributes\"]:         printf(h2(\"Attributes\"))     for attr in doc[\"Attributes\"]:         # Name         printf(f\"- **{attr.name}**\", end=\"\")         # Type annotation         if attr.type:             printf(f\" (*{attr.type}*)\", end=\"\")         printf(\"\\n\", file=file)         # Description         desc = \" \".join(attr.desc)         if desc:             printf(f\"    {desc}\\n\")     printf(\"\")      # Examples     if doc[\"Examples\"]:         printf(h2(\"Examples\"))          in_code = False         after_space = False          for line in inspect.cleandoc(\"\\n\".join(doc[\"Examples\"])).splitlines():             if (                 in_code                 and after_space                 and line                 and not line.startswith(\"&gt;&gt;&gt;\")                 and not line.startswith(\"...\")             ):                 printf(\"```\\n\")                 in_code = False                 after_space = False              if not in_code and line.startswith(\"&gt;&gt;&gt;\"):                 printf(\"```python\")                 in_code = True              after_space = False             if not line:                 after_space = True              printf(line)          if in_code:             printf(\"```\")     printf(\"\")      # Methods     if inspect.isclass(obj) and doc[\"Methods\"]:         printf(h2(\"Methods\"))         printf_indent = lambda x, **kwargs: printf(f\"    {x}\", **kwargs)  # noqa: E731          for meth in doc[\"Methods\"]:             printf(paragraph(f'???- note \"{meth.name}\"'))              # Parse method docstring             docstring = inherit_docstring(c=obj, meth=meth.name)             if not docstring:                 continue             meth_doc = FunctionDoc(func=None, doc=docstring)              printf_indent(paragraph(\" \".join(meth_doc[\"Summary\"])))             if meth_doc[\"Extended Summary\"]:                 printf_indent(paragraph(\" \".join(meth_doc[\"Extended Summary\"])))              # We infer the type annotations from the signatures, and therefore rely on the signature             # instead of the docstring for documenting parameters             signature = inherit_signature(obj, meth.name)             params_desc = {                 param.name: \" \".join(param.desc) for param in doc[\"Parameters\"]             }              # Parameters             if (                 len(signature.parameters) &gt; 1             ):  # signature is never empty, but self doesn't count                 printf_indent(\"**Parameters**\\n\")             for param in signature.parameters.values():                 if param.name == \"self\":                     continue                 # Name                 printf_indent(f\"- **{param.name}**\", end=\"\")                 # Type annotation                 if param.annotation is not param.empty:                     printf_indent(                         f\" (*{inspect.formatannotation(param.annotation)}*)\", end=\"\"                     )                 # Default value                 if param.default is not param.empty:                     printf_indent(f\" \u2013 defaults to `{param.default}`\", end=\"\")                 printf_indent(\"\", file=file)                 # Description                 desc = params_desc.get(param.name)                 if desc:                     printf_indent(f\"    {desc}\")             printf_indent(\"\")              # Returns             if meth_doc[\"Returns\"]:                 printf_indent(\"**Returns**\\n\")                 return_val = meth_doc[\"Returns\"][0]                 if signature.return_annotation is not inspect._empty:                     if inspect.isclass(signature.return_annotation):                         printf_indent(                             f\"*{signature.return_annotation.__name__}*: \", end=\"\"                         )                     else:                         printf_indent(f\"*{signature.return_annotation}*: \", end=\"\")                 printf_indent(return_val.type)                 printf_indent(\"\")      # Notes     if doc[\"Notes\"]:         printf(h2(\"Notes\"))         printf(paragraph(\"\\n\".join(doc[\"Notes\"])))      # References     if doc[\"References\"]:         printf(h2(\"References\"))         printf(paragraph(\"\\n\".join(doc[\"References\"]))) In\u00a0[\u00a0]: Copied! <pre>def print_module(mod, path, overview, is_submodule=False):\n    mod_name = mod.__name__.split(\".\")[-1]\n\n    # Create a directory for the module\n    mod_slug = snake_to_kebab(mod_name)\n    mod_path = path.joinpath(mod_slug)\n    mod_short_path = str(mod_path).replace(\"docs/api/\", \"\")\n    os.makedirs(mod_path, exist_ok=True)\n    with open(mod_path.joinpath(\".pages\"), \"w\") as f:\n        f.write(f\"title: {mod_name}\")\n\n    # Add the module to the overview\n    if is_submodule:\n        print(h3(mod_name), file=overview)\n    else:\n        print(h2(mod_name), file=overview)\n    if mod.__doc__:\n        print(paragraph(mod.__doc__), file=overview)\n\n    # Extract all public classes and functions\n    ispublic = lambda x: x.__name__ in mod.__all__ and not x.__name__.startswith(\"_\")  # noqa: E731\n    classes = inspect.getmembers(mod, lambda x: inspect.isclass(x) and ispublic(x))\n    funcs = inspect.getmembers(mod, lambda x: inspect.isfunction(x) and ispublic(x))\n\n    # Classes\n\n    if classes and funcs:\n        print(\"\\n**Classes**\\n\", file=overview)\n\n    for _, c in classes:\n        print(f\"{mod_name}.{c.__name__}\")\n\n        # Add the class to the overview\n        slug = snake_to_kebab(c.__name__)\n        print(\n            li(link(c.__name__, f\"../{mod_short_path}/{slug}\")), end=\"\", file=overview\n        )\n\n        # Write down the class' docstring\n        with open(mod_path.joinpath(slug).with_suffix(\".md\"), \"w\") as file:\n            print_docstring(obj=c, file=file, depth=mod_short_path.count(\"/\") + 1)\n\n    # Functions\n\n    if classes and funcs:\n        print(\"\\n**Functions**\\n\", file=overview)\n\n    for _, f in funcs:\n        print(f\"{mod_name}.{f.__name__}\")\n\n        # Add the function to the overview\n        slug = snake_to_kebab(f.__name__)\n        print(\n            li(link(f.__name__, f\"../{mod_short_path}/{slug}\")), end=\"\", file=overview\n        )\n\n        # Write down the function' docstring\n        with open(mod_path.joinpath(slug).with_suffix(\".md\"), \"w\") as file:\n            print_docstring(obj=f, file=file, depth=mod_short_path.count(\".\") + 1)\n\n    # Sub-modules\n    for name, submod in inspect.getmembers(mod, inspect.ismodule):\n        # We only want to go through the public submodules, such as optim.schedulers\n        if (\n            name in (\"tags\", \"typing\", \"inspect\", \"skmultiflow_utils\")\n            or name not in mod.__all__\n            or name.startswith(\"_\")\n        ):\n            continue\n        print_module(mod=submod, path=mod_path, overview=overview, is_submodule=True)\n\n    print(\"\", file=overview)\n</pre> def print_module(mod, path, overview, is_submodule=False):     mod_name = mod.__name__.split(\".\")[-1]      # Create a directory for the module     mod_slug = snake_to_kebab(mod_name)     mod_path = path.joinpath(mod_slug)     mod_short_path = str(mod_path).replace(\"docs/api/\", \"\")     os.makedirs(mod_path, exist_ok=True)     with open(mod_path.joinpath(\".pages\"), \"w\") as f:         f.write(f\"title: {mod_name}\")      # Add the module to the overview     if is_submodule:         print(h3(mod_name), file=overview)     else:         print(h2(mod_name), file=overview)     if mod.__doc__:         print(paragraph(mod.__doc__), file=overview)      # Extract all public classes and functions     ispublic = lambda x: x.__name__ in mod.__all__ and not x.__name__.startswith(\"_\")  # noqa: E731     classes = inspect.getmembers(mod, lambda x: inspect.isclass(x) and ispublic(x))     funcs = inspect.getmembers(mod, lambda x: inspect.isfunction(x) and ispublic(x))      # Classes      if classes and funcs:         print(\"\\n**Classes**\\n\", file=overview)      for _, c in classes:         print(f\"{mod_name}.{c.__name__}\")          # Add the class to the overview         slug = snake_to_kebab(c.__name__)         print(             li(link(c.__name__, f\"../{mod_short_path}/{slug}\")), end=\"\", file=overview         )          # Write down the class' docstring         with open(mod_path.joinpath(slug).with_suffix(\".md\"), \"w\") as file:             print_docstring(obj=c, file=file, depth=mod_short_path.count(\"/\") + 1)      # Functions      if classes and funcs:         print(\"\\n**Functions**\\n\", file=overview)      for _, f in funcs:         print(f\"{mod_name}.{f.__name__}\")          # Add the function to the overview         slug = snake_to_kebab(f.__name__)         print(             li(link(f.__name__, f\"../{mod_short_path}/{slug}\")), end=\"\", file=overview         )          # Write down the function' docstring         with open(mod_path.joinpath(slug).with_suffix(\".md\"), \"w\") as file:             print_docstring(obj=f, file=file, depth=mod_short_path.count(\".\") + 1)      # Sub-modules     for name, submod in inspect.getmembers(mod, inspect.ismodule):         # We only want to go through the public submodules, such as optim.schedulers         if (             name in (\"tags\", \"typing\", \"inspect\", \"skmultiflow_utils\")             or name not in mod.__all__             or name.startswith(\"_\")         ):             continue         print_module(mod=submod, path=mod_path, overview=overview, is_submodule=True)      print(\"\", file=overview) In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    api_path = pathlib.Path(\"docs/api\")\n\n    # Create a directory for the API reference\n    shutil.rmtree(api_path, ignore_errors=True)\n    os.makedirs(api_path, exist_ok=True)\n    with open(api_path.joinpath(\".pages\"), \"w\") as f:\n        f.write(\"title: API reference\\narrange:\\n  - overview.md\\n  - ...\\n\")\n\n    overview = open(api_path.joinpath(\"overview.md\"), \"w\")\n    print(h1(\"Overview\"), file=overview)\n\n    linkifier = Linkifier()\n\n    for mod_name, mod in inspect.getmembers(\n        importlib.import_module(f\"{package}\"), inspect.ismodule\n    ):\n        if mod_name.startswith(\"_\"):\n            continue\n        print(mod_name)\n        print_module(mod, path=api_path, overview=overview)\n</pre> if __name__ == \"__main__\":     api_path = pathlib.Path(\"docs/api\")      # Create a directory for the API reference     shutil.rmtree(api_path, ignore_errors=True)     os.makedirs(api_path, exist_ok=True)     with open(api_path.joinpath(\".pages\"), \"w\") as f:         f.write(\"title: API reference\\narrange:\\n  - overview.md\\n  - ...\\n\")      overview = open(api_path.joinpath(\"overview.md\"), \"w\")     print(h1(\"Overview\"), file=overview)      linkifier = Linkifier()      for mod_name, mod in inspect.getmembers(         importlib.import_module(f\"{package}\"), inspect.ismodule     ):         if mod_name.startswith(\"_\"):             continue         print(mod_name)         print_module(mod, path=api_path, overview=overview)"}]}