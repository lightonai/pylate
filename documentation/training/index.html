
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Neural Search">
      
      
        <meta name="author" content="Raphael Sourty">
      
      
        <link rel="canonical" href="https://lightonai.github.io/pylate/documentation/training/">
      
      
      
        <link rel="next" href="../datasets/">
      
      
      <link rel="icon" href="../../img/favicon.ico">
      <meta name="generator" content="mkdocs-1.6.0, mkdocs-material-9.5.32">
    
    
      
        <title>Training - pylate</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.3cba04c6.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Fira+Sans:300,300i,400,400i,700,700i%7CFira+Code:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Fira Sans";--md-code-font:"Fira Code"}</style>
      
    
    
      <link rel="stylesheet" href="../../stylesheets/extra.css">
    
      <link rel="stylesheet" href="../../css/version-select.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="blue" data-md-color-accent="blue">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#colbert-training" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="pylate" class="md-header__button md-logo" aria-label="pylate" data-md-component="logo">
      
  <img src="../../img/favicon.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            pylate
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Training
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="blue" data-md-color-accent="blue"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="blue" data-md-color-accent="blue"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/lightonai/pylate" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    lighton/pylate
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="./" class="md-tabs__link">
          
  
    
  
  Documentation

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../models/models/" class="md-tabs__link">
          
  
    
  
  Models

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../api/overview/" class="md-tabs__link">
          
  
    
  
  API reference

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="pylate" class="md-nav__button md-logo" aria-label="pylate" data-md-component="logo">
      
  <img src="../../img/favicon.png" alt="logo">

    </a>
    pylate
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/lightonai/pylate" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    lighton/pylate
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
    
    
      
        
        
      
      
    
    
      
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1" checked>
        
          
          <label class="md-nav__link" for="__nav_1" id="__nav_1_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Documentation
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_1_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_1">
            <span class="md-nav__icon md-icon"></span>
            Documentation
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Training
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Training
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#contrastive-training" class="md-nav__link">
    <span class="md-ellipsis">
      Contrastive Training
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#knowledge-distillation-training" class="md-nav__link">
    <span class="md-ellipsis">
      Knowledge Distillation Training
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#colbert-parameters" class="md-nav__link">
    <span class="md-ellipsis">
      ColBERT parameters
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sentence-transformers-training-arguments" class="md-nav__link">
    <span class="md-ellipsis">
      Sentence Transformers Training Arguments
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sentence-transformer-trainer-arguments" class="md-nav__link">
    <span class="md-ellipsis">
      Sentence Transformer Trainer arguments
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../datasets/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Datasets
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../retrieval/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Retrieval
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../evaluation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Evaluation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../fastapi/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    FastAPI
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
    
    
      
      
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../models/models/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    Models
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../api/overview/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    API reference
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#contrastive-training" class="md-nav__link">
    <span class="md-ellipsis">
      Contrastive Training
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#knowledge-distillation-training" class="md-nav__link">
    <span class="md-ellipsis">
      Knowledge Distillation Training
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#colbert-parameters" class="md-nav__link">
    <span class="md-ellipsis">
      ColBERT parameters
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sentence-transformers-training-arguments" class="md-nav__link">
    <span class="md-ellipsis">
      Sentence Transformers Training Arguments
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sentence-transformer-trainer-arguments" class="md-nav__link">
    <span class="md-ellipsis">
      Sentence Transformer Trainer arguments
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="colbert-training">ColBERT Training<a class="headerlink" href="#colbert-training" title="Permanent link">&para;</a></h1>
<p>PyLate training is based on Sentence Transformer (and thus transformers) trainer, enabling a lot of functionality such multi-GPU and FP16/BF16 training as well as logging to Weights &amp; Biases out-of-the-box. This allows efficient, and scalable training. </p>
<details class="info" open="open">
<summary>Info</summary>
<p>There are two primary ways to train ColBERT models using PyLate:</p>
<ol>
<li>
<p><strong>Contrastive Loss</strong>: Simplest method, it only requires a dataset containing triplets, each consisting of a query, a positive document (relevant to the query), and a negative document (irrelevant to the query). This method trains the model to maximize the similarity between the query and the positive document, while minimizing it with the negative document.</p>
</li>
<li>
<p><strong>Knowledge Distillation</strong>: To train a ColBERT model using knowledge distillation, you need to provide a dataset with three components: queries, documents, and the relevance scores between them. This method compresses the knowledge of a larger model / more accurate model (cross-encoder) into a smaller one, using the relevance scores to guide the training process.</p>
</li>
</ol>
</details>
<h2 id="contrastive-training">Contrastive Training<a class="headerlink" href="#contrastive-training" title="Permanent link">&para;</a></h2>
<p>ColBERT was originally trained using contrastive learning. This approach involves teaching the model to distinguish between relevant (positive) and irrelevant (negative) documents for a given query. The model is trained to maximize the similarity between a query and its corresponding positive document while minimizing the similarity with irrelevant documents.</p>
<p>PyLate uses contrastive learning with a triplet dataset, where each query is paired with one positive and one negative example. <strong>This makes it fully compatible with any triplet datasets from the sentence-transformers library</strong>.</p>
<p>During training, the model is optimized to maximize the similarity between the query and its positive example while minimizing the similarity with all negative examples and the positives from other queries in the batch. This approach leverages in-batch negatives for more effective learning.</p>
<p>Here is an example of code to run contrastive training with PyLate:</p>
<div class="language-python highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>
<span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">SentenceTransformerTrainer</span><span class="p">,</span>
    <span class="n">SentenceTransformerTrainingArguments</span><span class="p">,</span>
<span class="p">)</span>

<span class="kn">from</span> <span class="nn">pylate</span> <span class="kn">import</span> <span class="n">evaluation</span><span class="p">,</span> <span class="n">losses</span><span class="p">,</span> <span class="n">models</span><span class="p">,</span> <span class="n">utils</span>

<span class="c1"># Define model parameters for contrastive training</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;bert-base-uncased&quot;</span>  <span class="c1"># Choose the pre-trained model you want to use as base</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>  <span class="c1"># Larger batch size often improves results, but requires more memory</span>

<span class="n">num_train_epochs</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># Adjust based on your requirements</span>
<span class="c1"># Set the run name for logging and output directory</span>
<span class="n">run_name</span> <span class="o">=</span> <span class="s2">&quot;contrastive-bert-base-uncased&quot;</span>
<span class="n">output_dir</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;output/</span><span class="si">{</span><span class="n">run_name</span><span class="si">}</span><span class="s2">&quot;</span>

<span class="c1"># 1. Here we define our ColBERT model. If not a ColBERT model, will add a linear layer to the base encoder.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">ColBERT</span><span class="p">(</span><span class="n">model_name_or_path</span><span class="o">=</span><span class="n">model_name</span><span class="p">)</span>

<span class="c1"># Compiling the model makes the training faster</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="c1"># Load dataset</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;sentence-transformers/msmarco-bm25&quot;</span><span class="p">,</span> <span class="s2">&quot;triplet&quot;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
<span class="c1"># Split the dataset (this dataset does not have a validation set, so we split the training set)</span>
<span class="n">splits</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">train_test_split</span><span class="p">(</span><span class="n">test_size</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">splits</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">]</span>
<span class="n">eval_dataset</span> <span class="o">=</span> <span class="n">splits</span><span class="p">[</span><span class="s2">&quot;test&quot;</span><span class="p">]</span>

<span class="c1"># Define the loss function</span>
<span class="n">train_loss</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">Contrastive</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>

<span class="c1"># Initialize the evaluator</span>
<span class="n">dev_evaluator</span> <span class="o">=</span> <span class="n">evaluation</span><span class="o">.</span><span class="n">ColBERTTripletEvaluator</span><span class="p">(</span>
    <span class="n">anchors</span><span class="o">=</span><span class="n">eval_dataset</span><span class="p">[</span><span class="s2">&quot;query&quot;</span><span class="p">],</span>
    <span class="n">positives</span><span class="o">=</span><span class="n">eval_dataset</span><span class="p">[</span><span class="s2">&quot;positive&quot;</span><span class="p">],</span>
    <span class="n">negatives</span><span class="o">=</span><span class="n">eval_dataset</span><span class="p">[</span><span class="s2">&quot;negative&quot;</span><span class="p">],</span>
<span class="p">)</span>

<span class="c1"># Configure the training arguments (e.g., batch size, evaluation strategy, logging steps)</span>
<span class="n">args</span> <span class="o">=</span> <span class="n">SentenceTransformerTrainingArguments</span><span class="p">(</span>
    <span class="n">output_dir</span><span class="o">=</span><span class="n">output_dir</span><span class="p">,</span>
    <span class="n">num_train_epochs</span><span class="o">=</span><span class="n">num_train_epochs</span><span class="p">,</span>
    <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
    <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
    <span class="n">fp16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># Set to False if you get an error that your GPU can&#39;t run on FP16</span>
    <span class="n">bf16</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>  <span class="c1"># Set to True if you have a GPU that supports BF16</span>
    <span class="n">run_name</span><span class="o">=</span><span class="n">run_name</span><span class="p">,</span>  <span class="c1"># Will be used in W&amp;B if `wandb` is installed</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">3e-6</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Initialize the trainer for the contrastive training</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">SentenceTransformerTrainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span>
    <span class="n">eval_dataset</span><span class="o">=</span><span class="n">eval_dataset</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">train_loss</span><span class="p">,</span>
    <span class="n">evaluator</span><span class="o">=</span><span class="n">dev_evaluator</span><span class="p">,</span>
    <span class="n">data_collator</span><span class="o">=</span><span class="n">utils</span><span class="o">.</span><span class="n">ColBERTCollator</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">tokenize</span><span class="p">),</span>
<span class="p">)</span>
<span class="c1"># Start the training process</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</code></pre></div>
<details class="tip" open="open">
<summary>Tip</summary>
<p>Please note that for multi-GPU training, running <code>python training.py</code> <strong>will use Data Parallel (DP) by default</strong>. We strongly suggest using using Distributed Data Parallelism (DDP) using accelerate or torchrun: <code>accelerate launch --num_processes num_gpu training.py</code>.</p>
<p>Refer to this <a href="https://sbert.net/docs/sentence_transformer/training/distributed.html">documentation</a> for more information.</p>
</details>
<h2 id="knowledge-distillation-training">Knowledge Distillation Training<a class="headerlink" href="#knowledge-distillation-training" title="Permanent link">&para;</a></h2>
<p>Training late-interaction models, such as ColBERT, has been shown to benefit from knowledge distillation compared to simpler contrastive learning approaches. Knowledge distillation training focuses on teaching ColBERT models to replicate the outputs of a more capable teacher model (e.g., a cross-encoder). This is achieved using a dataset that includes queries, documents, and the scores assigned by the teacher model to each query/document pair.</p>
<p>Below is an example of code to run knowledge distillation training using PyLate:</p>
<div class="language-python highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>
<span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">SentenceTransformerTrainer</span><span class="p">,</span>
    <span class="n">SentenceTransformerTrainingArguments</span><span class="p">,</span>
<span class="p">)</span>

<span class="kn">from</span> <span class="nn">pylate</span> <span class="kn">import</span> <span class="n">losses</span><span class="p">,</span> <span class="n">models</span><span class="p">,</span> <span class="n">utils</span>

<span class="c1"># Load the datasets required for knowledge distillation (train, queries, documents)</span>
<span class="n">train</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span>
    <span class="n">path</span><span class="o">=</span><span class="s2">&quot;lightonai/ms-marco-en-bge&quot;</span><span class="p">,</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">queries</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span>
    <span class="n">path</span><span class="o">=</span><span class="s2">&quot;lightonai/ms-marco-en-bge&quot;</span><span class="p">,</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;queries&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">documents</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span>
    <span class="n">path</span><span class="o">=</span><span class="s2">&quot;lightonai/ms-marco-en-bge&quot;</span><span class="p">,</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;documents&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Set the transformation to load the documents/queries texts using the corresponding ids on the fly</span>
<span class="n">train</span><span class="o">.</span><span class="n">set_transform</span><span class="p">(</span>
    <span class="n">utils</span><span class="o">.</span><span class="n">KDProcessing</span><span class="p">(</span><span class="n">queries</span><span class="o">=</span><span class="n">queries</span><span class="p">,</span> <span class="n">documents</span><span class="o">=</span><span class="n">documents</span><span class="p">)</span><span class="o">.</span><span class="n">transform</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Define the base model, training parameters, and output directory</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;bert-base-uncased&quot;</span>  <span class="c1"># Choose the pre-trained model you want to use as base</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">num_train_epochs</span> <span class="o">=</span> <span class="mi">1</span>
<span class="c1"># Set the run name for logging and output directory</span>
<span class="n">run_name</span> <span class="o">=</span> <span class="s2">&quot;knowledge-distillation-bert-base&quot;</span>
<span class="n">output_dir</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;output/</span><span class="si">{</span><span class="n">run_name</span><span class="si">}</span><span class="s2">&quot;</span>

<span class="c1"># Initialize the ColBERT model from the base model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">ColBERT</span><span class="p">(</span><span class="n">model_name_or_path</span><span class="o">=</span><span class="n">model_name</span><span class="p">)</span>

<span class="c1"># Compiling the model to make the training faster</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="c1"># Configure the training arguments (e.g., epochs, batch size, learning rate)</span>
<span class="n">args</span> <span class="o">=</span> <span class="n">SentenceTransformerTrainingArguments</span><span class="p">(</span>
    <span class="n">output_dir</span><span class="o">=</span><span class="n">output_dir</span><span class="p">,</span>
    <span class="n">num_train_epochs</span><span class="o">=</span><span class="n">num_train_epochs</span><span class="p">,</span>
    <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
    <span class="n">fp16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># Set to False if you get an error that your GPU can&#39;t run on FP16</span>
    <span class="n">bf16</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>  <span class="c1"># Set to True if you have a GPU that supports BF16</span>
    <span class="n">run_name</span><span class="o">=</span><span class="n">run_name</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Use the Distillation loss function for training</span>
<span class="n">train_loss</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">Distillation</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>

<span class="c1"># Initialize the trainer</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">SentenceTransformerTrainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">train</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">train_loss</span><span class="p">,</span>
    <span class="n">data_collator</span><span class="o">=</span><span class="n">utils</span><span class="o">.</span><span class="n">ColBERTCollator</span><span class="p">(</span><span class="n">tokenize_fn</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">tokenize</span><span class="p">),</span>
<span class="p">)</span>

<span class="c1"># Start the training process</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</code></pre></div>
<details class="tip" open="open">
<summary>Tip</summary>
<p>Please note that for multi-GPU training, running <code>python training.py</code> <strong>will use Data Parallel (DP) by default</strong>. We strongly suggest using using Distributed Data Parallelism (DDP) using accelerate or torchrun: <code>accelerate launch --num_processes num_gpu training.py</code>.</p>
<p>Refer to this <a href="https://sbert.net/docs/sentence_transformer/training/distributed.html">documentation</a> for more information.</p>
</details>
<h2 id="colbert-parameters">ColBERT parameters<a class="headerlink" href="#colbert-parameters" title="Permanent link">&para;</a></h2>
<p>All the parameters of the ColBERT modeling can be found <a href="https://lightonai.github.io/pylate/api/models/ColBERT/#parameters">here</a>. Important parameters to consider are:</p>
<details class="info" open="open">
<summary>Info</summary>
<ul>
<li><code>model_name_or_path</code> the name of the base encoder model or PyLate model to init from.</li>
<li><code>embedding_size</code> the output size of the projection layer. Large values give more capacity to the model but are heavier to store.</li>
<li><code>query_prefix</code> and <code>document_prefix</code> represents the strings that will be prepended to query and document respectively.</li>
<li><code>query_length</code> and <code>document_length</code> set the maximum size of queries and documents. Queries will be padded/truncated to the maximum length while documents are only truncated.</li>
<li><code>attend_to_expansion_tokens</code> define whether the model will attend to the query expansion tokens (padding of queries) or if only the expansion tokens will attend to the other tokens. In the original ColBERT, the tokens <strong>do not attend</strong> to expansion tokens.</li>
<li><code>skiplist_words</code> is list of words to skip from the documents scoring (note that these tokens are used for encoding and are only skipped during the scoring), the default is the list of string.punctuation as in the original ColBERT.</li>
</ul>
</details>
<h2 id="sentence-transformers-training-arguments">Sentence Transformers Training Arguments<a class="headerlink" href="#sentence-transformers-training-arguments" title="Permanent link">&para;</a></h2>
<p>PyLate is built on top of SentenceTransformer, so you can use the same arguments you are already familiar with to control the training process. The table below lists the arguments available in the SentenceTransformerTrainingArguments class. For more details, please refer to the <a href="https://sbert.net/docs/sentence_transformer/training_overview.html#">SentenceTransformers documentation</a>.</p>
<div class="tabbed-set tabbed-alternate" data-tabs="1:1"><input checked="checked" id="__tabbed_1_1" name="__tabbed_1" type="radio" /><div class="tabbed-labels"><label for="__tabbed_1_1">Table</label></div>
<div class="tabbed-content">
<div class="tabbed-block"></div>
</div>
</div>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Name</th>
<th>Definition</th>
<th>Training Performance</th>
<th>Observing Performance</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>output_dir</code></td>
<td><code>str</code></td>
<td>The output directory where the model predictions and checkpoints will be written.</td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>overwrite_output_dir</code></td>
<td><code>bool</code>, <em>optional</em>, defaults to <code>False</code></td>
<td>If <code>True</code>, overwrite the content of the output directory. Use this to continue training if <code>output_dir</code> points to a checkpoint directory.</td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>do_train</code></td>
<td><code>bool</code>, <em>optional</em>, defaults to <code>False</code></td>
<td>Whether to run training or not. Intended to be used by your training/evaluation scripts.</td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>do_eval</code></td>
<td><code>bool</code>, <em>optional</em></td>
<td>Whether to run evaluation on the validation set. Will be <code>True</code> if <code>eval_strategy</code> is not <code>"no"</code>. Intended to be used by your training/evaluation scripts.</td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>do_predict</code></td>
<td><code>bool</code>, <em>optional</em>, defaults to <code>False</code></td>
<td>Whether to run predictions on the test set or not. Intended to be used by your training/evaluation scripts.</td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>eval_strategy</code></td>
<td><code>str</code> or <code>~trainer_utils.IntervalStrategy</code>, <em>optional</em>, defaults to <code>"no"</code></td>
<td>The evaluation strategy to adopt during training. Possible values are <code>"no"</code>, <code>"steps"</code>, or <code>"epoch"</code>.</td>
<td></td>
<td>✅</td>
</tr>
<tr>
<td><code>prediction_loss_only</code></td>
<td><code>bool</code>, <em>optional</em>, defaults to <code>False</code></td>
<td>When performing evaluation and generating predictions, only returns the loss.</td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>per_device_train_batch_size</code></td>
<td><code>int</code>, <em>optional</em>, defaults to 8</td>
<td>The batch size per GPU/XPU/TPU/MPS/NPU core/CPU for training.</td>
<td>✅</td>
<td></td>
</tr>
<tr>
<td><code>per_device_eval_batch_size</code></td>
<td><code>int</code>, <em>optional</em>, defaults to 8</td>
<td>The batch size per GPU/XPU/TPU/MPS/NPU core/CPU for evaluation.</td>
<td>✅</td>
<td></td>
</tr>
<tr>
<td><code>gradient_accumulation_steps</code></td>
<td><code>int</code>, <em>optional</em>, defaults to 1</td>
<td>Number of updates steps to accumulate gradients before performing a backward/update pass.</td>
<td>✅</td>
<td></td>
</tr>
<tr>
<td><code>eval_accumulation_steps</code></td>
<td><code>int</code>, <em>optional</em></td>
<td>Number of predictions steps to accumulate the output tensors before moving the results to CPU.</td>
<td>✅</td>
<td></td>
</tr>
<tr>
<td><code>eval_delay</code></td>
<td><code>float</code>, <em>optional</em></td>
<td>Number of epochs or steps to wait before the first evaluation depending on <code>eval_strategy</code>.</td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>torch_empty_cache_steps</code></td>
<td><code>int</code>, <em>optional</em></td>
<td>Number of steps to wait before calling <code>torch.&lt;device&gt;.empty_cache()</code> to avoid CUDA out-of-memory errors.</td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>learning_rate</code></td>
<td><code>float</code>, <em>optional</em>, defaults to 5e-5</td>
<td>The initial learning rate for <code>AdamW</code> optimizer.</td>
<td>✅</td>
<td></td>
</tr>
<tr>
<td><code>num_train_epochs</code></td>
<td><code>float</code>, <em>optional</em>, defaults to 3.0</td>
<td>Total number of training epochs to perform.</td>
<td>✅</td>
<td></td>
</tr>
<tr>
<td><code>max_steps</code></td>
<td><code>int</code>, <em>optional</em>, defaults to -1</td>
<td>If set to a positive number, the total number of training steps to perform. Overrides <code>num_train_epochs</code>.</td>
<td>✅</td>
<td></td>
</tr>
<tr>
<td><code>lr_scheduler_type</code></td>
<td><code>str</code> or <code>SchedulerType</code>, <em>optional</em>, defaults to <code>"linear"</code></td>
<td>The scheduler type to use.</td>
<td>✅</td>
<td></td>
</tr>
<tr>
<td><code>lr_scheduler_kwargs</code></td>
<td><code>dict</code>, <em>optional</em>, defaults to {}</td>
<td>Extra arguments for the learning rate scheduler.</td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>warmup_ratio</code></td>
<td><code>float</code>, <em>optional</em>, defaults to 0.0</td>
<td>Ratio of total training steps used for linear warmup from 0 to <code>learning_rate</code>.</td>
<td>✅</td>
<td></td>
</tr>
<tr>
<td><code>warmup_steps</code></td>
<td><code>int</code>, <em>optional</em>, defaults to 0</td>
<td>Number of steps used for linear warmup from 0 to <code>learning_rate</code>. Overrides any effect of <code>warmup_ratio</code>.</td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>log_level</code></td>
<td><code>str</code>, <em>optional</em>, defaults to <code>passive</code></td>
<td>Logger log level to use on the main process.</td>
<td></td>
<td>✅</td>
</tr>
<tr>
<td><code>log_level_replica</code></td>
<td><code>str</code>, <em>optional</em>, defaults to <code>"warning"</code></td>
<td>Logger log level to use on replicas. Same choices as <code>log_level</code>.</td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>log_on_each_node</code></td>
<td><code>bool</code>, <em>optional</em>, defaults to <code>True</code></td>
<td>Whether to log using <code>log_level</code> once per node or only on the main node.</td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>logging_dir</code></td>
<td><code>str</code>, <em>optional</em></td>
<td>TensorBoard log directory.</td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>logging_strategy</code></td>
<td><code>str</code> or <code>~trainer_utils.IntervalStrategy</code>, <em>optional</em>, defaults to <code>"steps"</code></td>
<td>The logging strategy to adopt during training. Possible values are <code>"no"</code>, <code>"epoch"</code>, or <code>"steps"</code>.</td>
<td></td>
<td>✅</td>
</tr>
<tr>
<td><code>logging_first_step</code></td>
<td><code>bool</code>, <em>optional</em>, defaults to <code>False</code></td>
<td>Whether to log the first <code>global_step</code> or not.</td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>logging_steps</code></td>
<td><code>int</code> or <code>float</code>, <em>optional</em>, defaults to 500</td>
<td>Number of update steps between two logs if <code>logging_strategy="steps"</code>.</td>
<td></td>
<td>✅</td>
</tr>
<tr>
<td><code>logging_nan_inf_filter</code></td>
<td><code>bool</code>, <em>optional</em>, defaults to <code>True</code></td>
<td>Whether to filter <code>nan</code> and <code>inf</code> losses for logging.</td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>save_strategy</code></td>
<td><code>str</code> or <code>~trainer_utils.IntervalStrategy</code>, <em>optional</em>, defaults to <code>"steps"</code></td>
<td>The checkpoint save strategy to adopt during training.</td>
<td></td>
<td>✅</td>
</tr>
<tr>
<td><code>save_steps</code></td>
<td><code>int</code> or <code>float</code>, <em>optional</em>, defaults to 500</td>
<td>Number of update steps before two checkpoint saves if <code>save_strategy="steps"</code>.</td>
<td></td>
<td>✅</td>
</tr>
<tr>
<td><code>save_total_limit</code></td>
<td><code>int</code>, <em>optional</em></td>
<td>Limit for total number of checkpoints.</td>
<td></td>
<td>✅</td>
</tr>
<tr>
<td><code>save_safetensors</code></td>
<td><code>bool</code>, <em>optional</em>, defaults to <code>True</code></td>
<td>Use safetensors saving and loading for state dicts instead of default <code>torch.load</code> and <code>torch.save</code>.</td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>save_on_each_node</code></td>
<td><code>bool</code>, <em>optional</em>, defaults to <code>False</code></td>
<td>Whether to save models and checkpoints on each node or only on the main one during multi-node distributed training.</td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>seed</code></td>
<td><code>int</code>, <em>optional</em>, defaults to 42</td>
<td>Random seed set at the beginning of training for reproducibility.</td>
<td></td>
<td></td>
</tr>
<tr>
<td><code>auto_find_batch_size</code></td>
<td><code>bool</code>, <em>optional</em>, defaults to <code>False</code></td>
<td>Whether to find a batch size that will fit into memory automatically.</td>
<td>✅</td>
<td></td>
</tr>
<tr>
<td><code>fp16</code></td>
<td><code>bool</code>, <em>optional</em>, defaults to <code>False</code></td>
<td>Whether to use fp16 16-bit (mixed) precision training instead of 32-bit training.</td>
<td>✅</td>
<td></td>
</tr>
<tr>
<td><code>bf16</code></td>
<td><code>bool</code>, <em>optional</em>, defaults to <code>False</code></td>
<td>Whether to use bf16 16-bit (mixed) precision training instead of 32-bit training.</td>
<td>✅</td>
<td></td>
</tr>
<tr>
<td><code>push_to_hub</code></td>
<td><code>bool</code>, <em>optional</em>, defaults to <code>False</code></td>
<td>Whether to push the model to the Hub every time the model is saved.</td>
<td></td>
<td>✅</td>
</tr>
<tr>
<td><code>hub_model_id</code></td>
<td><code>str</code>, <em>optional</em></td>
<td>The name of the repository to keep in sync with the local <code>output_dir</code>.</td>
<td></td>
<td>✅</td>
</tr>
<tr>
<td><code>hub_strategy</code></td>
<td><code>str</code> or <code>~trainer_utils.HubStrategy</code>, <em>optional</em>, defaults to <code>"every_save"</code></td>
<td>Defines the scope of what is pushed to the Hub and when.</td>
<td></td>
<td>✅</td>
</tr>
<tr>
<td><code>hub_private_repo</code></td>
<td><code>bool</code>, <em>optional</em>, defaults to <code>False</code></td>
<td>If <code>True</code>, the Hub repo will be set to private.</td>
<td></td>
<td>✅</td>
</tr>
<tr>
<td><code>load_best_model_at_end</code></td>
<td><code>bool</code>, <em>optional</em>, defaults to <code>False</code></td>
<td>Whether or not to load the best model found during training at the end of training.</td>
<td></td>
<td>✅</td>
</tr>
<tr>
<td><code>report_to</code></td>
<td><code>str</code> or <code>List[str]</code>, <em>optional</em>, defaults to <code>"all"</code></td>
<td>The list of integrations to report the results and logs to.</td>
<td></td>
<td>✅</td>
</tr>
</tbody>
</table>
<h2 id="sentence-transformer-trainer-arguments">Sentence Transformer Trainer arguments<a class="headerlink" href="#sentence-transformer-trainer-arguments" title="Permanent link">&para;</a></h2>
<div class="tabbed-set tabbed-alternate" data-tabs="2:1"><input checked="checked" id="__tabbed_2_1" name="__tabbed_2" type="radio" /><div class="tabbed-labels"><label for="__tabbed_2_1">Table</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Name</th>
<th>Definition</th>
</tr>
</thead>
<tbody>
<tr>
<td>model</td>
<td><code>~sentence_transformers.SentenceTransformer</code>, <em>optional</em></td>
<td>The model to train, evaluate, or use for predictions. If not provided, a <code>model_init</code> must be passed.</td>
</tr>
<tr>
<td>args</td>
<td><code>~sentence_transformers.training_args.SentenceTransformerTrainingArguments</code>, <em>optional</em></td>
<td>The arguments to tweak for training. Defaults to a basic instance of <code>SentenceTransformerTrainingArguments</code> with the <code>output_dir</code> set to a directory named <em>tmp_trainer</em> in the current directory if not provided.</td>
</tr>
<tr>
<td>train_dataset</td>
<td><code>datasets.Dataset</code>, <code>datasets.DatasetDict</code>, or <code>Dict[str, datasets.Dataset]</code>, <em>optional</em></td>
<td>The dataset to use for training. Must have a format accepted by your loss function. Refer to <code>Training Overview &gt; Dataset Format</code>.</td>
</tr>
<tr>
<td>eval_dataset</td>
<td><code>datasets.Dataset</code>, <code>datasets.DatasetDict</code>, or <code>Dict[str, datasets.Dataset]</code>, <em>optional</em></td>
<td>The dataset to use for evaluation. Must have a format accepted by your loss function. Refer to <code>Training Overview &gt; Dataset Format</code>.</td>
</tr>
<tr>
<td>loss</td>
<td><code>torch.nn.Module</code>, <code>Dict[str, torch.nn.Module]</code>, Callable, or Dict[str, Callable], <em>optional</em></td>
<td>The loss function to use for training. It can be a loss class instance, a dictionary mapping dataset names to loss instances, a function returning a loss instance given a model, or a dictionary mapping dataset names to such functions. Defaults to <code>CoSENTLoss</code> if not provided.</td>
</tr>
<tr>
<td>evaluator</td>
<td><code>~sentence_transformers.evaluation.SentenceEvaluator</code> or <code>List[~sentence_transformers.evaluation.SentenceEvaluator]</code>, <em>optional</em></td>
<td>The evaluator instance for useful metrics during training. Can be used with or without an <code>eval_dataset</code>. A list of evaluators will be wrapped in a <code>SequentialEvaluator</code> to run sequentially. Generally, evaluator metrics are more useful than loss values from <code>eval_dataset</code>.</td>
</tr>
<tr>
<td>callbacks</td>
<td><code>List[transformers.TrainerCallback]</code>, <em>optional</em></td>
<td>A list of callbacks to customize the training loop. Adds to the list of default callbacks. To remove a default callback, use the <code>Trainer.remove_callback</code> method.</td>
</tr>
<tr>
<td>optimizers</td>
<td><code>Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]</code>, <em>optional</em>, defaults to <code>(None, None)</code></td>
<td>A tuple containing the optimizer and scheduler to use. Defaults to an instance of <code>torch.optim.AdamW</code> for the model and a scheduler given by <code>transformers.get_linear_schedule_with_warmup</code>, controlled by <code>args</code>.</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2023
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/lightonai/pylate" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["content.code.copy", "navigation.tabs", "navigation.instant", "navigation.indexes", "navigation.prune"], "search": "../../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.471ce7a9.min.js"></script>
      
        <script src="../../javascripts/config.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3.2/es5/tex-mml-chtml.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/vega@5"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/vega-lite@5"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/vega-embed@6"></script>
      
        <script src="https://unpkg.com/tablesort@5.3.0/dist/tablesort.min.js"></script>
      
        <script src="../../javascripts/tablesort.js"></script>
      
    
  </body>
</html>